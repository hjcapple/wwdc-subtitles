1
00:00:06,640 --> 00:00:11,044 line:0
（在Create ML中
训练声音分类模型）


2
00:00:13,881 --> 00:00:14,715 line:-1
早上好


3
00:00:15,549 --> 00:00:16,950 line:-1
我是Dan Klingler


4
00:00:17,217 --> 00:00:20,254 line:-2
我是Apple音效团队的一名
软件工程师


5
00:00:20,554 --> 00:00:22,990 line:-1
今天 我很高兴有机会能和你们聊聊


6
00:00:23,056 --> 00:00:26,627 line:-2
在Create ML中
训练声音分类模型


7
00:00:28,795 --> 00:00:30,330 line:-1
在我们开始之前


8
00:00:30,664 --> 00:00:34,334 line:-1
你可能想知道什么是声音分类


9
00:00:34,535 --> 00:00:37,271 line:-1
在你的app中有什么用处？


10
00:00:39,039 --> 00:00:42,609 line:-1
声音分类是一项录制一段声音


11
00:00:42,676 --> 00:00:45,345 line:-1
然后把它分类到不同的类别的技术


12
00:00:46,547 --> 00:00:47,781 line:-1
但仔细想一想


13
00:00:47,848 --> 00:00:50,851 line:-1
我们有很多种分类声音的方式


14
00:00:52,486 --> 00:00:55,956 line:-1
第一种是发声体


15
00:00:56,023 --> 00:00:57,391 line:-1
举个例子


16
00:00:57,457 --> 00:01:00,460 line:-1
我们有吉他或鼓的声音


17
00:01:01,028 --> 00:01:04,364 line:-1
不同的物件有不一样的声学特性


18
00:01:04,431 --> 00:01:07,668 line:-1
我们人类因此能分辨不同的声音


19
00:01:09,469 --> 00:01:12,472 line:-1
第二种声音分类的方式


20
00:01:12,940 --> 00:01:14,808 line:-1
是声音的源头


21
00:01:15,142 --> 00:01:16,977 line:-1
如果你有徒步过


22
00:01:17,044 --> 00:01:18,779 line:-1
或者在繁华城市中心


23
00:01:19,613 --> 00:01:22,816 line:-1
你能分辨出你周围声音的纹理


24
00:01:22,883 --> 00:01:24,084 line:-1
是非常不一样的


25
00:01:24,585 --> 00:01:26,587 line:-1
即使没有任何


26
00:01:26,653 --> 00:01:28,222 line:-1
脱颖而出的声音


27
00:01:31,425 --> 00:01:34,261 line:-1
第三种声音分类的方式


28
00:01:34,895 --> 00:01:36,930 line:-1
是声音的属性


29
00:01:36,997 --> 00:01:38,866 line:-1
或是声音的特性


30
00:01:39,132 --> 00:01:43,804 line:-2
举个例子
一个婴儿的笑声和哭声


31
00:01:44,071 --> 00:01:46,139 line:-1
都来自相同的源头


32
00:01:46,406 --> 00:01:48,675 line:-1
但是声音的特性却非常不同


33
00:01:48,742 --> 00:01:52,112 line:-1
它允许我们分辨出这些声音的不同


34
00:01:54,448 --> 00:01:56,216 line:-1
现在 作为app的开发者


35
00:01:56,550 --> 00:01:58,151 line:-1
你有不同的app


36
00:01:58,218 --> 00:02:01,755 line:-2
对于声音分类 你可能有
不同的使用场景


37
00:02:02,856 --> 00:02:06,326 line:-2
如果你能训练你自己的模型
为你的app量身定做


38
00:02:06,393 --> 00:02:09,463 line:-1
不是很棒吗？


39
00:02:11,632 --> 00:02:13,300 line:-1
现在使用Xcode中


40
00:02:13,367 --> 00:02:16,703 line:-2
的Create ML app
你就能做到了


41
00:02:17,137 --> 00:02:21,141 line:-1
这是训练声音分类模型最简单的方式


42
00:02:23,510 --> 00:02:25,546 line:-1
创建一个声音分类器


43
00:02:25,612 --> 00:02:29,516 line:-2
你需要为Create ML
提供标记过的声音数据


44
00:02:29,583 --> 00:02:31,518 line:-1
音频文件


45
00:02:33,086 --> 00:02:36,723 line:-2
接下来Create ML
就会用你自定义的文件


46
00:02:36,790 --> 00:02:38,492 line:-1
开始训练声音分类器模型


47
00:02:40,060 --> 00:02:42,596 line:-1
然后 你就能在你的app中


48
00:02:42,663 --> 00:02:44,731 line:-1
使用这个声音分类器


49
00:02:45,832 --> 00:02:49,102 line:-1
今天 我来用一个例子


50
00:02:49,169 --> 00:02:50,370 line:-1
向你展示如何实现


51
00:03:00,747 --> 00:03:04,084 line:-2
首先 我将打开
Create ML app


52
00:03:04,151 --> 00:03:07,054 line:-1
它是Xcode中附带安装的


53
00:03:09,289 --> 00:03:11,325 line:-1
我们将会创建一个新的文件


54
00:03:12,326 --> 00:03:14,661 line:-2
从模板选择中
选择“声音”


55
00:03:18,198 --> 00:03:19,466 line:-1
点击“下一步”


56
00:03:19,933 --> 00:03:22,870 line:-2
将我们的项目取名为
MySoundClassifier


57
00:03:24,304 --> 00:03:26,874 line:-2
保存这个项目
到我们的文档目录下


58
00:03:28,675 --> 00:03:30,410 line:-2
当Create ML app
开始运行时


59
00:03:30,477 --> 00:03:32,112 line:-1
你会看到主界面


60
00:03:32,446 --> 00:03:34,882 line:-1
左侧的输入标签被选中


61
00:03:36,083 --> 00:03:39,453 line:-2
为了训练我们自定义的模型
这是我们上传训练数据到


62
00:03:39,520 --> 00:03:41,522 line:-1
Create ML app的入口


63
00:03:43,323 --> 00:03:46,460 line:-1
你可以看到上方还有其他的标签


64
00:03:46,527 --> 00:03:49,062 line:-1
比如训练验证和测试


65
00:03:49,429 --> 00:03:51,665 line:-1
在训练的各个阶段


66
00:03:51,732 --> 00:03:55,235 line:-2
它们将为我们的模型
提供一些精确的统计数据


67
00:03:56,570 --> 00:03:57,638 line:-1
最后


68
00:03:57,704 --> 00:04:00,274 line:-2
我们可以在输出标签里
找到训练后的


69
00:04:00,340 --> 00:04:01,742 line:-1
模型


70
00:04:02,476 --> 00:04:05,312 line:-1
我们可以实时查看我们的模型


71
00:04:06,780 --> 00:04:10,450 line:-1
现在 我将开始训练一个乐器分类器


72
00:04:10,851 --> 00:04:13,487 line:-1
我带来了一些乐器


73
00:04:14,821 --> 00:04:18,058 line:-2
我有一个TrainingData文件夹
我们打开


74
00:04:18,125 --> 00:04:20,827 line:-1
这是我收集的一些声音文件


75
00:04:22,396 --> 00:04:27,034 line:-1
举个例子 它们包括


76
00:04:27,367 --> 00:04:30,037 line:-1
吉他 牛铃和振动筛


77
00:04:32,005 --> 00:04:33,340 line:-1
为了训练我们的模型


78
00:04:33,407 --> 00:04:34,608 line:-1
我们只需


79
00:04:34,675 --> 00:04:37,978 line:-2
将文件夹直接拖到
Create ML里


80
00:04:40,013 --> 00:04:41,548 line:-1
Create ML检测出


81
00:04:41,615 --> 00:04:45,152 line:-2
我们今天将会使用的声音文件
一共有49个


82
00:04:45,485 --> 00:04:48,422 line:-1
有7种不同的类别


83
00:04:50,624 --> 00:04:53,160 line:-1
我们点击“训练”按钮


84
00:04:53,227 --> 00:04:54,995 line:-1
我们的模型开始训练了


85
00:04:55,796 --> 00:04:58,432 line:-1
Create ML在训练模型时


86
00:04:58,498 --> 00:05:00,000 line:-1
首先会做的事情是


87
00:05:00,067 --> 00:05:03,136 line:-1
扫描我们上传的每个音频文件


88
00:05:03,203 --> 00:05:07,174 line:-1
检测出每个文件的声音特性


89
00:05:07,708 --> 00:05:10,344 line:-1
当它收集完所有的声音特性后


90
00:05:10,410 --> 00:05:12,713 line:-1
它会开始处理我们现在看到的


91
00:05:12,779 --> 00:05:15,749 line:-1
不断更新迭代模型的权重


92
00:05:17,317 --> 00:05:19,186 line:-1
模型的权重不断更新


93
00:05:19,253 --> 00:05:21,555 line:-1
你会看到性能表现不断被优化


94
00:05:21,989 --> 00:05:24,992 line:-1
精准度达到了100%


95
00:05:25,058 --> 00:05:27,895 line:-1
这是个好兆头 我们的模型正在收敛


96
00:05:28,929 --> 00:05:31,732 line:-1
我们今天收集的音频文件


97
00:05:31,798 --> 00:05:33,133 line:-1
特征都相当明显


98
00:05:33,200 --> 00:05:36,370 line:-2
比如牛铃和木吉他
听起来非常不同


99
00:05:36,436 --> 00:05:38,272 line:-1
所以我们训练的这个特殊模型


100
00:05:38,338 --> 00:05:40,307 line:-1
处理声音非常的成功


101
00:05:40,374 --> 00:05:41,575 line:-1
如你所见


102
00:05:41,875 --> 00:05:44,778 line:-1
训练和验证集合


103
00:05:46,613 --> 00:05:50,517 line:-1
测试面板是提供大数据集的好地方


104
00:05:50,584 --> 00:05:53,086 line:-1
你可以拿它作为基准参考


105
00:05:53,887 --> 00:05:58,659 line:-2
Create ML app
允许你同时训练多个模型


106
00:05:58,959 --> 00:06:01,662 line:-1
它同时提供了多个数据集


107
00:06:02,129 --> 00:06:06,567 line:-2
如果你想为你训练的
不同的模型结构提供一个常用基准


108
00:06:06,633 --> 00:06:09,369 line:-1
你可以使用测试面板


109
00:06:10,938 --> 00:06:13,473 line:-1
最后 我们来看Output标签


110
00:06:13,774 --> 00:06:17,377 line:-1
我们在用户界面能与我们的模型互动


111
00:06:18,412 --> 00:06:20,180 line:-1
现在 我有一个


112
00:06:20,247 --> 00:06:22,549 line:-1
在训练集没有的文件


113
00:06:23,517 --> 00:06:26,386 line:-2
我将它放到
TestingData文件夹


114
00:06:27,254 --> 00:06:29,623 line:-1
当我将文件夹拖入用户界面时


115
00:06:29,690 --> 00:06:33,327 line:-2
你能看到它扫描了这个文件
叫做分类测试


116
00:06:35,229 --> 00:06:37,264 line:-1
当我们滚动这个文件


117
00:06:37,331 --> 00:06:38,932 line:-1
Create ML


118
00:06:38,999 --> 00:06:43,103 line:-1
将第一秒识别为了背景噪音


119
00:06:43,904 --> 00:06:46,340 line:-1
接下来的几秒是讲话声


120
00:06:46,840 --> 00:06:48,609 line:-1
最后是振动筛


121
00:06:50,544 --> 00:06:54,281 line:-1
现在我们来看这个分类是否准确


122
00:06:54,348 --> 00:06:57,150 line:-1
我们能在这个界面听一下


123
00:06:58,852 --> 00:07:00,888 line:-1
测试 一 二 三


124
00:07:10,831 --> 00:07:14,067 line:-1
至少 它能准确的识别


125
00:07:14,134 --> 00:07:16,937 line:-1
我们上传的这个文件


126
00:07:17,304 --> 00:07:19,506 line:-1
现在 更棒的是


127
00:07:19,773 --> 00:07:22,176 line:-1
我们来用这个模型进行实时互动


128
00:07:22,643 --> 00:07:23,810 line:-1
为了实现目标


129
00:07:23,877 --> 00:07:27,214 line:-1
我们添加了一个按钮“录制麦克风”


130
00:07:28,849 --> 00:07:30,384 line:-1
只要我开始录制


131
00:07:30,684 --> 00:07:33,754 line:-1
我的Mac就会开始向模型


132
00:07:33,820 --> 00:07:35,522 line:-1
传入麦克风录制的声音数据


133
00:07:43,931 --> 00:07:46,733 line:-1
所以 你看 只要我一说话


134
00:07:46,800 --> 00:07:49,803 line:-1
模型就能高效的识别出说话声


135
00:07:50,070 --> 00:07:51,405 line:-1
当我安静时


136
00:07:51,471 --> 00:07:54,675 line:-1
模型则变为背景音状态


137
00:07:57,144 --> 00:07:58,979 line:-1
我带来了一些乐器


138
00:07:59,046 --> 00:08:01,615 line:-1
我现在弹奏它们 看模型能否识别


139
00:08:02,583 --> 00:08:04,051 line:-1
首先是振动筛


140
00:08:12,726 --> 00:08:14,728 line:-1
接下来是牛铃


141
00:08:20,100 --> 00:08:22,402 line:-1
更多牛铃


142
00:08:22,703 --> 00:08:25,506 line:-2
好吧 你们还想再听牛铃
来吧


143
00:08:30,978 --> 00:08:33,813 line:-1
接下来 是我的木吉他


144
00:08:34,147 --> 00:08:36,149 line:-1
我们同样来试一下


145
00:08:38,719 --> 00:08:40,988 line:-1
以单音节来开始


146
00:08:47,127 --> 00:08:49,263 line:-1
下面 我来试一下和弦


147
00:09:06,313 --> 00:09:07,714 line:-1
识别的非常完美


148
00:09:07,781 --> 00:09:09,750 line:-1
我觉得它很完美


149
00:09:10,250 --> 00:09:12,419 line:-1
现在我停止录制


150
00:09:12,953 --> 00:09:14,321 line:-1
在Create ML app里


151
00:09:14,388 --> 00:09:16,957 line:-1
我能回滚这段录音


152
00:09:17,024 --> 00:09:20,427 line:-1
查看之前已被识别的片段


153
00:09:21,128 --> 00:09:24,331 line:-1
我们来看一下是否有不准确的地方


154
00:09:24,398 --> 00:09:26,300 line:-1
或有误的地方


155
00:09:26,366 --> 00:09:28,302 line:-1
我们也可以在文件中截取片段


156
00:09:28,368 --> 00:09:30,337 line:-1
用作测试数据


157
00:09:30,404 --> 00:09:32,439 line:-1
来提高我们的模型


158
00:09:33,407 --> 00:09:34,575 line:-1
最后


159
00:09:34,641 --> 00:09:37,477 line:-1
我们很高兴看到模型优秀的表现


160
00:09:37,778 --> 00:09:40,380 line:-1
我们将模型拖到桌面


161
00:09:40,447 --> 00:09:42,649 line:-1
集成到我们的app里面


162
00:09:43,584 --> 00:09:47,254 line:-2
以上就是如何在Create ML app
训练声音分类器


163
00:09:47,321 --> 00:09:49,957 line:-1
不用一分钟 一行代码


164
00:09:59,132 --> 00:10:01,134 line:-1
在演示app里


165
00:10:01,735 --> 00:10:04,938 line:-2
当你收集你的训练数据时
有一些细节需要留意


166
00:10:05,839 --> 00:10:07,307 line:-1
第一个你需要注意的地方是


167
00:10:07,374 --> 00:10:10,010 line:-1
我如何在文件夹里收集数据


168
00:10:11,712 --> 00:10:13,881 line:-1
所有吉他的声音


169
00:10:13,947 --> 00:10:16,083 line:-1
都了放在Guitar的目录里


170
00:10:16,483 --> 00:10:20,621 line:-2
其他文件也是一样
比如鼓声和背景音


171
00:10:21,722 --> 00:10:24,291 line:-1
现在 我们来聊一下背景音类别


172
00:10:25,959 --> 00:10:28,629 line:-1
即便我们训练了乐器分类器


173
00:10:28,862 --> 00:10:31,131 line:-1
你还是需要留意


174
00:10:31,198 --> 00:10:33,834 line:-1
当没有乐器声音时


175
00:10:34,401 --> 00:10:37,471 line:-1
如果你只让你的模型识别乐器


176
00:10:37,771 --> 00:10:39,673 line:-1
但如果你给模型识别背景音


177
00:10:39,740 --> 00:10:41,775 line:-1
它并不知道有这种声音


178
00:10:41,842 --> 00:10:45,012 line:-1
所以 当你在训练一个声音分类器时


179
00:10:45,078 --> 00:10:47,614 line:-1
你需要为你的模型考虑多种状况


180
00:10:48,148 --> 00:10:49,750 line:-1
你需要将


181
00:10:50,083 --> 00:10:52,586 line:-1
背景音单独划分为一类


182
00:10:55,923 --> 00:10:59,359 line:-1
现在假设你有一个声音文件


183
00:10:59,726 --> 00:11:03,063 line:-1
这个文件最开始是鼓声


184
00:11:03,463 --> 00:11:05,766 line:-1
接着是背景噪音


185
00:11:06,133 --> 00:11:08,902 line:-1
随后是吉他的声音


186
00:11:09,937 --> 00:11:11,505 line:-1
如果你直接将它拖到


187
00:11:12,072 --> 00:11:15,008 line:-1
Create ML app里


188
00:11:15,075 --> 00:11:17,344 line:-1
它并不会是合适的训练文件


189
00:11:17,411 --> 00:11:18,312 line:-1
那是因为


190
00:11:18,378 --> 00:11:21,849 line:-1
这个文件包含了多个音乐类别


191
00:11:23,350 --> 00:11:24,651 line:-1
请记住


192
00:11:24,718 --> 00:11:27,821 line:-1
你必须使用标记的文件夹来训练模型


193
00:11:27,888 --> 00:11:30,591 line:-1
所以现在我们要做的事


194
00:11:30,657 --> 00:11:32,593 line:-1
将这个文件截取为三个文件


195
00:11:32,659 --> 00:11:35,529 line:-1
将它们命名为鼓 吉他和背景音


196
00:11:36,763 --> 00:11:38,932 line:-1
这样模型才会有良好的表现


197
00:11:38,999 --> 00:11:42,002 line:-1
当你训练模型时 你要这样分开它们


198
00:11:44,972 --> 00:11:48,308 line:-1
在收集音频数据时 你还需要注意


199
00:11:49,543 --> 00:11:53,046 line:-1
第一 你需要保证你收集的音频是


200
00:11:53,113 --> 00:11:55,616 line:-1
在真实世界存在的


201
00:11:57,251 --> 00:12:01,388 line:-1
你的app将在很多不同房间


202
00:12:01,455 --> 00:12:03,023 line:-1
和声音场景工作


203
00:12:03,090 --> 00:12:06,226 line:-1
你可以使用卷积技术


204
00:12:06,293 --> 00:12:08,629 line:-1
模拟不同的声音场景


205
00:12:08,695 --> 00:12:10,731 line:-1
或是不同的房间


206
00:12:13,800 --> 00:12:15,602 line:-1
另外一个重要的点是


207
00:12:15,669 --> 00:12:17,804 line:-1
是设备上的录音处理


208
00:12:19,173 --> 00:12:21,875 line:-1
你可能会检查AV音频会话模式


209
00:12:22,409 --> 00:12:24,077 line:-1
来为你的app的


210
00:12:24,144 --> 00:12:26,480 line:-2
录音处理
选择不同的模式


211
00:12:26,547 --> 00:12:29,082 line:-1
选择你app最匹配的模式


212
00:12:29,349 --> 00:12:32,319 line:-1
或是符合你收集的训练数据


213
00:12:34,521 --> 00:12:35,689 line:-1
最后一点


214
00:12:36,023 --> 00:12:38,859 line:-1
是要留意你的模型结构


215
00:12:39,259 --> 00:12:41,328 line:-1
这个声音分类器模型


216
00:12:41,395 --> 00:12:45,365 line:-1
它能很好地区分不同类别的声音


217
00:12:45,632 --> 00:12:48,368 line:-1
但它并不是很适合


218
00:12:48,435 --> 00:12:50,637 line:-1
用来训练所有的会话内容


219
00:12:50,704 --> 00:12:52,506 line:-1
有更好的工具来做这个任务


220
00:12:52,573 --> 00:12:55,309 line:-1
所以 你要确保选择了正确的工具


221
00:12:57,644 --> 00:13:00,681 line:-1
现在你有了ML模型


222
00:13:00,747 --> 00:13:02,416 line:-1
我们来讲一下如何


223
00:13:02,482 --> 00:13:03,884 line:-1
将它集成到你的app


224
00:13:06,019 --> 00:13:08,121 line:-1
让模型能尽可能简单的


225
00:13:08,188 --> 00:13:10,858 line:-1
在你的app中发挥作用


226
00:13:10,924 --> 00:13:14,795 line:-2
我们发布了一个新框架
叫做SoundAnalysis


227
00:13:16,096 --> 00:13:17,564 line:-1
SoundAnalysis


228
00:13:17,631 --> 00:13:21,001 line:-1
是一个用来分析声音的高级框架


229
00:13:22,069 --> 00:13:24,338 line:-1
它使用了Core ML模型


230
00:13:25,339 --> 00:13:28,375 line:-1
它能在内部进行一些常用的声音操作


231
00:13:28,442 --> 00:13:33,046 line:-2
比如通道映射
采样率转换和后粘连


232
00:13:35,249 --> 00:13:36,917 line:-1
让我们在后台看一下


233
00:13:36,984 --> 00:13:39,453 line:-2
SoundAnalysis
是如何工作的


234
00:13:39,853 --> 00:13:42,890 line:-1
现在 上面的部分是你的app


235
00:13:43,190 --> 00:13:44,491 line:-1
下面是


236
00:13:44,558 --> 00:13:47,361 line:-2
SoundAnalysis
后台处理的流程


237
00:13:48,629 --> 00:13:50,197 line:-1
你要做的第一件事是


238
00:13:50,264 --> 00:13:53,267 line:-2
给SoundAnalysis框架
提供你用Create ML


239
00:13:53,534 --> 00:13:55,235 line:-1
训练后的模型


240
00:13:57,538 --> 00:14:01,241 line:-1
接着 你的app会提供


241
00:14:01,308 --> 00:14:02,843 line:-1
需要被识别一些音频


242
00:14:04,878 --> 00:14:08,182 line:-1
这个音频会先被通道映射处理


243
00:14:08,248 --> 00:14:12,319 line:-2
它能保证你的模型将会收到
一段音频


244
00:14:12,386 --> 00:14:13,754 line:-1
就像我们之前那样


245
00:14:14,087 --> 00:14:16,089 line:-1
传给模型一个音频


246
00:14:16,156 --> 00:14:17,424 line:-1
甚至在一个客户端


247
00:14:17,491 --> 00:14:20,360 line:-1
比如 你在上传一个立体音频数据


248
00:14:22,529 --> 00:14:25,699 line:-2
下一步是
采样率转换


249
00:14:26,400 --> 00:14:27,634 line:-1
我们训练的模型


250
00:14:27,701 --> 00:14:31,238 line:-1
适合处理16赫兹音频


251
00:14:31,538 --> 00:14:34,007 line:-1
这样能保证你提供的音频


252
00:14:34,074 --> 00:14:37,511 line:-1
会转化成模型想要的速率


253
00:14:41,381 --> 00:14:43,784 line:-2
最后一步是
SoundAnalysis


254
00:14:43,851 --> 00:14:45,752 line:-1
这是一个音频缓冲的操作


255
00:14:46,820 --> 00:14:48,822 line:-1
我们现在使用的大多数模型


256
00:14:49,389 --> 00:14:53,694 line:-1
需要一定量的音频数据来处理分析块


257
00:14:54,061 --> 00:14:58,332 line:-1
通常情况下 客户端里的音频


258
00:14:58,398 --> 00:15:01,335 line:-1
通常都会流进任意大小的缓冲区


259
00:15:01,401 --> 00:15:04,905 line:-2
要实现一个有效的循环缓冲区
需要很多工作量


260
00:15:04,972 --> 00:15:08,108 line:-1
来确保向你的模型传入


261
00:15:08,175 --> 00:15:09,309 line:-1
一个正确大小的音频块


262
00:15:09,843 --> 00:15:11,912 line:-2
如果你的模型需要大约一分钟左右的
音频文件


263
00:15:11,979 --> 00:15:14,781 line:-1
这一步能保证


264
00:15:14,848 --> 00:15:17,918 line:-1
传给模型的文件都是合规的


265
00:15:18,752 --> 00:15:22,523 line:-1
最后 当数据传给模型后


266
00:15:22,589 --> 00:15:24,391 line:-1
你的app将会收到包含关于音频


267
00:15:24,458 --> 00:15:28,562 line:-1
分类结果一个反馈


268
00:15:29,663 --> 00:15:31,164 line:-1
现在 好消息是


269
00:15:31,231 --> 00:15:33,634 line:-1
你并不需要知道所有的这些


270
00:15:33,901 --> 00:15:36,436 line:-1
你只需将音频提供给


271
00:15:36,503 --> 00:15:38,505 line:-1
SoundAnalysis框架


272
00:15:38,572 --> 00:15:41,108 line:-2
接着在你的app里
处理结果就可以了


273
00:15:43,710 --> 00:15:46,346 line:-1
那么 我们来聊一下


274
00:15:46,413 --> 00:15:48,949 line:-2
你从SoundAnalysis
获取的结果


275
00:15:49,650 --> 00:15:51,251 line:-1
音频是一个流媒体


276
00:15:51,318 --> 00:15:53,587 line:-1
它并不像图片一样


277
00:15:53,654 --> 00:15:54,888 line:-1
有开始和结尾


278
00:15:55,355 --> 00:15:56,723 line:-1
因此


279
00:15:56,957 --> 00:15:59,960 line:-1
这个结果可能和我们预想的有点不同


280
00:16:01,094 --> 00:16:03,497 line:-1
你的结果包含了一段时间


281
00:16:03,564 --> 00:16:07,868 line:-1
它和分析结果的音频时间相对应


282
00:16:08,702 --> 00:16:10,103 line:-1
在这个例子中


283
00:16:10,170 --> 00:16:12,639 line:-1
训练模型的块大小是被指定的


284
00:16:12,706 --> 00:16:15,309 line:-1
你能看到大约是一分钟


285
00:16:17,077 --> 00:16:19,880 line:-1
当你不断为模型提供音频时


286
00:16:19,947 --> 00:16:21,815 line:-1
你会不断收到包含


287
00:16:21,882 --> 00:16:26,386 line:-1
你分析的音频块的顶级分类结果


288
00:16:27,688 --> 00:16:29,189 line:-1
现在你可能留意到


289
00:16:29,256 --> 00:16:32,025 line:-1
这一秒的结果覆盖了之前的


290
00:16:32,092 --> 00:16:33,560 line:-1
大约50%


291
00:16:33,894 --> 00:16:35,662 line:-1
我们就是这样设计的


292
00:16:36,663 --> 00:16:40,033 line:-1
你要确定你提供的每个音频片段


293
00:16:40,100 --> 00:16:43,770 line:-1
都能出现在分析视图的中间


294
00:16:44,037 --> 00:16:47,441 line:-1
否则它会出现在分析视图左右两边


295
00:16:47,508 --> 00:16:50,511 line:-1
模型就不会正常发挥作用


296
00:16:51,311 --> 00:16:55,048 line:-1
所以 分析结果时默认是50%重合


297
00:16:55,115 --> 00:16:57,017 line:-1
但如果你有相应的使用场景时


298
00:16:57,084 --> 00:16:59,620 line:-1
在API中是可配置的


299
00:17:00,921 --> 00:17:03,457 line:-1
当你持续提供音频数据时


300
00:17:04,090 --> 00:17:06,359 line:-1
你会一直收到结果


301
00:17:07,294 --> 00:17:10,063 line:-1
只要音频流是有效的


302
00:17:10,130 --> 00:17:12,465 line:-1
你就能一直上传数据 然后得到结果


303
00:17:15,935 --> 00:17:18,105 line:-1
现在 我们来快速过一下


304
00:17:18,172 --> 00:17:21,608 line:-2
SoundAnalysis框架
提供的API


305
00:17:23,410 --> 00:17:25,512 line:-1
假设我们有一个音频文件


306
00:17:25,579 --> 00:17:26,813 line:-1
我们想要用今天


307
00:17:26,880 --> 00:17:29,550 line:-1
训练的模型来分析它


308
00:17:30,551 --> 00:17:31,618 line:-1
首先


309
00:17:31,685 --> 00:17:33,954 line:-1
我们创建了一个音频文件分析器


310
00:17:34,021 --> 00:17:37,291 line:-1
将文件的URL提供给分析器


311
00:17:38,825 --> 00:17:42,462 line:-2
接着 我们创建了一个
classifySoundRequest


312
00:17:42,529 --> 00:17:45,132 line:-1
接着实例化一个模型


313
00:17:45,199 --> 00:17:47,067 line:-1
MySoundClassifier


314
00:17:49,469 --> 00:17:52,906 line:-1
接着 我们向分析器提出请求


315
00:17:53,273 --> 00:17:54,675 line:-1
提供一个观察器


316
00:17:54,741 --> 00:17:57,911 line:-1
它会处理我们模型产出的结果


317
00:17:59,746 --> 00:18:02,783 line:-1
最后 我们来分析文件


318
00:18:02,850 --> 00:18:06,486 line:-1
它会先扫描文件然后处理结果


319
00:18:09,156 --> 00:18:10,958 line:-1
现在 在你的app那一侧


320
00:18:11,325 --> 00:18:13,627 line:-1
你需要确保你的一个类


321
00:18:13,694 --> 00:18:17,097 line:-1
符合SNResultsObserving协议


322
00:18:17,865 --> 00:18:20,701 line:-1
这就是你会从框架收到的结果


323
00:18:22,169 --> 00:18:26,373 line:-2
你要实现的第一个方法是
请求didProduce结果


324
00:18:27,574 --> 00:18:30,444 line:-1
这个方法可能会被调用很多次


325
00:18:30,711 --> 00:18:34,147 line:-1
只要当新的监测可用时


326
00:18:35,916 --> 00:18:39,453 line:-2
你就有可能获取到
顶级分类结果


327
00:18:39,520 --> 00:18:41,355 line:-1
和与之相关的时间范围


328
00:18:41,421 --> 00:18:44,224 line:-1
这就是你app中处理声音分类


329
00:18:44,291 --> 00:18:47,561 line:-1
事件时运行的逻辑


330
00:18:49,630 --> 00:18:51,565 line:-1
另外一个你可能感兴趣的方法是


331
00:18:51,632 --> 00:18:53,934 line:-2
请求
didFailWithError


332
00:18:54,234 --> 00:18:56,370 line:-1
如果由于某种原因分析失败了


333
00:18:56,436 --> 00:18:57,971 line:-1
这个方法就会被调用


334
00:18:58,238 --> 00:19:02,109 line:-2
接下来 你将不会从
分析器收到任何结果了


335
00:19:02,843 --> 00:19:06,146 line:-1
或者这个流媒体被成功处理了


336
00:19:06,213 --> 00:19:08,081 line:-1
比如 在文件的最后


337
00:19:08,415 --> 00:19:11,084 line:-2
你会收到
请求didComplete


338
00:19:13,554 --> 00:19:16,390 line:-1
下面 我们来做一下今天的总结


339
00:19:17,691 --> 00:19:21,161 line:-2
你知道了如何在Create ML
使用你自己的音频数据


340
00:19:21,361 --> 00:19:23,163 line:-1
来训练声音分类器


341
00:19:25,799 --> 00:19:28,735 line:-2
以及 在设备中
使用SoundAnalysis


342
00:19:28,802 --> 00:19:30,804 line:-1
框架来运行模型


343
00:19:33,040 --> 00:19:34,508 line:0
了解更多信息


344
00:19:34,575 --> 00:19:38,979 line:0
可以在developer.apple.com
查看声音分类的文章


345
00:19:39,546 --> 00:19:41,148 line:0
你会在文章中找到一个


346
00:19:41,215 --> 00:19:45,219 line:0
使用设备内置的麦克风
和AV音频引擎


347
00:19:45,285 --> 00:19:47,020 line:0
来进行声音分类的例子


348
00:19:47,387 --> 00:19:50,457 line:0
和之前大家看到的乐器分类
演示app类似


349
00:19:52,993 --> 00:19:54,428 line:-1
感谢聆听


350
00:19:54,494 --> 00:19:58,765 line:-2
希望大家都能在app中
使用声音分类


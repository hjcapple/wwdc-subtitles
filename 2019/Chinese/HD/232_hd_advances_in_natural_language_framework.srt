1
00:00:00,506 --> 00:00:04,500
[音乐]


2
00:00:07,516 --> 00:00:11,286
[掌声]


3
00:00:11,786 --> 00:00:13,076
>> 大家下午好


4
00:00:13,546 --> 00:00:14,736
欢迎来到我们的讲演


5
00:00:14,736 --> 00:00:15,976
自然语言处理


6
00:00:16,566 --> 00:00:18,266
我叫 Vivek 我的同事 


7
00:00:18,266 --> 00:00:19,656
Doug Davidson 会和我一起


8
00:00:19,656 --> 00:00:20,806
完成讲演


9
00:00:21,646 --> 00:00:23,086
我们开始吧


10
00:00:23,716 --> 00:00:27,796
如你所知 文本无处不在 随处可见


11
00:00:28,436 --> 00:00:29,826
用户在 App 中  


12
00:00:29,826 --> 00:00:31,256
和文本交互的模式


13
00:00:31,906 --> 00:00:34,796
主要有两个


14
00:00:34,796 --> 00:00:36,376
一是通过自然语言输入


15
00:00:36,456 --> 00:00:42,296
用户在 App 内写入文本 或生成文本


16
00:00:43,176 --> 00:00:46,136
比如用户可能在 App 里 


17
00:00:46,136 --> 00:00:48,266
用键盘键入文本


18
00:00:48,526 --> 00:00:49,996
这种 App 有很多


19
00:00:49,996 --> 00:00:51,506
比如 《信息》


20
00:00:51,506 --> 00:00:53,566
用户写入文本然后分享给其他人


21
00:00:53,566 --> 00:00:55,456
还有《备忘录》


22
00:00:55,456 --> 00:00:56,956
或者任何效率 App 


23
00:00:56,956 --> 00:00:58,776
部分功能需要你键入文本


24
00:01:00,246 --> 00:01:01,136
另一种用户


25
00:01:01,136 --> 00:01:03,126
和 App 内文本互动的方式


26
00:01:03,126 --> 00:01:05,626
是通过自然语言输出


27
00:01:06,046 --> 00:01:07,696
App 把文本内容


28
00:01:07,696 --> 00:01:09,126
提供给用户


29
00:01:09,286 --> 00:01:11,706
用户使用或者读取这个文本


30
00:01:12,836 --> 00:01:14,256
这类 App 有


31
00:01:14,436 --> 00:01:17,346
比如《新闻》


32
00:01:17,346 --> 00:01:18,466
信息或者文本展示给用户


33
00:01:18,466 --> 00:01:20,796
用户阅读这个信息


34
00:01:21,666 --> 00:01:24,116
所以 不论是文本输入还是输出


35
00:01:24,226 --> 00:01:26,686
为了从原始文本中


36
00:01:26,686 --> 00:01:28,366
提取可操作的情报


37
00:01:28,546 --> 00:01:30,346
自然语言处理


38
00:01:30,346 --> 00:01:32,146
都是非常重要的


39
00:01:32,146 --> 00:01:34,816
去年 我们介绍了


40
00:01:34,816 --> 00:01:36,086
自然语言框架


41
00:01:36,796 --> 00:01:38,236
自然语言框架


42
00:01:38,406 --> 00:01:41,016
是 Apple 所有平台通用的 


43
00:01:41,016 --> 00:01:42,796
可处理所有东西的自然语言处理的主力


44
00:01:43,406 --> 00:01:44,476
所以我们提供了几个


45
00:01:44,546 --> 00:01:46,256
基础的 NLP 模块


46
00:01:46,696 --> 00:01:48,276
比如语言识别


47
00:01:48,276 --> 00:01:51,666
分词 词性标注等等


48
00:01:51,666 --> 00:01:53,036
我们展示了这些基础功能


49
00:01:53,036 --> 00:01:55,796
并跨语种提供这些功能


50
00:01:55,796 --> 00:01:58,456
这是通过无缝融合 


51
00:01:58,456 --> 00:02:00,156
语言学和机器学习实现的


52
00:02:00,156 --> 00:02:01,516
所以你可以只关注  


53
00:02:01,516 --> 00:02:03,356
通过使用这些 API


54
00:02:03,356 --> 00:02:05,526
搭建你的 App


55
00:02:05,526 --> 00:02:07,706
繁重的工作 我们在幕后处理


56
00:02:08,336 --> 00:02:09,955
现在 如果你退一步


57
00:02:09,955 --> 00:02:11,306
看看所有这些功能


58
00:02:11,536 --> 00:02:13,466
实际上 如果你看看


59
00:02:13,466 --> 00:02:14,936
大部分 NLP 功能


60
00:02:14,936 --> 00:02:16,676
它们可以分为


61
00:02:17,006 --> 00:02:19,176
两大类任务


62
00:02:19,176 --> 00:02:20,936
第一类是文本分类


63
00:02:21,066 --> 00:02:22,796
文本分类的目的是


64
00:02:22,796 --> 00:02:24,866
给一个文本


65
00:02:24,866 --> 00:02:26,286
这个文本可以是一个句子


66
00:02:26,286 --> 00:02:28,266
可以是一个段落


67
00:02:28,266 --> 00:02:30,256
或者一个文件


68
00:02:30,256 --> 00:02:31,566
你想向这个文本


69
00:02:31,566 --> 00:02:33,336
分配标签


70
00:02:33,336 --> 00:02:35,096
这些标签可以是情感标签


71
00:02:35,096 --> 00:02:37,096
可以是话题标签 任何你想分配的标签


72
00:02:38,426 --> 00:02:40,036
另一类 NLP 任务被称为


73
00:02:40,036 --> 00:02:41,696
单词标注


74
00:02:41,696 --> 00:02:43,576
这里的任务


75
00:02:43,576 --> 00:02:45,176
或这里的目的是


76
00:02:45,176 --> 00:02:46,756
给一系列词 也被称为 token


77
00:02:46,756 --> 00:02:48,606
我们想给这个序列里的


78
00:02:48,606 --> 00:02:51,456
每个 token 分配一个标签


79
00:02:51,656 --> 00:02:54,366
今年 在文本分类


80
00:02:54,366 --> 00:02:55,986
和单词标记中 


81
00:02:55,986 --> 00:02:57,776
我们都有新的 API


82
00:02:57,776 --> 00:02:59,646
首先 我们先从


83
00:02:59,646 --> 00:03:01,616
情感分析开始


84
00:03:02,356 --> 00:03:04,016
情感分析是一个文本分类 API


85
00:03:04,016 --> 00:03:05,666
这是个新的 API


86
00:03:05,666 --> 00:03:07,946
是这样运作的


87
00:03:08,536 --> 00:03:09,766
你要做的是


88
00:03:09,766 --> 00:03:11,496
把文本传输到这个 API


89
00:03:11,496 --> 00:03:14,016
API 分析文本


90
00:03:14,046 --> 00:03:15,816
给你一个


91
00:03:15,816 --> 00:03:16,656
情感分值


92
00:03:17,466 --> 00:03:18,666
这个情感分值


93
00:03:18,786 --> 00:03:20,536
捕捉文本里的 


94
00:03:20,536 --> 00:03:21,686
情感程度


95
00:03:22,786 --> 00:03:24,106
情感分值


96
00:03:24,106 --> 00:03:26,626
从负一到正一不等


97
00:03:26,626 --> 00:03:27,516
表示情感的程度


98
00:03:28,216 --> 00:03:29,516
比如 -1.0 表明是一个


99
00:03:29,516 --> 00:03:31,396
非常强烈的消极情感


100
00:03:31,396 --> 00:03:33,256
1.0 表明是


101
00:03:33,256 --> 00:03:34,336
非常强烈的积极情感


102
00:03:34,886 --> 00:03:36,056
所以基本上 


103
00:03:36,056 --> 00:03:37,506
我们提供一个分值 


104
00:03:37,506 --> 00:03:38,846
让你为 App 测定分值


105
00:03:39,036 --> 00:03:41,356
举个例子 如果有一句话


106
00:03:41,356 --> 00:03:42,836
比如 我们在夏威夷


107
00:03:42,836 --> 00:03:44,386
和家人玩得很开心


108
00:03:44,586 --> 00:03:45,946
API 可能会给出 0.8 分


109
00:03:45,946 --> 00:03:48,166
表明这句话


110
00:03:48,166 --> 00:03:50,476
是一句积极的句子


111
00:03:51,616 --> 00:03:52,756
相反 如果这句话是


112
00:03:52,796 --> 00:03:54,266
我们在夏威夷玩得不好


113
00:03:54,266 --> 00:03:55,656
因为妈妈扭伤了她的脚踝


114
00:03:55,656 --> 00:03:57,166
这不是一句积极的话


115
00:03:57,166 --> 00:03:59,156
所以你得到 -0.8 分


116
00:03:59,156 --> 00:04:00,956
然后你就可以判定


117
00:04:00,956 --> 00:04:02,836
这是一个消极的情感


118
00:04:04,126 --> 00:04:05,576
非常棒 你怎么用呢


119
00:04:06,376 --> 00:04:07,676
用起来真的很简单


120
00:04:08,006 --> 00:04:09,646
对于你们 习惯使用 


121
00:04:09,646 --> 00:04:11,036
NaturalLanguage 的人


122
00:04:11,036 --> 00:04:11,886
这会非常简单


123
00:04:12,416 --> 00:04:13,736
导入 NaturalLanguage


124
00:04:14,236 --> 00:04:15,496
创建一个实例 NLTagger


125
00:04:15,536 --> 00:04:17,766
现在你做的就是


126
00:04:17,836 --> 00:04:19,646
指定一个新的标签方案


127
00:04:19,786 --> 00:04:22,456
这个标签方案被称为情感分值


128
00:04:23,266 --> 00:04:25,116
然后把想分析的字符串 


129
00:04:25,116 --> 00:04:26,466
附加到 tagger


130
00:04:26,466 --> 00:04:28,256
然后你只需要


131
00:04:28,256 --> 00:04:29,576
在句子层面


132
00:04:29,906 --> 00:04:32,446
或者在段落层面请求情感分值


133
00:04:33,476 --> 00:04:34,996
看一下实际运行情况


134
00:04:36,316 --> 00:04:37,576
这儿有一个


135
00:04:37,576 --> 00:04:38,516
假设 App


136
00:04:38,876 --> 00:04:40,106
它是个奶酪 App


137
00:04:40,596 --> 00:04:42,766
作为这个 App 的一部分


138
00:04:42,766 --> 00:04:44,246
用户可以做很多事情


139
00:04:44,536 --> 00:04:46,136
他们可以写关于奶酪的笔记


140
00:04:46,416 --> 00:04:47,946
他们可以写评价


141
00:04:47,946 --> 00:04:49,756
发表对不同奶酪的看法


142
00:04:50,176 --> 00:04:51,496
尽管这个 App 


143
00:04:51,496 --> 00:04:53,156
是关于奶酪的


144
00:04:53,156 --> 00:04:56,276
但它并不油腻 它处理的是精细的奶酪


145
00:04:56,656 --> 00:04:57,846
我将要为大家展示的是


146
00:04:57,846 --> 00:05:00,646
一个用户在写评价


147
00:05:01,066 --> 00:05:03,006
在他写评价时


148
00:05:03,006 --> 00:05:05,036
文本传送到


149
00:05:05,036 --> 00:05:06,286
情感分类 API


150
00:05:06,286 --> 00:05:09,246
我们得到一个分值


151
00:05:09,316 --> 00:05:11,416
根据情感分值给文本上色


152
00:05:11,726 --> 00:05:13,646
我们看 如果你键入类似


153
00:05:13,646 --> 00:05:15,496
很好 非常美味的评语


154
00:05:16,336 --> 00:05:19,016
你可以看到


155
00:05:19,016 --> 00:05:20,386
这是一个积极的情感


156
00:05:21,796 --> 00:05:24,886
相反 如果你键入


157
00:05:24,886 --> 00:05:28,306
入口还不错


158
00:05:28,306 --> 00:05:30,296
但是后味很糟糕


159
00:05:30,296 --> 00:05:32,006
可以看到这是个消极情感


160
00:05:32,276 --> 00:05:33,936
你可以看到


161
00:05:33,936 --> 00:05:36,386
这些都是实时发生的


162
00:05:36,746 --> 00:05:38,046
这是因为 API 


163
00:05:38,046 --> 00:05:39,156
性能非常出色


164
00:05:39,556 --> 00:05:41,126
它实际使用神经网络模型


165
00:05:41,126 --> 00:05:43,206
所有 Apple 平台上


166
00:05:43,206 --> 00:05:44,516
都激活了硬件


167
00:05:44,516 --> 00:05:46,336
所以基本上


168
00:05:46,336 --> 00:05:47,616
你可以实时做这些


169
00:05:48,756 --> 00:05:49,846
我们支持情感分析 API


170
00:05:49,846 --> 00:05:51,936
用于七种不同的语言


171
00:05:51,936 --> 00:05:53,966
英语 法语


172
00:05:54,016 --> 00:05:55,596
意大利语 德语 西班牙语


173
00:05:55,636 --> 00:05:57,646
葡萄牙语和简体中文


174
00:05:58,176 --> 00:05:59,436
我觉得你们肯定会喜欢这个


175
00:06:00,516 --> 00:06:06,546
[掌声]


176
00:06:07,046 --> 00:06:08,126
当然 所有的这些


177
00:06:08,126 --> 00:06:09,656
都全部发生在设备上


178
00:06:09,656 --> 00:06:11,536
用户数据不需要离开设备


179
00:06:11,896 --> 00:06:13,786
设备本身就能提供这项强大的功能


180
00:06:14,756 --> 00:06:16,306
我想简短地讨论一下


181
00:06:16,306 --> 00:06:17,926
语言素材


182
00:06:18,296 --> 00:06:19,746
我刚才已经提及 


183
00:06:19,746 --> 00:06:21,246
NLP 功能是多种多样的


184
00:06:21,896 --> 00:06:23,476
支持多种


185
00:06:23,476 --> 00:06:24,646
不同的语言


186
00:06:25,176 --> 00:06:26,186
现在 对于用户


187
00:06:26,186 --> 00:06:28,006
我们确保他们能够拥有


188
00:06:28,006 --> 00:06:29,986
自己感兴趣的


189
00:06:29,986 --> 00:06:31,286
语言的素材


190
00:06:31,856 --> 00:06:33,316
但是对于大家来说 


191
00:06:33,316 --> 00:06:34,686
出于开发的目的 


192
00:06:34,686 --> 00:06:36,846
可能对按需素材更感兴趣


193
00:06:36,846 --> 00:06:39,286
实际上 这是大家的一个普遍请求


194
00:06:39,286 --> 00:06:40,716
所以我们要介绍一个


195
00:06:40,716 --> 00:06:42,746
新的便捷 API


196
00:06:42,746 --> 00:06:44,236
叫做请求素材


197
00:06:44,686 --> 00:06:46,076
你可以按照自己的需求


198
00:06:46,076 --> 00:06:49,216
触发一个特定素材的下载


199
00:06:49,436 --> 00:06:50,896
你只需指定


200
00:06:50,896 --> 00:06:51,836
自己喜欢的


201
00:06:51,836 --> 00:06:53,306
语言和标签方案


202
00:06:53,306 --> 00:06:55,186
我们会在后台复刻一个下载


203
00:06:55,276 --> 00:06:56,476
然后你就可以


204
00:06:56,476 --> 00:06:57,736
及时在设备上获取素材


205
00:06:57,736 --> 00:07:00,226
这会帮助你开发 App 


206
00:07:00,226 --> 00:07:01,866
并提高你


207
00:07:01,866 --> 00:07:03,476
搭建 App 的效率


208
00:07:05,156 --> 00:07:07,226
我刚才介绍的是文本分类


209
00:07:07,226 --> 00:07:09,856
现在我们进入单词标记部分


210
00:07:10,956 --> 00:07:12,486
回顾一下


211
00:07:12,486 --> 00:07:14,616
单词标记是一个任务


212
00:07:14,616 --> 00:07:16,116
给定一系列 token  


213
00:07:16,116 --> 00:07:17,576
我们想要给序列中每一个 token


214
00:07:17,576 --> 00:07:18,896
分配标签


215
00:07:19,086 --> 00:07:21,776
就像这个例子


216
00:07:21,856 --> 00:07:23,056
我们可以给许多 token


217
00:07:23,056 --> 00:07:24,246
分配不同的标签


218
00:07:24,376 --> 00:07:25,546
Timothy 是一个人名


219
00:07:25,546 --> 00:07:27,326
瑞士是一个地方


220
00:07:27,326 --> 00:07:29,366
这句话里还有很多名词


221
00:07:30,466 --> 00:07:31,226
很好


222
00:07:31,226 --> 00:07:32,736
如果你只是想用我们的 API


223
00:07:32,736 --> 00:07:36,346
做命名实体识别


224
00:07:36,346 --> 00:07:39,616
或用 API 做词性标注也可以  


225
00:07:39,616 --> 00:07:43,096
但这里还有几个例子 你可以做更适合你任务的东西


226
00:07:43,826 --> 00:07:46,456
你想知道的不仅是格吕耶尔 干酪 是两个名词


227
00:07:46,816 --> 00:07:48,296
你还想知道


228
00:07:48,296 --> 00:07:50,426
它是种瑞士奶酪 我们在搭建一个奶酪 App


229
00:07:50,426 --> 00:07:53,136
你当然想得到这个信息


230
00:07:53,856 --> 00:07:55,626
但是 默认 tagger 


231
00:07:55,626 --> 00:07:56,696
不包含任何关于奶酪的信息


232
00:07:56,756 --> 00:07:57,966
那我们要如何


233
00:07:57,966 --> 00:07:58,846
提供这个信息呢


234
00:07:59,446 --> 00:08:00,926
所以自然语言框架里


235
00:08:01,336 --> 00:08:02,526
有一个新功能


236
00:08:02,526 --> 00:08:04,366
我们称之为文本目录


237
00:08:05,646 --> 00:08:07,326
文本目录非常简单


238
00:08:07,866 --> 00:08:09,446
你只需要提供


239
00:08:09,446 --> 00:08:11,446
一个自定义列表


240
00:08:11,446 --> 00:08:13,036
可能是一个非常大的实体列表


241
00:08:13,276 --> 00:08:14,546
列表中的每一个实体


242
00:08:14,546 --> 00:08:15,666
都有一个标签


243
00:08:16,556 --> 00:08:18,476
实际情况下


244
00:08:18,476 --> 00:08:20,026
这些列表可能是数百万


245
00:08:20,026 --> 00:08:21,046
甚至是几亿


246
00:08:21,856 --> 00:08:23,416
你需要做的是


247
00:08:23,676 --> 00:08:25,646
把这种词库传送到 Create ML


248
00:08:25,646 --> 00:08:27,596
创建一个 MLGazetteer 实例


249
00:08:27,596 --> 00:08:29,316
Gazetteer 只是


250
00:08:29,316 --> 00:08:30,746
文本目录的术语


251
00:08:31,026 --> 00:08:32,196
两个名称可以混用


252
00:08:32,196 --> 00:08:34,015
你得到的输出就是


253
00:08:34,015 --> 00:08:35,436
文本目录


254
00:08:35,785 --> 00:08:37,145
这是输入词库的


255
00:08:37,216 --> 00:08:40,456
非常精简和有效的形式


256
00:08:41,015 --> 00:08:43,736
非常简单


257
00:08:43,856 --> 00:08:45,486
你要做的就是先提供这个词库


258
00:08:45,486 --> 00:08:46,956
我们不能在这里


259
00:08:46,956 --> 00:08:48,066
展现数百万的实例


260
00:08:48,066 --> 00:08:49,166
我们只能用几个实例作例子


261
00:08:49,166 --> 00:08:50,546
但是它可以是


262
00:08:50,546 --> 00:08:52,086
一个非常非常大的词库


263
00:08:53,406 --> 00:08:54,916
然后你就可以


264
00:08:54,916 --> 00:08:56,946
创建 MLGazetteer 实例


265
00:08:56,946 --> 00:08:59,326
传送词库 将它写到磁盘


266
00:08:59,776 --> 00:09:01,026
这一切看起来都没有什么危险 


267
00:09:01,026 --> 00:09:02,386
你可能在想


268
00:09:02,386 --> 00:09:03,386
我只是把词库写到磁盘


269
00:09:03,386 --> 00:09:04,236
这是在做什么


270
00:09:05,176 --> 00:09:06,866
如果你的调用正确


271
00:09:06,866 --> 00:09:07,896
会发生一些神奇的事情


272
00:09:08,516 --> 00:09:10,536
Create ML 在内部调用自然语言


273
00:09:10,596 --> 00:09:12,246
自然语言会把这个


274
00:09:12,246 --> 00:09:13,916
非常大的词库


275
00:09:13,956 --> 00:09:15,566
压缩到一个 


276
00:09:15,706 --> 00:09:17,306
Bloom 过滤器


277
00:09:17,306 --> 00:09:18,286
这是一种非常紧凑的形式


278
00:09:18,286 --> 00:09:19,946
那么你得到的输出


279
00:09:19,946 --> 00:09:21,586
就是文本目录


280
00:09:22,426 --> 00:09:24,046
实际上 我们已经用了这个方法


281
00:09:24,046 --> 00:09:25,146
并达到了效果


282
00:09:25,146 --> 00:09:27,466
我们已经压缩了


283
00:09:27,876 --> 00:09:29,166
维基百科几乎所有的


284
00:09:29,166 --> 00:09:30,986
人名 机构名称 位置 


285
00:09:30,986 --> 00:09:32,536
差不多有 250 万个


286
00:09:32,536 --> 00:09:33,916
压缩到


287
00:09:34,306 --> 00:09:35,726
磁盘上的 2 兆


288
00:09:36,136 --> 00:09:40,136
在某种程度上 你一直在使用这个模型


289
00:09:40,356 --> 00:09:41,466
当你在 NaturalLanguage 里


290
00:09:41,466 --> 00:09:42,476
结合统计模型


291
00:09:42,476 --> 00:09:44,306
调用命名实例识别 API 时


292
00:09:44,306 --> 00:09:47,146
你就在用这个 Bloom 过滤器


293
00:09:47,146 --> 00:09:48,636
和 Gazetteer


294
00:09:48,636 --> 00:09:51,816
现在我们把这个能力给了你


295
00:09:51,896 --> 00:09:53,906
一旦你创建了 Gazetteer 


296
00:09:53,906 --> 00:09:57,276
或文本目录 用起来就会特别简单


297
00:09:58,116 --> 00:09:59,666
通过指定文本目录的路径 


298
00:09:59,696 --> 00:10:01,376
刚才写到磁盘上的文本目录


299
00:10:01,376 --> 00:10:03,576
你创建了一个 


300
00:10:03,576 --> 00:10:06,616
MLGazetteer 实例


301
00:10:06,616 --> 00:10:08,706
你可以在这里用自己喜欢的标签方案


302
00:10:08,706 --> 00:10:09,826
可以是词汇类 名称类型


303
00:10:09,826 --> 00:10:11,246
任何标签方案都可以  


304
00:10:11,246 --> 00:10:12,836
只需要把 Gazetteer


305
00:10:12,836 --> 00:10:14,756
附加到标签方案


306
00:10:15,636 --> 00:10:17,696
然后 每当有一个文本  


307
00:10:17,696 --> 00:10:19,456
这个自定义的 Gazetteer


308
00:10:19,456 --> 00:10:23,916
会覆盖 NaturalLanguage 提供的默认标签


309
00:10:25,246 --> 00:10:27,526
这样 你就可以自定义你的 App


310
00:10:28,646 --> 00:10:29,456
现在 返回奶酪 App


311
00:10:29,486 --> 00:10:30,876
如果有一句话


312
00:10:30,876 --> 00:10:32,626
比如 比卡芒贝尔奶酪


313
00:10:32,626 --> 00:10:34,196
或牛乳奶酪更淡


314
00:10:34,196 --> 00:10:36,796
你可以使用奶酪的文本目录


315
00:10:36,846 --> 00:10:39,956
识别一个是法国奶酪


316
00:10:39,956 --> 00:10:41,116
另一个是瑞士奶酪


317
00:10:41,116 --> 00:10:42,596
你也许可以创建一个超链接


318
00:10:42,596 --> 00:10:44,076
通过这种方式 


319
00:10:44,076 --> 00:10:46,116
制作一个更好的 App


320
00:10:46,586 --> 00:10:49,306
这是一种在 NaturalLanguage 里


321
00:10:49,306 --> 00:10:51,686
使用文字目录


322
00:10:51,686 --> 00:10:53,066
标注单词的方法


323
00:10:53,626 --> 00:10:57,246
我刚才介绍了文本分类


324
00:10:57,246 --> 00:10:58,676
介绍了单词标注


325
00:10:59,356 --> 00:11:01,086
但是近几年 NLP 领域


326
00:11:01,086 --> 00:11:02,846
发生了巨大的变化


327
00:11:02,846 --> 00:11:04,676
有两个催化剂


328
00:11:04,676 --> 00:11:06,586
促成了这一变化 


329
00:11:07,436 --> 00:11:08,916
一是单词嵌入概念


330
00:11:08,916 --> 00:11:12,006
单词嵌入只是单词的


331
00:11:12,006 --> 00:11:13,206
向量表示


332
00:11:13,206 --> 00:11:15,876
另一个是 NLP 里


333
00:11:15,876 --> 00:11:17,986
神经网络的使用


334
00:11:19,056 --> 00:11:21,056
我很高兴地宣布


335
00:11:21,386 --> 00:11:22,576
今年你可以


336
00:11:22,576 --> 00:11:24,056
通过 NaturalLanguage


337
00:11:24,056 --> 00:11:25,366
使用这些功能搭建 App


338
00:11:25,946 --> 00:11:30,166
我们先从单词嵌入开始 谢谢


339
00:11:31,596 --> 00:11:33,606
在我们进入


340
00:11:33,606 --> 00:11:34,996
单词嵌入部分之前


341
00:11:34,996 --> 00:11:36,426
我想通过几张幻灯片


342
00:11:36,426 --> 00:11:37,726
解释一下什么是嵌入


343
00:11:37,726 --> 00:11:40,336
理论层面上 


344
00:11:40,336 --> 00:11:43,946
嵌入只不过是把离散对象集合


345
00:11:43,946 --> 00:11:45,376
映射到持续向量表示


346
00:11:46,046 --> 00:11:48,026
我们有这些离散对象


347
00:11:48,466 --> 00:11:50,356
这个集合里的每个对象


348
00:11:50,356 --> 00:11:52,536
可以用有限向量表示


349
00:11:52,536 --> 00:11:54,176
在这个例子中 我们是用 


350
00:11:54,176 --> 00:11:55,466
3D 向量展示的


351
00:11:56,146 --> 00:11:57,096
用 3D 是因为  


352
00:11:57,146 --> 00:11:59,236
设计和看起来比较容易


353
00:11:59,236 --> 00:12:01,176
但是实际上 这些向量


354
00:12:01,176 --> 00:12:02,476
可以是任意维度 


355
00:12:02,786 --> 00:12:04,556
可以是 100D 300D


356
00:12:04,556 --> 00:12:05,966
甚至在某些情况下


357
00:12:06,266 --> 00:12:07,656
是 1000D 向量


358
00:12:08,756 --> 00:12:10,146
这些嵌入的特性是非常条理


359
00:12:10,146 --> 00:12:11,926
当你要设计


360
00:12:11,926 --> 00:12:13,326
这些嵌入时


361
00:12:14,176 --> 00:12:15,456
语义相似的对象


362
00:12:15,556 --> 00:12:17,316
会聚在一起


363
00:12:18,446 --> 00:12:19,976
在这个例子中


364
00:12:19,976 --> 00:12:21,306
油漆罐和油漆刷


365
00:12:21,306 --> 00:12:23,246
聚在了一起


366
00:12:24,376 --> 00:12:28,276
运动鞋 和高跟鞋聚在了一起


367
00:12:28,716 --> 00:12:30,896
所以 嵌入的特性非常条理


368
00:12:31,096 --> 00:12:33,086
嵌入的这个特性


369
00:12:33,086 --> 00:12:34,806
不仅可以用于单词


370
00:12:34,806 --> 00:12:36,046
实际上还可用于


371
00:12:36,046 --> 00:12:37,126
不同的形式


372
00:12:37,646 --> 00:12:39,066
可以是图片嵌入


373
00:12:39,166 --> 00:12:40,826
当你有一张图片 


374
00:12:40,826 --> 00:12:42,876
通过 VGG 网络


375
00:12:42,876 --> 00:12:43,946
或者任何卷积神经网络传输时


376
00:12:43,946 --> 00:12:45,456
你得到的输出


377
00:12:45,456 --> 00:12:48,046
就是这个特性图像嵌入


378
00:12:48,986 --> 00:12:51,696
同样 你可以嵌入单词 短语 


379
00:12:51,696 --> 00:12:54,016
当你在做推荐系统时


380
00:12:54,016 --> 00:12:55,676
里面有歌曲名 


381
00:12:55,676 --> 00:12:57,136
或者产品名


382
00:12:57,136 --> 00:13:00,076
他们都是通过向量表示的


383
00:13:00,526 --> 00:13:02,066
所以 它们只是嵌入


384
00:13:02,926 --> 00:13:04,326
总的来说


385
00:13:04,326 --> 00:13:06,506
嵌入只是将字符串  


386
00:13:06,506 --> 00:13:08,376
映射到持续的数字序列


387
00:13:08,376 --> 00:13:10,556
或者数字向量


388
00:13:11,076 --> 00:13:15,026
我们已经在 iOS 12 里


389
00:13:15,066 --> 00:13:16,836
非常成功地使用了这些嵌入


390
00:13:16,836 --> 00:13:18,656
我将向大家介绍


391
00:13:18,656 --> 00:13:19,836
如何在照片中使用嵌入


392
00:13:21,226 --> 00:13:23,236
在照片搜索中


393
00:13:23,316 --> 00:13:24,426
当你输入一个想查找的词时


394
00:13:24,426 --> 00:13:26,366
比如说 雷雨的照片


395
00:13:26,416 --> 00:13:28,506
在屏幕下面


396
00:13:28,506 --> 00:13:29,936
照片库里的所有照片


397
00:13:29,936 --> 00:13:33,626
都通过卷积神经网络编了索引


398
00:13:33,626 --> 00:13:35,106
卷积神经网络的输出


399
00:13:35,106 --> 00:13:36,466
固定在一定数量的类上


400
00:13:36,466 --> 00:13:37,696
可能是 1000 个类


401
00:13:37,696 --> 00:13:40,556
也可能是 2000 个类


402
00:13:41,426 --> 00:13:42,406
如果你的卷积神经网络  


403
00:13:42,406 --> 00:13:43,666
不知道雷雨是什么 


404
00:13:43,666 --> 00:13:45,506
那么你永远无法找到


405
00:13:45,506 --> 00:13:46,716
雷雨的索引照片


406
00:13:46,716 --> 00:13:48,476
因为不知道雷雨这个单词


407
00:13:48,476 --> 00:13:50,396
但是因为有单词嵌入


408
00:13:50,396 --> 00:13:55,076
我们知道雷雨和天空多云相关


409
00:13:55,076 --> 00:13:56,986
这些标签


410
00:13:57,876 --> 00:13:59,996
你的卷积神经网络是明白的


411
00:14:00,386 --> 00:14:02,266
所以 在 iOS 12 中


412
00:14:02,266 --> 00:14:04,366
你可以使用单词嵌入


413
00:14:04,366 --> 00:14:07,076
在照片搜索中实现模糊搜索


414
00:14:07,706 --> 00:14:09,396
所以 通过单词嵌入  


415
00:14:09,666 --> 00:14:11,676
你可以找到


416
00:14:11,676 --> 00:14:12,876
自己想找的照片


417
00:14:12,876 --> 00:14:14,006
实际上 它可以应用到 


418
00:14:14,006 --> 00:14:15,216
所有的搜索 App


419
00:14:15,466 --> 00:14:16,716
如果你有一串字符 


420
00:14:16,716 --> 00:14:18,226
你想模糊搜索


421
00:14:18,226 --> 00:14:19,426
你可以关联原单词


422
00:14:19,426 --> 00:14:21,946
和与其相近的单词


423
00:14:22,656 --> 00:14:24,826
说到这儿 你可以用嵌入


424
00:14:24,826 --> 00:14:25,846
做些什么呢


425
00:14:26,676 --> 00:14:27,696
你可以用单词嵌入


426
00:14:27,696 --> 00:14:29,716
进行 4 项基本操作


427
00:14:30,556 --> 00:14:32,486
一是 你可以得到


428
00:14:32,716 --> 00:14:34,466
一个单词的向量


429
00:14:35,576 --> 00:14:37,486
二是 如果有两个单词


430
00:14:37,486 --> 00:14:38,716
你可以找到两个词的距离


431
00:14:39,116 --> 00:14:40,356
因为你可以查看 


432
00:14:40,356 --> 00:14:41,156
每个单词的


433
00:14:41,156 --> 00:14:42,356
对应向量


434
00:14:42,616 --> 00:14:44,446
比如说 猫和狗


435
00:14:44,446 --> 00:14:45,576
如果我想得到这两个词


436
00:14:45,576 --> 00:14:46,816
之间的距离


437
00:14:46,816 --> 00:14:47,866
那么这个距离


438
00:14:47,866 --> 00:14:48,996
应该挺近的


439
00:14:49,886 --> 00:14:51,976
但如果是 狗和靴子


440
00:14:51,976 --> 00:14:53,706
那么在语义场内 


441
00:14:53,706 --> 00:14:55,246
它们距离应该挺远


442
00:14:55,246 --> 00:14:57,366
那么你会得到更远一些的距离


443
00:14:58,546 --> 00:14:59,696
三是 你可以得到 


444
00:14:59,696 --> 00:15:01,086
和某个单词最相近的词


445
00:15:01,086 --> 00:15:02,916
这应该是到目前为止


446
00:15:02,916 --> 00:15:04,676
对于单词嵌入 


447
00:15:04,676 --> 00:15:06,106
最流行的用法


448
00:15:06,106 --> 00:15:07,516
我刚才展示的照片搜索 App


449
00:15:07,516 --> 00:15:10,236
正是在做这个


450
00:15:10,236 --> 00:15:12,936
寻找和某个单词最相近的单词


451
00:15:12,936 --> 00:15:15,836
最后一点是


452
00:15:15,836 --> 00:15:19,376
你可以得到一个向量的最近邻居


453
00:15:19,376 --> 00:15:20,776
假设你有一个句子


454
00:15:20,836 --> 00:15:22,116
句子里有好几个单词


455
00:15:22,116 --> 00:15:23,726
句子里每一个单词


456
00:15:23,726 --> 00:15:25,176
都可以得到一个单词嵌入


457
00:15:25,176 --> 00:15:26,276
你可以把它们


458
00:15:26,326 --> 00:15:27,516
总结起来


459
00:15:27,516 --> 00:15:28,886
这样你会得到一个新的向量


460
00:15:28,886 --> 00:15:31,036
有了这个向量


461
00:15:31,036 --> 00:15:32,466
你就可以得到


462
00:15:32,466 --> 00:15:33,266
所有和这个向量相近的单词


463
00:15:33,266 --> 00:15:35,576
这也是一种应用单词嵌入的方法


464
00:15:35,576 --> 00:15:38,186
单词嵌入内容很多 


465
00:15:38,186 --> 00:15:39,396
但是最重要的是   


466
00:15:39,396 --> 00:15:40,476
我们为你提供了这个功能


467
00:15:40,476 --> 00:15:42,436
你可以在 OS 上


468
00:15:42,436 --> 00:15:43,786
便捷地使用它


469
00:15:44,076 --> 00:15:45,326
很高兴告诉大家


470
00:15:45,456 --> 00:15:46,906
这些单词嵌入


471
00:15:46,906 --> 00:15:48,396
支持 7 种语言


472
00:15:48,396 --> 00:15:51,426
我刚才提到的所有功能


473
00:15:51,456 --> 00:15:53,366
都只需要一两行代码


474
00:15:53,496 --> 00:15:54,946
就可以实现


475
00:15:55,386 --> 00:15:56,686
单词嵌入支持 7 种语言


476
00:15:56,686 --> 00:15:58,406
英语 西班牙语


477
00:15:58,406 --> 00:15:59,686
法语 意大利语 德语


478
00:15:59,766 --> 00:16:01,706
葡萄牙语和简体中文


479
00:16:02,986 --> 00:16:04,146
非常棒


480
00:16:04,146 --> 00:16:05,546
OS 嵌入通常是在


481
00:16:05,626 --> 00:16:07,536
通用语料库上构建的


482
00:16:07,536 --> 00:16:09,866
文本数量庞大 有数亿单词


483
00:16:10,406 --> 00:16:11,936
所以他们对于


484
00:16:12,056 --> 00:16:14,126
某个词和其他词的关系


485
00:16:14,126 --> 00:16:15,156
有一个大致概念


486
00:16:15,876 --> 00:16:17,226
但是很多时候


487
00:16:17,226 --> 00:16:18,216
你想做一些更自定义的事


488
00:16:19,846 --> 00:16:21,016
也许你活跃在


489
00:16:21,216 --> 00:16:22,366
不同的领域


490
00:16:22,926 --> 00:16:25,566
医药领域 法律领域


491
00:16:25,566 --> 00:16:26,316
或金融领域


492
00:16:27,046 --> 00:16:28,286
如果你的领域各不相同


493
00:16:28,286 --> 00:16:30,316
那么你想在 App 中


494
00:16:30,316 --> 00:16:31,916
使用的词汇


495
00:16:31,916 --> 00:16:33,076
会非常不同


496
00:16:33,076 --> 00:16:34,806
也许你只是想  


497
00:16:34,806 --> 00:16:36,076
让一个 OS 不支持的语言


498
00:16:36,076 --> 00:16:37,426
实现单词嵌入


499
00:16:37,426 --> 00:16:40,886
你应该怎么做呢


500
00:16:40,886 --> 00:16:42,466
我们对此也有准备 


501
00:16:43,366 --> 00:16:45,786
你可以使用自定义单词嵌入


502
00:16:46,506 --> 00:16:47,246
对于熟悉单词嵌入


503
00:16:47,246 --> 00:16:50,166
见证这个领域发展的人


504
00:16:50,166 --> 00:16:51,856
有许多第三方工具


505
00:16:51,856 --> 00:16:53,576
可以训练自定义嵌入


506
00:16:53,576 --> 00:16:55,866
比如 word2vec GloVe fasttext


507
00:16:56,366 --> 00:16:57,906
所以 你可以用自己的文本


508
00:16:58,306 --> 00:16:59,666
也可以用自己在 


509
00:16:59,666 --> 00:17:01,726
Keras TensorFlow 或 PyTorch


510
00:17:01,726 --> 00:17:01,793
训练的自定义神经网络


511
00:17:01,793 --> 00:17:04,756
你可以通过原始文本


512
00:17:04,756 --> 00:17:06,756
创建自己的嵌入


513
00:17:06,756 --> 00:17:07,935
你也可以从任一网站


514
00:17:07,935 --> 00:17:09,256
下载它们训练好的


515
00:17:09,256 --> 00:17:10,526
单词嵌入


516
00:17:11,205 --> 00:17:14,185
问题是 你要下载的这些嵌入


517
00:17:14,185 --> 00:17:15,866
体积非常非常大


518
00:17:16,256 --> 00:17:18,306
1GB 或 2GB 那么大


519
00:17:18,695 --> 00:17:19,826
但是你想非常精简有效地


520
00:17:19,826 --> 00:17:20,896
在你的 App 中使用它们


521
00:17:20,896 --> 00:17:23,266
我们实现了这一点


522
00:17:23,266 --> 00:17:26,915
这些来自第三方 App 的嵌入


523
00:17:26,915 --> 00:17:28,406
体积非常大


524
00:17:28,406 --> 00:17:30,076
我们自动将它们


525
00:17:30,076 --> 00:17:32,296
压缩成非常紧凑的格式


526
00:17:32,296 --> 00:17:33,576
有了这个格式


527
00:17:33,746 --> 00:17:35,446
你就可以像用 OS 嵌入一样


528
00:17:35,446 --> 00:17:36,716
使用它们了


529
00:17:37,436 --> 00:17:38,666
接下来将由 Doug    


530
00:17:38,666 --> 00:17:40,206
为大家介绍


531
00:17:40,206 --> 00:17:41,746
如何使用 OS 嵌入


532
00:17:41,746 --> 00:17:42,836
和自定义嵌入


533
00:17:42,936 --> 00:17:45,666
他会给大家做个示范


534
00:17:45,666 --> 00:17:46,786
后半段的讲演交给他了


535
00:17:47,426 --> 00:17:48,066
轮到你了 Doug


536
00:17:49,516 --> 00:17:55,836
[掌声]


537
00:17:56,336 --> 00:17:57,036
>> 好的


538
00:17:57,036 --> 00:17:58,196
我将通过一个演示 App


539
00:17:58,196 --> 00:18:00,066
向大家展示运行起来 


540
00:18:00,066 --> 00:18:00,976
是什么样的


541
00:18:01,656 --> 00:18:02,986
首先 我写了个


542
00:18:02,986 --> 00:18:04,796
非常小的演示 App 


543
00:18:04,796 --> 00:18:06,496
帮助大家理解


544
00:18:06,496 --> 00:18:08,566
单词嵌入


545
00:18:08,606 --> 00:18:10,456
在这儿键入一个单词


546
00:18:10,456 --> 00:18:11,636
它会显示最近的邻居


547
00:18:11,636 --> 00:18:13,676
嵌入空间内 


548
00:18:13,676 --> 00:18:15,296
离那个单词最近的邻居


549
00:18:15,636 --> 00:18:16,826
我们先从英语开始


550
00:18:16,826 --> 00:18:19,456
使用内置 OS 单词嵌入 


551
00:18:19,806 --> 00:18:21,466
我键入一个单词 比如椅子


552
00:18:21,666 --> 00:18:23,076
可以看到  


553
00:18:23,076 --> 00:18:24,476
它的邻居都是


554
00:18:24,476 --> 00:18:25,886
意思和它相近的单词


555
00:18:25,886 --> 00:18:28,246
椅子 沙发 长榻 等等


556
00:18:28,246 --> 00:18:31,116
我也可以键入自行车


557
00:18:31,796 --> 00:18:33,466
最近的邻居是


558
00:18:33,466 --> 00:18:34,666
自行车 摩托车 等等


559
00:18:34,666 --> 00:18:36,166
这些单词意思


560
00:18:36,166 --> 00:18:39,596
和自行车相近


561
00:18:40,406 --> 00:18:42,576
我也可以键入 书 就能得到和书意思相近的词


562
00:18:43,026 --> 00:18:44,616
从这里 我们可以看出


563
00:18:44,936 --> 00:18:46,746
内置 OS 单词嵌入


564
00:18:46,746 --> 00:18:48,316
显示原本的  


565
00:18:48,716 --> 00:18:50,826
词义和语言


566
00:18:50,826 --> 00:18:54,736
并识别出该语言在


567
00:18:54,776 --> 00:18:59,206
通用文本里相近的意思


568
00:19:00,206 --> 00:19:02,596
当然 


569
00:19:02,596 --> 00:19:04,686
这里我最感兴趣的是


570
00:19:05,066 --> 00:19:06,216
这些嵌入 


571
00:19:06,216 --> 00:19:08,916
和奶酪有什么关系


572
00:19:08,916 --> 00:19:09,976
毕竟我们做的是


573
00:19:09,976 --> 00:19:10,726
奶酪 App


574
00:19:11,426 --> 00:19:13,246
所以 我们键入一个 奶酪 单词


575
00:19:14,556 --> 00:19:16,156
看一下这里


576
00:19:16,156 --> 00:19:18,996
你立刻就能看到


577
00:19:18,996 --> 00:19:21,446
内置嵌入知道奶酪是什么


578
00:19:21,446 --> 00:19:24,506
但是我很失望


579
00:19:24,966 --> 00:19:26,196
这些嵌入只知道奶酪 


580
00:19:26,196 --> 00:19:28,206
但不了解任何细节


581
00:19:29,406 --> 00:19:31,086
否则 他们就不会把这些特殊的奶酪


582
00:19:31,086 --> 00:19:32,796
和奶酪相关的东西


583
00:19:32,796 --> 00:19:34,086
放在一起


584
00:19:34,086 --> 00:19:35,436
它们不应该放在一起


585
00:19:36,236 --> 00:19:37,706
我想要的效果是 


586
00:19:37,706 --> 00:19:39,136
它能明白 


587
00:19:39,136 --> 00:19:40,486
奶酪之间的关系


588
00:19:40,776 --> 00:19:42,046
所以 我就趁机训练  


589
00:19:42,046 --> 00:19:44,316
我自己的自定义奶酪嵌入


590
00:19:44,316 --> 00:19:46,446
它会基于相似性


591
00:19:46,446 --> 00:19:48,016
把奶酪放在一起


592
00:19:48,936 --> 00:19:50,116
切换到自定义嵌入


593
00:19:51,436 --> 00:19:52,616
这些是自定义奶酪嵌入里的 


594
00:19:52,616 --> 00:19:54,716
切德干酪的邻居


595
00:19:54,966 --> 00:19:55,806
这样就好多了


596
00:19:56,586 --> 00:19:57,786
可以看到 它把 


597
00:19:57,786 --> 00:19:59,646
和切德干酪


598
00:20:00,216 --> 00:20:01,596
口感相似的奶酪


599
00:20:01,596 --> 00:20:03,446
放在了一起


600
00:20:03,446 --> 00:20:05,676
比如 兰开夏奶酪 格洛斯特硬干酪和柴郡白干酪


601
00:20:06,666 --> 00:20:07,826
这是我们可以用在


602
00:20:07,826 --> 00:20:09,356
App 里的东西


603
00:20:09,946 --> 00:20:10,896
现在 我们再来看一下


604
00:20:10,896 --> 00:20:12,886
奶酪 App 是什么样的


605
00:20:15,426 --> 00:20:16,476
我在这个奶酪 App 上 


606
00:20:16,476 --> 00:20:18,266
尝试了一些想法


607
00:20:18,266 --> 00:20:20,676
看一下现在它的样子


608
00:20:21,226 --> 00:20:22,276
当用户键入时


609
00:20:22,276 --> 00:20:23,406
首先我会得到 


610
00:20:23,406 --> 00:20:25,486
一个情感分值


611
00:20:25,486 --> 00:20:27,826
看一下这是否是一句


612
00:20:27,826 --> 00:20:30,066
积极情感的话


613
00:20:30,066 --> 00:20:31,446
如果是的话 


614
00:20:31,446 --> 00:20:34,146
我会用我的标注 


615
00:20:34,146 --> 00:20:35,436
检查一下这句话


616
00:20:35,436 --> 00:20:39,156
当然 也会用我们的


617
00:20:39,156 --> 00:20:41,776
自定义奶酪 Gazetteer


618
00:20:41,776 --> 00:20:44,656
查看这句话里有没有提到什么奶酪


619
00:20:45,356 --> 00:20:47,076
我在找有没有提及奶酪


620
00:20:47,126 --> 00:20:48,826
如果用户真的提及了


621
00:20:48,826 --> 00:20:50,736
我就会把这个名字  


622
00:20:50,736 --> 00:20:52,296
传送到自定义奶酪嵌入


623
00:20:52,296 --> 00:20:56,366
查找相关奶酪


624
00:20:57,426 --> 00:20:58,336
听起来挺好的吧


625
00:20:58,336 --> 00:21:01,386
我们试一下


626
00:21:01,656 --> 00:21:02,726
调出我们的奶酪 App


627
00:21:02,726 --> 00:21:08,816
去年我去荷兰旅游


628
00:21:08,816 --> 00:21:10,136
然后我爱上了


629
00:21:10,136 --> 00:21:11,796
荷兰奶酪


630
00:21:11,796 --> 00:21:16,986
我将把这个告诉我的 App


631
00:21:16,986 --> 00:21:19,616
这肯定是一句


632
00:21:19,616 --> 00:21:20,686
情感积极的话


633
00:21:20,686 --> 00:21:22,716
而且确实提及了


634
00:21:22,716 --> 00:21:24,006
一种特定的奶酪


635
00:21:24,416 --> 00:21:27,586
然后我的 App 


636
00:21:27,876 --> 00:21:29,336
就可以为我推荐


637
00:21:29,336 --> 00:21:31,216
和我刚才提到的奶酪


638
00:21:31,216 --> 00:21:32,296
相似的奶酪


639
00:21:33,376 --> 00:21:35,186
这展示了单词嵌入的功能


640
00:21:35,226 --> 00:21:36,946
不仅如此


641
00:21:36,946 --> 00:21:39,166
它还展示了自然的 多变的


642
00:21:39,166 --> 00:21:40,626
NaturalLanguage API


643
00:21:40,626 --> 00:21:43,566
是如何和 App 功能 


644
00:21:43,566 --> 00:21:44,906
结合到一起的


645
00:21:46,516 --> 00:21:50,736
[掌声]


646
00:21:51,236 --> 00:21:52,656
现在我们退回幻灯片


647
00:21:52,656 --> 00:21:54,976
我想要简单的回顾一下


648
00:21:54,976 --> 00:21:57,496
在 API 中 这个是什么样子的


649
00:21:58,466 --> 00:22:00,546
如果你想要用


650
00:22:00,666 --> 00:22:02,926
内置的 OS 单词嵌入


651
00:22:02,926 --> 00:22:04,996
非常简单 你要做的就是请求


652
00:22:05,206 --> 00:22:06,696
请求某个特定语言的


653
00:22:06,696 --> 00:22:07,876
单词嵌入


654
00:22:07,876 --> 00:22:08,346
我们就会给你


655
00:22:08,346 --> 00:22:10,976
一旦你有了这些 


656
00:22:10,976 --> 00:22:12,766
NLEmbedding 对象的其中一个


657
00:22:12,766 --> 00:22:14,106
你就可以用它做很多事情


658
00:22:14,536 --> 00:22:16,886
当然 你可以得到组件 向量组件


659
00:22:16,886 --> 00:22:19,856
和任何特定目录相对应


660
00:22:21,136 --> 00:22:22,476
你可以得到在嵌入空间中


661
00:22:22,476 --> 00:22:24,326
两个单词之间的距离


662
00:22:24,326 --> 00:22:26,436
是近还是远


663
00:22:27,346 --> 00:22:28,936
正如在奶酪 App 里所见


664
00:22:28,936 --> 00:22:30,676
你可以浏览 


665
00:22:30,676 --> 00:22:32,166
找到在这个嵌入空间内


666
00:22:32,596 --> 00:22:35,296
任何特定名称


667
00:22:35,426 --> 00:22:37,186
最近的邻居


668
00:22:37,796 --> 00:22:40,436
如果你想用自定义单词嵌入


669
00:22:40,436 --> 00:22:43,346
要创建它


670
00:22:43,346 --> 00:22:46,496
你可以用 Create ML


671
00:22:46,546 --> 00:22:48,486
当然你需要


672
00:22:48,486 --> 00:22:50,846
所有表示嵌入的向量


673
00:22:51,186 --> 00:22:52,596
我不能在这里 通过幻灯片 


674
00:22:52,596 --> 00:22:53,986
向大家展示所有的向量


675
00:22:53,986 --> 00:22:55,536
因为他们有 50


676
00:22:55,856 --> 00:22:57,736
或者 100 个组件那么长


677
00:22:57,736 --> 00:22:59,906
但是这儿有一个例子


678
00:22:59,906 --> 00:23:01,146
在实际操作中


679
00:23:01,176 --> 00:23:02,396
你可能从一个文件中引入它们


680
00:23:02,396 --> 00:23:04,486
使用各种 Create ML 工具


681
00:23:04,486 --> 00:23:06,656
从文件中


682
00:23:06,656 --> 00:23:07,746
加载数据


683
00:23:08,076 --> 00:23:12,476
然后你就可以从中


684
00:23:12,476 --> 00:23:15,336
创建一个单词嵌入对象


685
00:23:15,336 --> 00:23:16,376
将它写到磁盘上


686
00:23:16,376 --> 00:23:18,376
这么做时 发生了什么呢


687
00:23:19,036 --> 00:23:22,256
在实际操作中


688
00:23:22,256 --> 00:23:23,796
这些嵌入一般会比较大


689
00:23:23,796 --> 00:23:25,936
数百维


690
00:23:25,936 --> 00:23:27,206
乘以数千个条目


691
00:23:27,206 --> 00:23:30,066
体积会很大


692
00:23:30,066 --> 00:23:32,206
会占用很多磁盘空间


693
00:23:32,776 --> 00:23:35,286
搜索就会比较贵


694
00:23:35,856 --> 00:23:38,336
当你把它们  


695
00:23:38,336 --> 00:23:41,416
编译进单词嵌入对象时


696
00:23:41,416 --> 00:23:43,836
在内部 我们做的是


697
00:23:43,836 --> 00:23:46,346
用产品量化技术


698
00:23:46,346 --> 00:23:47,906
实现很高程度的压缩


699
00:23:47,906 --> 00:23:50,856
然后添加索引


700
00:23:50,856 --> 00:23:53,336
这样你就可以快速搜索


701
00:23:53,506 --> 00:23:55,456
最近的邻居


702
00:23:55,456 --> 00:23:56,926
就像我们的示范一样


703
00:23:57,606 --> 00:24:00,926
来试一下 我们有一些


704
00:24:00,926 --> 00:24:04,256
开源的 非常大的嵌入


705
00:24:04,446 --> 00:24:06,916
这些是 GloVe 和 fasttext 嵌入


706
00:24:06,916 --> 00:24:08,876
未压缩的格式


707
00:24:08,876 --> 00:24:11,046
有 1GB 或 2GB 大


708
00:24:11,686 --> 00:24:13,956
当我们把它压缩成 


709
00:24:13,956 --> 00:24:15,606
NL 压缩格式时


710
00:24:16,206 --> 00:24:17,736
它们只有几十兆


711
00:24:18,136 --> 00:24:19,086
只需要几毫秒


712
00:24:19,086 --> 00:24:20,146
你就可以搜索到


713
00:24:20,146 --> 00:24:21,176
离它们最近的邻居


714
00:24:23,056 --> 00:24:23,976
有个 Apple 的例子


715
00:24:24,516 --> 00:24:27,716
[掌声]


716
00:24:28,216 --> 00:24:30,146
有个 Apple 的例子


717
00:24:30,266 --> 00:24:32,406
Apple 做了许多播客


718
00:24:32,536 --> 00:24:34,516
我们和播客团队


719
00:24:34,516 --> 00:24:36,616
交流了一下


720
00:24:36,616 --> 00:24:38,226
他们有一个


721
00:24:38,226 --> 00:24:40,426
为播客制作的嵌入


722
00:24:40,426 --> 00:24:43,686
表示各种播客之间的相似性


723
00:24:44,346 --> 00:24:46,196
所以我们想试一下


724
00:24:46,226 --> 00:24:48,186
看看把这个嵌入


725
00:24:48,186 --> 00:24:49,886
做成 NL 嵌入格式


726
00:24:49,886 --> 00:24:51,826
会发生什么


727
00:24:52,456 --> 00:24:54,156
这个嵌入代表


728
00:24:54,156 --> 00:24:57,466
66,000 个不同的播客


729
00:24:57,466 --> 00:24:59,486
原格式有 167MB


730
00:24:59,526 --> 00:25:00,906
但是我们压缩了它


731
00:25:00,906 --> 00:25:02,976
现在只占 3MB


732
00:25:03,326 --> 00:25:05,716
NL 嵌入的功能就是


733
00:25:05,716 --> 00:25:08,246
添加这些嵌入


734
00:25:08,246 --> 00:25:10,376
在你的 App 里


735
00:25:11,036 --> 00:25:13,236
设备上使用它们


736
00:25:16,716 --> 00:25:19,716
好的 接下来我想换一个话题


737
00:25:19,716 --> 00:25:22,646
介绍另一个


738
00:25:22,646 --> 00:25:24,976
和单词嵌入相关的东西


739
00:25:24,976 --> 00:25:28,036
那就是文本分类的迁移学习


740
00:25:29,356 --> 00:25:31,886
我想先谈一下


741
00:25:31,886 --> 00:25:33,516
我们是如何


742
00:25:33,516 --> 00:25:37,936
训练文本分类的


743
00:25:39,176 --> 00:25:40,886
在我们训练文本分类时


744
00:25:40,886 --> 00:25:44,076
我们给他一系列


745
00:25:44,076 --> 00:25:47,096
各种类的例子


746
00:25:47,636 --> 00:25:50,826
把这些数据传送给 Create ML


747
00:25:50,826 --> 00:25:52,946
Create ML 会调用自然语言


748
00:25:53,506 --> 00:25:55,766
训练分类器


749
00:25:55,876 --> 00:25:58,136
生成一个 Core ML 模型


750
00:25:58,506 --> 00:26:00,686
我们希望


751
00:26:00,686 --> 00:26:04,256
这些例子会充分提供


752
00:26:04,576 --> 00:26:06,626
关于各类的信息


753
00:26:06,686 --> 00:26:08,546
这样模型就可以概括


754
00:26:08,546 --> 00:26:11,916
分类它没有


755
00:26:11,916 --> 00:26:12,926
见过的例子


756
00:26:13,406 --> 00:26:15,126
当然 去年


757
00:26:15,366 --> 00:26:17,786
这个功能已经上市了


758
00:26:17,786 --> 00:26:20,566
我们有训练这些模型的算法


759
00:26:20,566 --> 00:26:23,136
最出名的是


760
00:26:23,136 --> 00:26:24,846
我们的标准算法 


761
00:26:24,846 --> 00:26:26,926
我们称之为 maxEnt 算法


762
00:26:26,926 --> 00:26:27,966
它基于逻辑压缩


763
00:26:28,336 --> 00:26:30,816
快速 强大 有效


764
00:26:31,666 --> 00:26:35,126
但是有一个问题


765
00:26:35,126 --> 00:26:36,876
除了你给的


766
00:26:36,876 --> 00:26:39,076
训练资料外


767
00:26:39,076 --> 00:26:41,846
它不知道其他的东西


768
00:26:42,116 --> 00:26:45,566
这样你就要确保


769
00:26:45,566 --> 00:26:46,876
你给他的训练材料


770
00:26:46,876 --> 00:26:50,736
包括你希望 


771
00:26:50,736 --> 00:26:52,296
在实际操作中


772
00:26:52,296 --> 00:26:55,156
能看到的所有东西


773
00:26:55,386 --> 00:26:57,336
在某种意义上说 


774
00:26:57,336 --> 00:26:58,736
我们提供算法 比较简单


775
00:26:58,736 --> 00:27:00,136
比较困难的部分留给了你们


776
00:27:00,136 --> 00:27:03,096
那就是提供训练数据


777
00:27:03,716 --> 00:27:06,916
但是如果我们用


778
00:27:06,916 --> 00:27:09,216
已知的语言知识 


779
00:27:09,216 --> 00:27:11,386
结合我们提供的


780
00:27:12,316 --> 00:27:16,966
体积小一点的训练材料


781
00:27:16,966 --> 00:27:18,526
来训练模型


782
00:27:18,526 --> 00:27:21,366
这样不是很好吗？


783
00:27:21,366 --> 00:27:24,616
通过两者的结合


784
00:27:24,696 --> 00:27:26,886
它会理解更多的实例


785
00:27:26,886 --> 00:27:31,176
即便是训练材料


786
00:27:31,176 --> 00:27:33,046
没有那么多


787
00:27:34,426 --> 00:27:37,866
这就是迁移学习的目标


788
00:27:38,576 --> 00:27:41,036
这是 NLP 重点研究领域


789
00:27:41,036 --> 00:27:43,876
很高兴 我们已经


790
00:27:43,876 --> 00:27:45,786
找到了一个解决办法


791
00:27:46,136 --> 00:27:47,326
现在介绍给大家


792
00:27:48,286 --> 00:27:50,426
自然语言训练模型


793
00:27:50,426 --> 00:27:52,946
生成一个 Core ML 模型


794
00:27:53,326 --> 00:27:54,916
但是我们怎么结合


795
00:27:54,916 --> 00:27:57,696
已知的语言知识呢


796
00:27:58,026 --> 00:27:58,886
我们从哪儿得到这个知识呢


797
00:27:59,946 --> 00:28:06,376
单词嵌入提供了许多语言的知识


798
00:28:06,376 --> 00:28:07,656
尤其是 它们知道


799
00:28:07,656 --> 00:28:10,626
单词的许多意思


800
00:28:11,296 --> 00:28:13,996
我们的方法是


801
00:28:13,996 --> 00:28:16,026
有单词嵌入


802
00:28:16,026 --> 00:28:18,056
和你提供的训练材料


803
00:28:18,056 --> 00:28:19,466
把他们放入单词嵌入


804
00:28:19,466 --> 00:28:21,686
在此之上


805
00:28:21,686 --> 00:28:25,596
训练神经网络模型


806
00:28:26,096 --> 00:28:27,986
这样我们就得到了


807
00:28:27,986 --> 00:28:30,456
迁移学习文本分类模型


808
00:28:31,556 --> 00:28:32,686
看起来比较复杂


809
00:28:32,686 --> 00:28:34,716
但是如果你想用 


810
00:28:34,716 --> 00:28:37,156
你只需要请求


811
00:28:38,496 --> 00:28:42,416
在训练迁移学习模型时


812
00:28:42,416 --> 00:28:45,436
你只需要在算法规范里


813
00:28:45,436 --> 00:28:46,876
改变一个参数


814
00:28:47,216 --> 00:28:49,366
现在这里有几个选择


815
00:28:50,056 --> 00:28:52,776
首先 显而易见


816
00:28:52,776 --> 00:28:54,696
你可以用


817
00:28:54,696 --> 00:28:56,806
表示单词原意的


818
00:28:57,296 --> 00:29:00,486
内置 OS 单词嵌入


819
00:29:00,656 --> 00:29:02,696
如果你有一个


820
00:29:02,696 --> 00:29:03,936
自定义单词嵌入


821
00:29:03,936 --> 00:29:06,106
也可以用


822
00:29:06,306 --> 00:29:10,066
我们知道一个给定单词


823
00:29:10,066 --> 00:29:11,236
可以有不同的意思


824
00:29:11,236 --> 00:29:12,456
这个词的意思


825
00:29:12,456 --> 00:29:13,586
取决于上下文


826
00:29:14,016 --> 00:29:15,316
比如 Apple 在这两句话中


827
00:29:15,316 --> 00:29:16,896
意思完全不同


828
00:29:18,206 --> 00:29:20,586
我们希望


829
00:29:20,586 --> 00:29:22,356
用在迁移学习上的嵌入


830
00:29:22,356 --> 00:29:24,766
可以根据单词的


831
00:29:25,386 --> 00:29:28,816
意思和上下文


832
00:29:28,816 --> 00:29:31,226
给这些单词不同的值


833
00:29:31,226 --> 00:29:32,366
当然 普通的单词嵌入


834
00:29:32,366 --> 00:29:34,966
只是将单词映射到向量


835
00:29:34,966 --> 00:29:37,306
不管单词是什么意思


836
00:29:37,306 --> 00:29:39,036
它只能给出


837
00:29:39,036 --> 00:29:40,076
同样的值


838
00:29:41,466 --> 00:29:44,876
但我们做的是


839
00:29:44,876 --> 00:29:46,866
训练一个特殊嵌入


840
00:29:46,976 --> 00:29:50,356
让它根据单词的


841
00:29:50,356 --> 00:29:51,656
意思和上下文


842
00:29:51,656 --> 00:29:52,866
给出不同的值


843
00:29:53,446 --> 00:29:54,886
大家可以感受到


844
00:29:54,886 --> 00:29:56,086
这个领域发展的有多快


845
00:29:56,086 --> 00:29:57,456
这只是一年前


846
00:29:57,456 --> 00:30:00,166
我们在研究的东西


847
00:30:00,166 --> 00:30:03,156
现在就可以公布出来


848
00:30:03,376 --> 00:30:04,796
如果你想用


849
00:30:04,796 --> 00:30:06,546
你只需要请求


850
00:30:07,076 --> 00:30:08,956
指定一个动态嵌入


851
00:30:09,186 --> 00:30:10,426
这个动态嵌入


852
00:30:10,956 --> 00:30:13,366
会根据单词


853
00:30:13,366 --> 00:30:15,106
所处的上下文


854
00:30:15,106 --> 00:30:17,766
改变单词的值


855
00:30:17,766 --> 00:30:20,016
这是做文本分类的迁移学习


856
00:30:20,016 --> 00:30:22,296
非常强大的一个技术


857
00:30:23,546 --> 00:30:25,206
看一下示范


858
00:30:27,056 --> 00:30:30,716
好的 这里是一些


859
00:30:30,716 --> 00:30:32,636
用 Create ML 训练文本分类


860
00:30:32,676 --> 00:30:34,566
非常标准的代码


861
00:30:34,566 --> 00:30:37,236
和我即将训练的东西


862
00:30:37,236 --> 00:30:39,056
它基于一个数据集


863
00:30:39,056 --> 00:30:41,326
从一个名为 DBpedia 的


864
00:30:41,766 --> 00:30:43,836
开源百科全书中得到的


865
00:30:44,676 --> 00:30:46,486
它包含许多话题的


866
00:30:46,486 --> 00:30:47,426
简短词条


867
00:30:47,496 --> 00:30:49,456
一些是关于人


868
00:30:49,456 --> 00:30:51,096
艺术家 作家 


869
00:30:51,096 --> 00:30:52,616
植物 动物等等


870
00:30:52,616 --> 00:30:55,146
这里的任务是


871
00:30:55,146 --> 00:30:56,746
根据词条确定分类


872
00:30:56,876 --> 00:30:59,186
是一个人 还是一个作家


873
00:30:59,186 --> 00:31:01,886
或艺术家 等等


874
00:31:01,956 --> 00:31:04,186
有 14 种不同的类


875
00:31:04,186 --> 00:31:05,546
我想通过 


876
00:31:05,546 --> 00:31:07,036
200 个实例


877
00:31:07,036 --> 00:31:08,406
尝试训练分类器


878
00:31:08,906 --> 00:31:11,016
这是一个非常困难的任务


879
00:31:11,016 --> 00:31:13,766
我们用现有的 maxEnt 模型


880
00:31:13,766 --> 00:31:16,236
来尝试一下


881
00:31:17,356 --> 00:31:19,006
快速写入发送


882
00:31:19,436 --> 00:31:21,316
开始 结束了


883
00:31:22,776 --> 00:31:25,296
非常快 非常简单


884
00:31:25,296 --> 00:31:30,966
看一下测试集的表现情况


885
00:31:31,286 --> 00:31:34,116
77% 的准确率


886
00:31:35,116 --> 00:31:39,796
还可以 但是能更好吗


887
00:31:39,796 --> 00:31:41,756
稍微改动一下


888
00:31:41,756 --> 00:31:43,926
这里的代码


889
00:31:44,266 --> 00:31:46,856
不再使用 maxEnt 模型


890
00:31:46,856 --> 00:31:47,986
而是使用


891
00:31:47,986 --> 00:31:49,526
带有动态嵌入的迁移学习


892
00:31:49,526 --> 00:31:53,266
开始试一下


893
00:31:54,736 --> 00:31:56,186
正如我刚才所说


894
00:31:56,186 --> 00:31:57,956
这是在训练一个神经网络模型


895
00:31:58,136 --> 00:31:59,526
所以时间会久一点


896
00:32:00,256 --> 00:32:02,606
所以在它训练时


897
00:32:02,606 --> 00:32:04,286
我们可以细看一下


898
00:32:04,326 --> 00:32:05,916
正在训练的数据


899
00:32:06,326 --> 00:32:08,196
实际上


900
00:32:08,196 --> 00:32:09,646
在你训练神经网络模型时


901
00:32:09,916 --> 00:32:11,556
你需要关注


902
00:32:11,556 --> 00:32:14,216
训练使用的数据


903
00:32:14,596 --> 00:32:17,166
这个数据是


904
00:32:17,276 --> 00:32:19,386
跨多个类的随机实例


905
00:32:19,386 --> 00:32:22,646
我整理了一下


906
00:32:22,646 --> 00:32:23,846
所以每个类的


907
00:32:23,846 --> 00:32:26,676
实例数大致相同


908
00:32:27,186 --> 00:32:28,346
这是一个比较均衡的集


909
00:32:29,366 --> 00:32:30,626
我们的训练集


910
00:32:30,626 --> 00:32:31,926
另外我们还有一个


911
00:32:31,926 --> 00:32:34,146
单独的验证集


912
00:32:34,146 --> 00:32:36,226
也是跨类的随机实例


913
00:32:36,226 --> 00:32:38,546
也许没有训练集


914
00:32:38,546 --> 00:32:39,756
体积那么大


915
00:32:39,756 --> 00:32:41,296
但也是均衡的


916
00:32:41,986 --> 00:32:43,246
验证集在这种训练中


917
00:32:43,306 --> 00:32:45,536
尤为重要


918
00:32:45,996 --> 00:32:48,466
神经网络训练


919
00:32:48,516 --> 00:32:50,846
容易过度拟合


920
00:32:50,846 --> 00:32:52,666
会或多或少记住训练内容


921
00:32:52,666 --> 00:32:54,816
但不会概括总结


922
00:32:55,096 --> 00:32:56,616
验证集


923
00:32:56,616 --> 00:32:57,976
会确保它


924
00:32:57,976 --> 00:32:59,316
继续概括


925
00:33:00,146 --> 00:33:01,376
当然我们还有一个


926
00:33:01,376 --> 00:33:03,946
单独的测试集


927
00:33:03,946 --> 00:33:05,846
实例也是随机但均衡的


928
00:33:05,846 --> 00:33:10,196
当然 训练验证测试集之间


929
00:33:10,196 --> 00:33:12,006
没有重叠部分


930
00:33:12,006 --> 00:33:12,796
不然就是欺骗


931
00:33:13,966 --> 00:33:16,606
我们需要测试集


932
00:33:16,606 --> 00:33:17,576
查看我们在做什么


933
00:33:17,576 --> 00:33:19,586
尤其是在这个例子中


934
00:33:19,586 --> 00:33:22,296
有了测试集


935
00:33:22,296 --> 00:33:24,646
我们就知道迁移学习模型


936
00:33:24,646 --> 00:33:28,126
是不是比 maxEnt 模型好


937
00:33:28,596 --> 00:33:29,866
好像是结束了


938
00:33:29,866 --> 00:33:31,326
我们来看一下


939
00:33:31,606 --> 00:33:36,486
在这里我们可以看到


940
00:33:36,486 --> 00:33:38,576
迁移学习实现了


941
00:33:38,576 --> 00:33:41,106
86.5% 的正确率


942
00:33:41,186 --> 00:33:45,976
比 maxEnt 模型好很多


943
00:33:46,516 --> 00:33:51,796
[掌声]


944
00:33:52,296 --> 00:33:55,996
那么如何把这个应用到


945
00:33:55,996 --> 00:33:56,976
我们的奶酪 App 呢


946
00:33:57,056 --> 00:33:59,536
我已经有


947
00:33:59,576 --> 00:34:01,746
奶酪口味的笔记


948
00:34:03,146 --> 00:34:05,726
按它们提到的奶酪


949
00:34:05,756 --> 00:34:07,116
给每一个都标上了标签


950
00:34:07,396 --> 00:34:09,626
我将用这个


951
00:34:09,626 --> 00:34:10,676
训练分类器模型


952
00:34:10,676 --> 00:34:13,196
我的奶酪分类器模型


953
00:34:13,196 --> 00:34:16,146
会拿到一个句子


954
00:34:16,246 --> 00:34:18,216
尝试对他进行分类


955
00:34:18,216 --> 00:34:20,866
找到它最接近的是哪种奶酪


956
00:34:21,016 --> 00:34:24,036
把它放入我的奶酪 App


957
00:34:24,036 --> 00:34:28,826
我将在这个奶酪 App 里做的是


958
00:34:28,826 --> 00:34:32,366
如果用户


959
00:34:32,366 --> 00:34:33,545
没有特别提到一种奶酪


960
00:34:33,585 --> 00:34:34,936
那么我需要尝试找出


961
00:34:34,936 --> 00:34:36,746
他们想要哪种奶酪


962
00:34:37,255 --> 00:34:39,456
我要做的只是向模型请求


963
00:34:39,456 --> 00:34:43,226
文本的标签非常简单


964
00:34:43,226 --> 00:34:45,000
我们来试一下


965
00:35:01,046 --> 00:35:03,000
在这里输入一句话


966
00:35:15,486 --> 00:35:20,966
然后让奶酪器分类解析它


967
00:35:21,516 --> 00:35:23,536
奶酪分类器判定


968
00:35:23,696 --> 00:35:24,996
我想要的最接近


969
00:35:25,046 --> 00:35:26,716
卡芒贝尔奶酪


970
00:35:27,066 --> 00:35:29,066
然后我的奶酪嵌入


971
00:35:29,066 --> 00:35:30,516
会推荐一些


972
00:35:30,516 --> 00:35:31,936
其他相似的奶酪


973
00:35:31,936 --> 00:35:32,756
布里奶酪等等


974
00:35:34,156 --> 00:35:36,000
我也可以输入


975
00:35:45,386 --> 00:35:46,826
质地结实的奶酪


976
00:35:46,826 --> 00:35:47,946
它认定为是


977
00:35:48,286 --> 00:35:49,826
切达奶酪


978
00:35:50,036 --> 00:35:53,001
并推荐了一些相似的奶酪 [掌声]


979
00:35:53,036 --> 00:35:54,386
这向我们展示了


980
00:35:54,426 --> 00:35:57,596
文本分类结合其他


981
00:35:57,596 --> 00:35:59,746
NaturalLanguage API 的


982
00:35:59,956 --> 00:36:01,486
强大功能


983
00:36:01,736 --> 00:36:03,276
最后我将说明一些


984
00:36:03,276 --> 00:36:05,366
使用文本分类


985
00:36:05,456 --> 00:36:07,656
必须要考虑的问题


986
00:36:07,656 --> 00:36:09,266
首先我们要注意


987
00:36:09,266 --> 00:36:10,756
哪些语言支持迁移学习


988
00:36:11,166 --> 00:36:12,846
考虑上下文


989
00:36:12,846 --> 00:36:14,956
不论是通过静态嵌入


990
00:36:14,956 --> 00:36:17,746
还是通过


991
00:36:17,886 --> 00:36:19,576
动态嵌入


992
00:36:20,026 --> 00:36:23,196
然后我想再说一下


993
00:36:23,196 --> 00:36:25,056
数据方面的问题


994
00:36:26,326 --> 00:36:28,446
处理数据的


995
00:36:28,446 --> 00:36:29,626
第一要求是


996
00:36:29,626 --> 00:36:31,676
你必须要了解


997
00:36:31,676 --> 00:36:32,296
自己的领域


998
00:36:33,506 --> 00:36:35,036
在实际操作中


999
00:36:35,036 --> 00:36:36,176
你将遇到什么样的文本


1000
00:36:36,396 --> 00:36:37,626
是句子片段


1001
00:36:37,626 --> 00:36:39,086
还是完整的句子


1002
00:36:39,086 --> 00:36:41,556
还是多个句子


1003
00:36:41,556 --> 00:36:43,576
确保你的训练数据


1004
00:36:43,576 --> 00:36:45,526
和在实际情况中


1005
00:36:45,526 --> 00:36:47,106
可能遇到 需要分类的数据


1006
00:36:47,106 --> 00:36:48,576
尽量的相似


1007
00:36:49,646 --> 00:36:52,026
并且尽量全面地包括


1008
00:36:52,286 --> 00:36:53,646
你在 App 中 


1009
00:36:53,646 --> 00:36:55,866
可能遇到的


1010
00:36:55,866 --> 00:36:57,776
文本的变体


1011
00:36:58,906 --> 00:37:02,746
就像刚才在 DBpedia 例子中


1012
00:37:02,746 --> 00:37:04,806
看到的那样


1013
00:37:05,266 --> 00:37:08,806
你要确保实例


1014
00:37:08,806 --> 00:37:11,316
尽量是随机的


1015
00:37:11,316 --> 00:37:13,606
训练集 验证集 测试集


1016
00:37:13,606 --> 00:37:15,266
尽量不同


1017
00:37:15,766 --> 00:37:17,696
这是基本的数据要求


1018
00:37:18,096 --> 00:37:21,706
如何知道


1019
00:37:21,706 --> 00:37:23,916
哪个算法最适合你呢 


1020
00:37:24,336 --> 00:37:25,616
普遍来讲 你需要尝试


1021
00:37:25,616 --> 00:37:28,426
但是也有一些参考


1022
00:37:28,626 --> 00:37:30,466
你可以先尝试 maxEnt 分类器


1023
00:37:30,466 --> 00:37:31,456
它速度很快


1024
00:37:31,456 --> 00:37:32,606
它会给你一个答案


1025
00:37:33,766 --> 00:37:36,356
但是 maxEnt 分类器能做什么呢


1026
00:37:36,896 --> 00:37:38,926
maxEnt 分类器


1027
00:37:38,926 --> 00:37:41,556
可以识别训练材料中


1028
00:37:42,146 --> 00:37:44,556
最常出现的词


1029
00:37:45,096 --> 00:37:46,806
比如说


1030
00:37:46,806 --> 00:37:49,756
你想训练识别积极和消极情感


1031
00:37:49,756 --> 00:37:51,486
它可能会注意到


1032
00:37:51,486 --> 00:37:52,676
爱和幸福是积极的


1033
00:37:52,736 --> 00:37:54,666
恨和不开心是消极的


1034
00:37:55,326 --> 00:37:57,406
如果实际使用中


1035
00:37:57,406 --> 00:37:59,106
遇到了这些单词


1036
00:37:59,106 --> 00:38:00,426
那么 maxEnt 分类器


1037
00:38:00,426 --> 00:38:02,146
就会做得很好


1038
00:38:02,676 --> 00:38:08,146
迁移学习做了什么呢


1039
00:38:08,456 --> 00:38:10,546
它注意的是不是单词的意思呢


1040
00:38:11,056 --> 00:38:12,906
如果在实际操作中


1041
00:38:12,906 --> 00:38:14,476
遇到了用不同的单词


1042
00:38:14,476 --> 00:38:16,926
表达相同意思的情况


1043
00:38:17,016 --> 00:38:20,426
这时迁移学习模型


1044
00:38:20,426 --> 00:38:21,786
就会大放异彩


1045
00:38:21,786 --> 00:38:24,476
它做的就会


1046
00:38:24,476 --> 00:38:27,546
比普通的 maxEnt 模型好


1047
00:38:28,046 --> 00:38:33,086
总的来说 我们有一些新的 API


1048
00:38:33,086 --> 00:38:35,676
可以用于情感分析


1049
00:38:35,676 --> 00:38:38,706
可以结合 MLGazetteer 用于文本目录 


1050
00:38:38,706 --> 00:38:41,906
可以结合 NL 嵌入


1051
00:38:41,996 --> 00:38:45,376
用于单词嵌入


1052
00:38:45,376 --> 00:38:47,306
我们有一种新型的文本分类


1053
00:38:47,466 --> 00:38:49,046
因为可以用迁移学习


1054
00:38:49,716 --> 00:38:52,056
所以这个新类型特别强大


1055
00:38:53,096 --> 00:38:55,226
希望这些可以在你的 App 中


1056
00:38:55,226 --> 00:38:58,086
起到帮助作用


1057
00:38:58,426 --> 00:39:01,496
线上还有更多的信息


1058
00:39:01,496 --> 00:39:04,256
你也可以查看其他


1059
00:39:04,256 --> 00:39:05,256
相关的讲解


1060
00:39:06,356 --> 00:39:12,500
谢谢 [掌声]


1
00:00:01,516 --> 00:00:04,500
[音乐]


2
00:00:08,516 --> 00:00:17,546
[掌声]


3
00:00:18,046 --> 00:00:19,896
>> 大家好 欢迎各位


4
00:00:20,656 --> 00:00:22,786
我的名字是 Gaurav 今天


5
00:00:22,786 --> 00:00:24,306
我们要讲的是


6
00:00:24,396 --> 00:00:27,456
机器学习的新功能


7
00:00:29,696 --> 00:00:32,326
成千上万的 App 都在用机器学习


8
00:00:33,356 --> 00:00:36,436
你们正在制作的 App 非常棒


9
00:00:37,006 --> 00:00:39,846
他们涉及到了用户生活的方方面面


10
00:00:40,456 --> 00:00:45,086
医院里 医生正在使用


11
00:00:45,086 --> 00:00:47,326
诸如 Butterfly iQ 这样的 App


12
00:00:47,326 --> 00:00:52,206
来进行实时的医学诊断


13
00:00:52,416 --> 00:00:54,866
体育界 教练正在使用


14
00:00:54,866 --> 00:00:57,646
诸如 HomeCourt 这样的 App
来训练他们的运动员


15
00:00:58,576 --> 00:01:02,896
在创造力方面 诸如


16
00:01:02,896 --> 00:01:04,936
Pixelmator Pro 这样的 App 帮助其他


17
00:01:04,936 --> 00:01:07,686
用户使用 ML 来增强他们的创造性


18
00:01:08,746 --> 00:01:10,366
这只是其中一些例子


19
00:01:10,576 --> 00:01:12,446
我们想要你们制作的


20
00:01:12,446 --> 00:01:14,746
App 也能成为


21
00:01:14,746 --> 00:01:17,396
这种优秀的 App


22
00:01:17,656 --> 00:01:19,686
所以现在的问题是新功能是什么


23
00:01:20,556 --> 00:01:24,736
那我就开始讲了


24
00:01:25,576 --> 00:01:28,896
我们有很多的材料


25
00:01:28,896 --> 00:01:31,556
所以一次或者两次会议是讲不完的


26
00:01:32,126 --> 00:01:36,856
我们需要十次会议来讲解整个材料


27
00:01:38,316 --> 00:01:39,606
我们还有一个


28
00:01:39,776 --> 00:01:43,216
与机器学习中的设计相交叉的会议


29
00:01:44,556 --> 00:01:47,216
我们也有每日实验室


30
00:01:47,216 --> 00:01:49,496
你可以在这里和 Apple 机器学习


31
00:01:49,496 --> 00:01:51,746
工程师一起相约探讨你的想法


32
00:01:53,116 --> 00:01:55,156
当你在 App 中整合机器学习


33
00:01:55,156 --> 00:01:56,956
我们都在这为你清除任何


34
00:01:56,956 --> 00:02:00,326
你可能遇到的障碍


35
00:02:02,876 --> 00:02:04,776
在所有的会议中


36
00:02:04,776 --> 00:02:07,016
你将会看到 我们遵循简单的原则


37
00:02:08,356 --> 00:02:12,976
我们想让机器学习变得越简单越好


38
00:02:12,976 --> 00:02:15,106
我们正全神贯注于做此事


39
00:02:15,736 --> 00:02:17,766
我们希望使其更灵活


40
00:02:17,766 --> 00:02:20,466
以便你可以通过各种任务来完成它


41
00:02:21,436 --> 00:02:22,926
我们希望使其更强大


42
00:02:22,976 --> 00:02:25,506
以便你可以在你的设备上


43
00:02:25,506 --> 00:02:28,136
运行最先进的机器学习模型


44
00:02:28,876 --> 00:02:32,976
机器学习适用于每个人


45
00:02:32,976 --> 00:02:35,036
不管你是研究员


46
00:02:35,036 --> 00:02:37,286
还是一名机器学习的新手


47
00:02:38,776 --> 00:02:40,046
通过在实验室中创建


48
00:02:40,046 --> 00:02:41,236
你可以创建最先进的


49
00:02:41,236 --> 00:02:42,816
以任务为中心的机器学习模型


50
00:02:44,526 --> 00:02:47,286
我们产品的下一大支柱是 Domain API


51
00:02:49,256 --> 00:02:50,746
Domain API 允许你用


52
00:02:50,746 --> 00:02:53,346
Apple 的内置智能和模型


53
00:02:54,266 --> 00:02:55,506
所以你不用担心


54
00:02:55,506 --> 00:02:57,426
收集数据和建造模型了


55
00:02:57,646 --> 00:03:00,296
简单地调用 API 即可完成


56
00:03:02,696 --> 00:03:04,326
今年 我们正在大力


57
00:03:04,356 --> 00:03:05,806
扩展我们的 Domain API


58
00:03:06,256 --> 00:03:08,676
我们在视觉 文本 语音和声音中


59
00:03:09,106 --> 00:03:13,196
都有 Domain API


60
00:03:13,406 --> 00:03:15,226
现在先让我们来看看


61
00:03:15,276 --> 00:03:16,386
其中的一些 API 


62
00:03:17,146 --> 00:03:19,746
让我们先从视觉开始


63
00:03:20,976 --> 00:03:22,906
视觉允许你推断


64
00:03:23,016 --> 00:03:24,716
图像的内容


65
00:03:26,206 --> 00:03:27,386
今年的新特点之一是


66
00:03:27,386 --> 00:03:29,036
图像显著性


67
00:03:30,096 --> 00:03:31,576
图像显著性可以帮你


68
00:03:31,576 --> 00:03:33,536
识别图像中


69
00:03:33,536 --> 00:03:35,256
最相关的区域


70
00:03:36,106 --> 00:03:40,146
在这个例子中 就是人周围的区域


71
00:03:41,936 --> 00:03:43,056
你可以用它来生成


72
00:03:43,056 --> 00:03:44,996
缩略图或进行图像


73
00:03:44,996 --> 00:03:49,976
裁剪 生成记忆 引导相机等等


74
00:03:50,046 --> 00:03:53,786
今年我们要介绍的另外一个


75
00:03:53,786 --> 00:03:56,126
重要特点是文本识别


76
00:03:57,666 --> 00:03:59,516
现在你可以对文档


77
00:03:59,516 --> 00:04:01,476
进行拍照 执行


78
00:04:01,476 --> 00:04:03,556
透视校正 照明


79
00:04:03,556 --> 00:04:05,566
校正和重新组织


80
00:04:05,616 --> 00:04:06,976
设备上的文本


81
00:04:07,516 --> 00:04:13,986
[掌声]


82
00:04:14,486 --> 00:04:18,666
这是很成功的 但并不是全部


83
00:04:18,666 --> 00:04:19,796
我们有图像内置


84
00:04:19,796 --> 00:04:22,016
分类器 人体探测器 宠物


85
00:04:22,016 --> 00:04:24,036
探测器 我们将


86
00:04:24,036 --> 00:04:27,046
在两个视觉会议中介绍它们


87
00:04:31,036 --> 00:04:33,106
下一个领域是自然语言


88
00:04:33,656 --> 00:04:36,356
就像是你用视觉来


89
00:04:36,356 --> 00:04:38,376
推断图像 你可以用


90
00:04:38,376 --> 00:04:43,076
自然语言来推断文本


91
00:04:43,256 --> 00:04:46,166
今年的新内容是内置的情感分析


92
00:04:46,166 --> 00:04:48,626
所以你可以用一种


93
00:04:48,686 --> 00:04:50,616
尊重隐私的方式在设备上


94
00:04:50,616 --> 00:04:53,216
实时分析文本的情感


95
00:04:53,836 --> 00:04:55,296
所以 举例来说 如果有人


96
00:04:55,296 --> 00:04:56,486
输入这样的文本 我


97
00:04:56,566 --> 00:04:57,926
对第二季结局感到很兴奋


98
00:04:57,926 --> 00:04:59,406
这是一种积极的情感


99
00:05:00,256 --> 00:05:02,656
但是最后有点失落


100
00:05:02,656 --> 00:05:05,406
这又是一个消极的情感


101
00:05:06,056 --> 00:05:08,656
所以你可以实时


102
00:05:08,656 --> 00:05:10,006
提供这种反馈


103
00:05:10,526 --> 00:05:14,516
我们还首次


104
00:05:14,516 --> 00:05:17,386
公开了内置的词嵌入


105
00:05:18,606 --> 00:05:20,096
词嵌入可以让你


106
00:05:20,166 --> 00:05:21,666
找到语义相近的单词


107
00:05:21,666 --> 00:05:23,426
例如 雷暴这个词在语义上


108
00:05:23,426 --> 00:05:25,246
非常接近多云


109
00:05:25,246 --> 00:05:28,406
但是离鞋子和靴子相差甚远


110
00:05:28,896 --> 00:05:31,486
词嵌入的一个重要使用案例是语义搜索


111
00:05:31,486 --> 00:05:35,246
我们接下来给你举一个例子


112
00:05:35,456 --> 00:05:36,546
自然语言将提前在


113
00:05:36,546 --> 00:05:38,446
自然语言框架会议中


114
00:05:38,446 --> 00:05:40,086
进行详细讨论


115
00:05:40,636 --> 00:05:45,836
用户与 App 交互的第三种方式


116
00:05:45,836 --> 00:05:48,426
是通过语言和声音


117
00:05:49,566 --> 00:05:51,076
现在我们有了设备语音


118
00:05:51,106 --> 00:05:52,496
支持 所以你可以在设备上


119
00:05:52,546 --> 00:05:54,986
转录文本


120
00:05:54,986 --> 00:05:56,226
你再也不用依赖


121
00:05:56,226 --> 00:05:59,266
网络连接 我们也


122
00:05:59,266 --> 00:06:01,086
有新的语音分析 API


123
00:06:01,436 --> 00:06:03,066
它不仅可以告诉你讲话的内容


124
00:06:03,066 --> 00:06:05,926
还可以告诉你讲话的方式


125
00:06:05,926 --> 00:06:07,066
所以你可以区分


126
00:06:07,066 --> 00:06:08,486
一个正常的声音和一个高度


127
00:06:08,576 --> 00:06:09,986
紧张的声音


128
00:06:11,016 --> 00:06:12,916
我们也有一个全新的声音


129
00:06:12,916 --> 00:06:14,396
分析框架 我们会在


130
00:06:14,396 --> 00:06:15,936
Create ML 大会上进行讨论


131
00:06:16,416 --> 00:06:21,496
这些域中的


132
00:06:21,496 --> 00:06:24,146
每一个域都有很多功能


133
00:06:24,146 --> 00:06:25,606
你可以做的另一件事就是


134
00:06:25,606 --> 00:06:27,396
无缝地组合这些域


135
00:06:28,216 --> 00:06:29,246
让我给你们举一个例子


136
00:06:30,026 --> 00:06:33,176
假设你想要构建一个


137
00:06:33,176 --> 00:06:35,566
在图像上进行语义搜索的功能


138
00:06:35,676 --> 00:06:36,896
这是一个非常复杂的功能


139
00:06:36,896 --> 00:06:38,746
因此 如果一个用户搜索


140
00:06:38,746 --> 00:06:41,056
雷暴 你不仅想


141
00:06:41,056 --> 00:06:42,366
向他们提供


142
00:06:42,366 --> 00:06:43,886
雷暴的结果 也想


143
00:06:44,026 --> 00:06:46,716
提供天空和多云的结果


144
00:06:46,836 --> 00:06:49,756
现在 你可以将视觉


145
00:06:49,756 --> 00:06:51,336
和自然语言结合起来


146
00:06:51,336 --> 00:06:53,806
仅需很少的代码就可实现此功能


147
00:06:55,176 --> 00:06:56,206
这就是你将要做的


148
00:06:56,206 --> 00:06:58,146
你将会在你的


149
00:06:58,146 --> 00:07:02,056
图像上运行图像分类器


150
00:07:02,056 --> 00:07:03,606
你也可以让它们使用内置的图像


151
00:07:03,606 --> 00:07:05,286
分类器来生成标记


152
00:07:06,016 --> 00:07:08,886
当用户输入


153
00:07:08,936 --> 00:07:10,356
类似雷暴的单词 你


154
00:07:10,356 --> 00:07:11,926
可以使用词嵌入来


155
00:07:11,926 --> 00:07:15,226
生成相似的单词 并找到


156
00:07:15,226 --> 00:07:16,896
与这些标记匹配的图像


157
00:07:17,626 --> 00:07:21,896
而且这不是全部


158
00:07:21,896 --> 00:07:23,756
你还可以将使用


159
00:07:23,756 --> 00:07:24,936
Create ML 和 Domain API   


160
00:07:24,936 --> 00:07:26,746
创建的自定义模型组合在一起


161
00:07:27,036 --> 00:07:28,646
我们将在使用 Core ML 和 ARKit


162
00:07:28,646 --> 00:07:30,686
创建伟大 App 的会议中


163
00:07:30,686 --> 00:07:32,996
展示该示例


164
00:07:33,546 --> 00:07:37,426
总之 Domain API


165
00:07:37,426 --> 00:07:38,766
允许你利用 Apple 


166
00:07:38,896 --> 00:07:40,576
内置智能和模型


167
00:07:40,856 --> 00:07:42,586
使用 API 所以你无需


168
00:07:42,586 --> 00:07:44,266
收据数据并制作模型


169
00:07:44,736 --> 00:07:45,776
今年我们在


170
00:07:45,866 --> 00:07:48,046
视觉自然语言


171
00:07:48,046 --> 00:07:49,976
语音和声音 API 方面有了显著的扩展


172
00:07:56,336 --> 00:07:58,176
现在让我们来谈谈 Core ML 3


173
00:07:58,526 --> 00:08:01,016
我们产品的第三大支柱


174
00:08:03,536 --> 00:08:06,106
现在我们所有的平台都


175
00:08:06,196 --> 00:08:10,156
支持 Core ML


176
00:08:10,216 --> 00:08:11,396
所有工作都在设备上完成


177
00:08:11,396 --> 00:08:13,936
因此可以维护用户的隐私


178
00:08:14,716 --> 00:08:16,686
Core ML 是硬件加速 


179
00:08:16,686 --> 00:08:18,466
所以你可以这样做 你可以用它来


180
00:08:18,466 --> 00:08:19,726
进行实时的机器学习


181
00:08:20,646 --> 00:08:22,586
无需服务器


182
00:08:22,586 --> 00:08:23,656
它总是可用的


183
00:08:24,356 --> 00:08:29,666
Core ML 始终支持


184
00:08:29,666 --> 00:08:31,056
各种机器学习


185
00:08:31,056 --> 00:08:33,006
模型 包括经典的


186
00:08:33,006 --> 00:08:34,566
广义线性模型 树集成


187
00:08:34,566 --> 00:08:35,976
支持向量机


188
00:08:35,976 --> 00:08:40,395
和卷积神经网络


189
00:08:40,395 --> 00:08:43,155
和循环神经网络等网络


190
00:08:46,236 --> 00:08:48,596
Core ML 的新功能是


191
00:08:48,596 --> 00:08:51,046
模型灵活化和模型个性化


192
00:08:51,626 --> 00:08:54,016
所以让我们来看看这些


193
00:08:55,956 --> 00:08:58,226
Core ML 扩展了


194
00:08:58,226 --> 00:09:01,036
对数据神经网络的支持


195
00:09:01,036 --> 00:09:03,686
我们增加了对


196
00:09:03,686 --> 00:09:06,106
100 多个神经网络层的支持


197
00:09:07,356 --> 00:09:09,046
这意味着你几乎可以


198
00:09:09,046 --> 00:09:11,186
携带 你可以将


199
00:09:11,186 --> 00:09:13,456
最先进的机器学习


200
00:09:13,456 --> 00:09:15,486
模型带入到你的 App 中


201
00:09:15,486 --> 00:09:16,926
比如 ELMo BERT Wavenet


202
00:09:17,816 --> 00:09:18,586
这是什么意思呢


203
00:09:20,136 --> 00:09:21,716
假设你有一个 App


204
00:09:21,716 --> 00:09:22,906
你想在你的 App 中


205
00:09:22,906 --> 00:09:24,056
集成一个最先进的


206
00:09:24,056 --> 00:09:26,476
问答系统


207
00:09:26,476 --> 00:09:28,156
这样一来 当用户提出一个问题


208
00:09:28,156 --> 00:09:30,606
今年的 WWDC 会有多少次大会


209
00:09:31,196 --> 00:09:33,336
你可以使用 BERT 模型来完成


210
00:09:34,536 --> 00:09:35,796
所以 该模型可以对语句


211
00:09:35,796 --> 00:09:38,116
进行分析并给出反馈


212
00:09:38,146 --> 00:09:43,956
结果答案是超过 100


213
00:09:43,956 --> 00:09:45,286
除了自然语言以外


214
00:09:45,286 --> 00:09:47,756
你还可以运行视觉方面的


215
00:09:47,756 --> 00:09:49,166
最新进展比如


216
00:09:49,166 --> 00:09:51,666
实例分割和音频生成


217
00:09:51,666 --> 00:09:53,256
方面的最新进展


218
00:09:56,876 --> 00:09:58,606
为了充分利用


219
00:09:58,606 --> 00:10:00,216
Core ML 的扩展支持


220
00:10:00,266 --> 00:10:03,556
我们正在更新我们的转换器


221
00:10:03,556 --> 00:10:04,986
因此我们将有一个全新的


222
00:10:04,986 --> 00:10:06,276
TensorFlow Core ML 转换器和


223
00:10:06,276 --> 00:10:09,566
ONNX 支持的 ML 转换器即将推出


224
00:10:10,176 --> 00:10:14,756
我们也在更新我们的模型库


225
00:10:14,756 --> 00:10:16,326
在我们的模型库中


226
00:10:16,366 --> 00:10:18,726
导入一些研究模型


227
00:10:18,726 --> 00:10:21,566
以便你可以立即开始使用它们


228
00:10:22,686 --> 00:10:24,466
因此代表着新模型的 ML 3 


229
00:10:24,466 --> 00:10:27,666
和带有模型库的转换器


230
00:10:28,196 --> 00:10:29,526
可以帮助你将


231
00:10:29,526 --> 00:10:31,316
最前沿的 ML 引入到 App 中


232
00:10:32,216 --> 00:10:34,456
现在 模型个性化是一个很大的特点


233
00:10:35,146 --> 00:10:36,836
所以让我解释一下这是什么


234
00:10:37,336 --> 00:10:39,086
所以到目前为止你可以看到


235
00:10:39,086 --> 00:10:41,106
你已经使用 Create ML 来构建


236
00:10:41,106 --> 00:10:44,006
模型 使用 Core ML 来部署


237
00:10:44,006 --> 00:10:47,616
模型 但是 Core ML 3 的新功能是


238
00:10:47,846 --> 00:10:49,806
设备模型个性化


239
00:10:50,266 --> 00:10:53,236
你可以在设备上微调模型


240
00:10:53,966 --> 00:10:58,196
我们在面部 ID 中


241
00:10:58,196 --> 00:11:00,466
使用这种技术并


242
00:11:00,466 --> 00:11:01,276
设置 Watch Face


243
00:11:02,026 --> 00:11:05,536
所以事情就是这样发生的


244
00:11:05,756 --> 00:11:08,506
今天你有了数据 你制作一个


245
00:11:08,566 --> 00:11:11,526
ML 模型 你把它发送到你的


246
00:11:11,736 --> 00:11:13,776
App 中 你的所有用户


247
00:11:13,776 --> 00:11:15,136
都下载了相同的模型


248
00:11:15,706 --> 00:11:19,716
这很棒 因为现在


249
00:11:20,196 --> 00:11:21,986
用户不用上传他们的门户网站


250
00:11:22,346 --> 00:11:24,166
你可以直接拍照


251
00:11:24,646 --> 00:11:25,956
在设备上运行模型


252
00:11:25,956 --> 00:11:31,616
得到诸如狗之类的推理


253
00:11:31,836 --> 00:11:34,126
但是如果你正在尝试


254
00:11:34,126 --> 00:11:35,496
处理一个对于每个用户来说都很独特


255
00:11:35,496 --> 00:11:36,526
的概念 该怎么办


256
00:11:37,096 --> 00:11:38,826
说一说我的狗这个概念吧


257
00:11:39,696 --> 00:11:41,006
你可能希望用户


258
00:11:41,006 --> 00:11:42,866
在图片库中找到他们自己的狗的照片


259
00:11:42,916 --> 00:11:44,456
而不是他们拍的


260
00:11:44,456 --> 00:11:47,216
所有狗的照片


261
00:11:47,336 --> 00:11:49,156
而且每个用户的狗看起来都如此不同


262
00:11:49,156 --> 00:11:50,426
例如 有人可能有


263
00:11:50,486 --> 00:11:51,706
金毛猎犬 有的人可能


264
00:11:52,196 --> 00:11:54,386
有斗牛犬 啊 这就是


265
00:11:54,386 --> 00:11:56,976
我的傻狗


266
00:11:58,976 --> 00:12:01,546
那么 我们应该怎么做呢


267
00:12:02,746 --> 00:12:05,106
所以对于用户 1 来说我们想要的


268
00:12:05,106 --> 00:12:06,936
需要的是带有这只狗的


269
00:12:06,936 --> 00:12:09,076
图像分类器 说这是他们的狗


270
00:12:10,286 --> 00:12:12,326
对用户 2 来说 这是英国


271
00:12:12,326 --> 00:12:14,906
斗牛犬 这是第三只狗


272
00:12:18,576 --> 00:12:21,286
所以为了做到这一点 其中一种


273
00:12:21,286 --> 00:12:23,886
方法是使用基于服务器的方法


274
00:12:23,916 --> 00:12:26,126
所以你可能要求


275
00:12:26,636 --> 00:12:32,476
每个用户在云端上传照片


276
00:12:32,476 --> 00:12:34,496
服务器为每个用户生成模型


277
00:12:34,496 --> 00:12:39,466
然后将它们发回来


278
00:12:39,466 --> 00:12:42,606
不幸的是 这种方法存在隐私问题


279
00:12:42,606 --> 00:12:45,006
你的用户可能并不乐意


280
00:12:45,266 --> 00:12:47,236
将他们的照片上传到你的


281
00:12:47,236 --> 00:12:48,496
服务器中 这样你就无法


282
00:12:48,496 --> 00:12:49,516
获得他们的照片


283
00:12:50,606 --> 00:12:51,866
你必须设置服务器


284
00:12:51,916 --> 00:12:53,296
所以需要一些成本


285
00:12:53,846 --> 00:12:56,346
我们假设你有一百万用户


286
00:12:56,346 --> 00:12:58,036
我们也真诚的希望你有


287
00:12:58,036 --> 00:12:59,396
你就必须得制作数百万个


288
00:12:59,396 --> 00:13:00,776
这样的模型 然后随着时间的推移


289
00:13:00,836 --> 00:13:02,966
来追踪它们


290
00:13:05,176 --> 00:13:07,356
使用 Core ML 3 你就可以


291
00:13:07,356 --> 00:13:09,186
在设备上进行更多个性化设置


292
00:13:09,396 --> 00:13:11,776
所以如果你设备上有


293
00:13:11,776 --> 00:13:13,886
训练数据 你就可以


294
00:13:13,886 --> 00:13:15,906
简单地使用它来微调


295
00:13:15,906 --> 00:13:17,146
设备上的模型


296
00:13:17,946 --> 00:13:23,366
所以你之前使用 LMH


297
00:13:23,366 --> 00:13:27,156
来进行推理 如今你可以


298
00:13:27,156 --> 00:13:28,936
从用户那里获取标签数据反馈


299
00:13:28,936 --> 00:13:30,396
提供一些


300
00:13:30,396 --> 00:13:32,386
训练数据 微调


301
00:13:32,386 --> 00:13:34,276
设备上的模型


302
00:13:35,516 --> 00:13:42,226
[掌声]


303
00:13:42,726 --> 00:13:44,256
每个用户都有一个


304
00:13:44,406 --> 00:13:46,006
个性化模型


305
00:13:48,176 --> 00:13:50,826
我们尊重每个用户的隐私


306
00:13:50,826 --> 00:13:52,696
所以你不必放置服务器


307
00:13:53,386 --> 00:13:56,386
所以 Core ML 3 支持


308
00:13:56,386 --> 00:13:59,326
神经网络的设备个性化


309
00:13:59,786 --> 00:14:01,146
我们也支持近邻


310
00:14:01,276 --> 00:14:03,346
算法 这个可以


311
00:14:03,346 --> 00:14:05,246
晚上在后台进行


312
00:14:08,536 --> 00:14:10,276
因此 为了总结和结束我们的


313
00:14:10,356 --> 00:14:13,046
会议 我们看到 Create ML 这个全新的


314
00:14:13,046 --> 00:14:16,006
App 来构建 ML 模型 我们


315
00:14:16,006 --> 00:14:17,556
在 Domain API 中进行了重大 


316
00:14:17,556 --> 00:14:21,126
扩展 Core ML


317
00:14:21,196 --> 00:14:22,586
也变得更加灵活 现在


318
00:14:22,586 --> 00:14:24,666
支持设备个性化


319
00:14:25,206 --> 00:14:29,016
你可以在我们的开发者网站大会 209


320
00:14:29,016 --> 00:14:30,796
上找到更多信息


321
00:14:30,796 --> 00:14:33,966
我们希望你们


322
00:14:33,996 --> 00:14:36,176
像我们一样对这些技术感兴趣


323
00:14:36,566 --> 00:14:38,226
我期待在实验室见到你


324
00:14:38,366 --> 00:14:38,706
谢谢


325
00:14:39,516 --> 00:14:43,500
[掌声]


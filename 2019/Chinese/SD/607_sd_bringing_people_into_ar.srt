1
00:00:06,640 --> 00:00:09,643 line:0
（带领大家进入AR的世界）


2
00:00:11,578 --> 00:00:12,579 line:-1
大家好


3
00:00:14,715 --> 00:00:15,949 line:-1
谢谢你们的到来


4
00:00:18,051 --> 00:00:20,087 line:0
这是关于ARKit的一个
深入的介绍


5
00:00:20,320 --> 00:00:22,956 line:0
我们会向你展示如何带领
人们走进AR的世界


6
00:00:24,191 --> 00:00:27,027 line:0
我是Adrian 我将会和同事
Tanmay一同为大家介绍


7
00:00:31,665 --> 00:00:34,902 line:0
这周早些时候Apple
发布了RealityKit框架


8
00:00:35,502 --> 00:00:38,472 line:0
这是为渲染逼真内容而
设计的一个新框架


9
00:00:39,139 --> 00:00:42,276 line:-1
它也是从底层开发用来支持AR


10
00:00:44,645 --> 00:00:48,549 line:-2
我们也有一个介绍ARKit 3
新功能的演讲


11
00:00:48,615 --> 00:00:51,251 line:0
我们向你展示了今年ARKit
许多很酷的新功能


12
00:00:52,019 --> 00:00:55,055 line:0
为了深入了解
我们将专注其中的两个方面


13
00:00:56,190 --> 00:00:58,492 line:0
首先 我将向你展示真人遮挡剔除
是如何工作的


14
00:00:58,725 --> 00:01:02,229 line:0
接下来 Tanmay会带你了解


15
00:01:02,296 --> 00:01:04,063 line:-1
动作捕捉是如何工作的


16
00:01:05,432 --> 00:01:06,433 line:-1
那么 我们开始吧


17
00:01:07,467 --> 00:01:08,802 line:-1
什么是真人遮挡剔除？


18
00:01:10,604 --> 00:01:15,375 line:-2
现在使用ARKit 我们已经可以
在真实世界


19
00:01:15,442 --> 00:01:16,443 line:-1
定位渲染的内容了


20
00:01:16,677 --> 00:01:18,579 line:-1
不过 如果我们看我身后的视频


21
00:01:18,645 --> 00:01:20,247 line:-1
我们能看到出现了严重的错误


22
00:01:21,748 --> 00:01:22,783 line:-1
我们期待看到的是


23
00:01:22,950 --> 00:01:25,853 line:-1
离相机近的人会挡住


24
00:01:26,086 --> 00:01:28,989 line:-1
被渲染的内容


25
00:01:30,424 --> 00:01:33,260 line:-1
使用真人遮挡剔除 我们就能做到了


26
00:01:33,861 --> 00:01:38,665 line:-1
我们能正确地在场景中处理渲染内容


27
00:01:38,732 --> 00:01:39,733 line:-1
和人相互遮挡对方


28
00:01:40,167 --> 00:01:43,136 line:-2
为了了解我们需要做一些什么
来实现这个功能


29
00:01:43,437 --> 00:01:44,838 line:-1
我将截取一帧


30
00:01:46,673 --> 00:01:48,509 line:-1
这里 我们有了照相机的图片


31
00:01:49,009 --> 00:01:51,211 line:-1
有两个人站在一张桌子周围


32
00:01:51,845 --> 00:01:55,182 line:-2
我们想要将一个渲染的物件放在
桌子上


33
00:01:56,617 --> 00:02:01,321 line:-2
目前为止 在ARKit 2上
定位渲染内容的方式


34
00:02:01,388 --> 00:02:03,690 line:-1
是简单地将它覆盖在图片上


35
00:02:04,057 --> 00:02:05,392 line:-1
当我们这样做的时候


36
00:02:05,559 --> 00:02:08,662 line:-1
我们看到了一个错误的遮挡关系


37
00:02:09,663 --> 00:02:11,365 line:-1
这不是我们所期待的


38
00:02:11,765 --> 00:02:14,668 line:-2
我们想要将红色的叉变为一个绿色
的对钩


39
00:02:15,269 --> 00:02:18,839 line:-1
为了实现 我们需要明白当有人比


40
00:02:18,906 --> 00:02:20,641 line:-1
渲染内容要靠近照相机时


41
00:02:21,008 --> 00:02:25,579 line:-2
我们需要正确地保证渲染物件
要被遮挡


42
00:02:27,414 --> 00:02:30,317 line:-1
这本质上是一个深度排序的问题


43
00:02:31,018 --> 00:02:33,554 line:-1
为了明白我们会如何解决这个问题


44
00:02:33,620 --> 00:02:35,189 line:-1
我会来分解这个场景


45
00:02:36,924 --> 00:02:39,793 line:-1
这是一张相同倾斜角度的图片


46
00:02:40,093 --> 00:02:44,431 line:-2
我们将场景分割散开
来查看我们有的


47
00:02:44,498 --> 00:02:45,499 line:-1
不同的深度基准面


48
00:02:46,033 --> 00:02:49,603 line:-2
我们在不同的深度基准面上
看到了不同的物件


49
00:02:49,670 --> 00:02:50,671 line:-1
真实和渲染的物件混合在了一起


50
00:02:51,104 --> 00:02:54,408 line:-1
每个人在他们自己的深度基准面上


51
00:02:54,708 --> 00:02:57,578 line:-1
中间是渲染的物件


52
00:03:00,280 --> 00:03:03,617 line:-2
对于渲染的物件
绘图管线已经知道


53
00:03:03,684 --> 00:03:06,787 line:-2
它的确切位置
简单地使用一个深度缓冲就可以了


54
00:03:07,688 --> 00:03:11,124 line:-2
我们为了对真实的内容也实现了
相同的功能


55
00:03:11,191 --> 00:03:14,194 line:-1
我们需要知道人在场景中的位置


56
00:03:14,862 --> 00:03:18,365 line:-1
为了实现 我们加入了两个新的缓冲


57
00:03:19,900 --> 00:03:24,471 line:-2
我们加入了段缓冲器
它会通过每个像素点来告诉你一个人


58
00:03:24,538 --> 00:03:25,539 line:-1
在场景中位置


59
00:03:26,306 --> 00:03:30,177 line:-1
我们也给了你一个相应的深度缓冲


60
00:03:30,244 --> 00:03:32,779 line:-1
它为你提供人的深度位置信息


61
00:03:34,815 --> 00:03:39,720 line:-2
现在 最酷的事情是我们
生产这些缓冲的这个功能


62
00:03:39,786 --> 00:03:44,992 line:-2
是由A12芯片
和机器学习一起提供的


63
00:03:45,058 --> 00:03:48,428 line:-2
只需相机的照片就可以
产生这些缓冲


64
00:03:49,930 --> 00:03:54,768 line:-1
我们的这两个新缓冲将作为


65
00:03:54,835 --> 00:03:56,370 line:-1
两个新属性暴露给ARFrame


66
00:03:56,436 --> 00:03:59,373 line:-2
segmentationBuffer
和estimatedDepthData


67
00:04:03,277 --> 00:04:05,946 line:-2
因为我们想要使用缓冲来实现
真人遮挡剔除


68
00:04:06,213 --> 00:04:10,517 line:-2
我们也要将它们以相同的频率
制作成相机的帧


69
00:04:11,185 --> 00:04:14,388 line:-2
所以 当你相机的帧以60帧每秒
的频率运行时


70
00:04:15,088 --> 00:04:19,226 line:-2
我们也能以60帧每秒的频率
为你产出这些缓冲


71
00:04:21,995 --> 00:04:25,399 line:-1
我们也想要这些缓冲和相机的图片


72
00:04:25,465 --> 00:04:26,533 line:-1
分辨率一样


73
00:04:27,601 --> 00:04:31,972 line:-1
不过 为了实时完成这个功能


74
00:04:32,039 --> 00:04:33,841 line:-1
神经网络只能检视到一张小的照片


75
00:04:34,575 --> 00:04:37,077 line:-1
所以 如果你拿到神经网络的输出


76
00:04:37,811 --> 00:04:40,848 line:-2
我们放大它
我们会看到这里有很多细节


77
00:04:40,914 --> 00:04:43,083 line:-1
神经网络并未检视到


78
00:04:44,484 --> 00:04:49,223 line:-2
所以为了弥补细节的损耗
我们做了其他的处理


79
00:04:50,691 --> 00:04:51,892 line:-1
我们使用遮片提取


80
00:04:52,793 --> 00:04:58,532 line:-2
遮片提取做了什么呢 根本上来说
它使用了segmentationBuffer


81
00:04:58,999 --> 00:05:00,601 line:-1
作为一个指导 接着看相机的照片


82
00:05:00,667 --> 00:05:02,936 line:-1
为了了解遗失的细节是什么


83
00:05:04,071 --> 00:05:07,708 line:-2
现在 使用一张遮片提取的照片
我们能正确地从场景中提取人


84
00:05:07,774 --> 00:05:11,745 line:-2
和estimatedDepthData
一同使用


85
00:05:11,979 --> 00:05:14,748 line:-2
接着 我们能定位他们
正确的深度基准面


86
00:05:15,382 --> 00:05:18,151 line:-1
最后 我们解决深度排序的问题


87
00:05:18,218 --> 00:05:20,454 line:-1
接着 我们就能重绘场景了


88
00:05:22,189 --> 00:05:23,924 line:-1
这使用了很多的技术


89
00:05:24,458 --> 00:05:26,827 line:-1
我们想要开发者能尽可能简单地


90
00:05:26,894 --> 00:05:28,762 line:-1
使用它


91
00:05:28,996 --> 00:05:31,832 line:-2
所以我们提供了三种不同的方式
来实现了这个功能


92
00:05:33,367 --> 00:05:38,338 line:-2
首先 我们有RealityKit
我们宣布的最新框架


93
00:05:39,072 --> 00:05:43,377 line:-2
但如果你已使用SceneKit了
我们也引入了


94
00:05:43,443 --> 00:05:46,046 line:-2
真人遮挡剔除的支持
通过使用ARCSNView


95
00:05:46,613 --> 00:05:48,582 line:-1
如果你有了自己的渲染器


96
00:05:48,982 --> 00:05:50,984 line:-1
或是使用了一个第三方渲染


97
00:05:51,451 --> 00:05:56,123 line:-2
我们为你提供了建筑块来让你
在app中通过Metal的


98
00:05:56,323 --> 00:05:58,458 line:-1
协助完成真人遮挡剔除


99
00:05:59,760 --> 00:06:01,828 line:-2
那么 我们来看一下我们如何使用
RealityKit来实现


100
00:06:03,830 --> 00:06:09,102 line:-2
如果你要创建一个新的AR app
我们推荐你用RealityKit


101
00:06:10,037 --> 00:06:14,641 line:-2
它有新的UI元素叫做ARView
它为你提供了一个


102
00:06:14,842 --> 00:06:19,279 line:-2
简单易用的API
为AR增加了照片写实


103
00:06:19,713 --> 00:06:24,952 line:-2
更近一步混合了真实
和渲染内容的边界


104
00:06:26,920 --> 00:06:29,723 line:-1
它也在内部支持真人遮挡剔除功能


105
00:06:30,257 --> 00:06:32,159 line:-1
如果你参加过新功能介绍的演讲


106
00:06:32,226 --> 00:06:36,063 line:-1
你已看过在一个ARView中启用


107
00:06:36,129 --> 00:06:37,130 line:-1
真人遮挡剔除功能的一段实时示例


108
00:06:37,464 --> 00:06:40,000 line:-2
为了深入了解它
我们来快速回顾一下


109
00:06:40,601 --> 00:06:41,702 line:-1
我们来看一些代码


110
00:06:43,637 --> 00:06:47,241 line:-2
这里是我的视图控制器里的
viewDidLoad函数


111
00:06:47,608 --> 00:06:49,076 line:-1
所以 我要做的第一件事是


112
00:06:49,142 --> 00:06:51,245 line:-1
来确保是否支持这个功能


113
00:06:52,646 --> 00:06:57,351 line:-2
我通过检查我的配置 就是这个
WorldTrackingConfiguration


114
00:06:58,085 --> 00:07:01,288 line:-2
我需要一个新的属性叫做
FrameSemantics


115
00:07:01,588 --> 00:07:04,424 line:-2
我们将要使用FrameSemantics的
personSegmentationWithDepth


116
00:07:04,491 --> 00:07:06,560 line:-1
来启用真人遮挡剔除功能


117
00:07:07,728 --> 00:07:11,999 line:-1
只要我知道它在我的配置中被支持


118
00:07:12,733 --> 00:07:16,904 line:-2
我只需在我的配置中设置
FrameSemantics就行


119
00:07:17,271 --> 00:07:19,506 line:-1
接下来 当会话开始运行时


120
00:07:19,573 --> 00:07:24,344 line:-2
ARView会自动地为
我的AR app


121
00:07:24,411 --> 00:07:25,679 line:-1
启用真人遮挡剔除功能


122
00:07:27,381 --> 00:07:31,451 line:-2
所以 我们所需做的是
改变我们的配置


123
00:07:31,518 --> 00:07:34,788 line:-2
使用我们今年介绍的新属性
FrameSemantics


124
00:07:35,055 --> 00:07:39,459 line:-2
就像你在之前的例子中看到的
我使用personSegmentationWithDepth


125
00:07:39,893 --> 00:07:42,029 line:-2
但我们也可以只使用
PersonSegmentation


126
00:07:42,095 --> 00:07:44,565 line:-1
当你想要启用一个似气象员的体验


127
00:07:46,166 --> 00:07:50,938 line:-2
现在 我们推荐使用ARView
来实现真人遮挡剔除功能


128
00:07:51,205 --> 00:07:54,708 line:-2
原因是因为它内部集成了一个
深度渲染器


129
00:07:55,442 --> 00:07:58,612 line:-1
那意味着整个绘图管线都能意识到


130
00:07:58,679 --> 00:08:00,747 line:-1
场景中有人


131
00:08:00,981 --> 00:08:04,418 line:-1
它也因此可以使用这个功能来


132
00:08:04,484 --> 00:08:05,786 line:-1
处理透明的物件


133
00:08:06,687 --> 00:08:10,357 line:-1
它也会同时创建最佳的性能体验


134
00:08:11,491 --> 00:08:14,628 line:-2
如果你担心使用真人遮挡剔除
和RealityKit


135
00:08:14,695 --> 00:08:17,331 line:-1
的用户体验


136
00:08:17,397 --> 00:08:19,499 line:-1
我想为你带来一部短片


137
00:08:38,018 --> 00:08:39,285 line:-1
这是Swiftstrike


138
00:08:39,720 --> 00:08:43,056 line:-2
它是一个很酷的示例
我们在Dub-dub上展示的


139
00:08:43,390 --> 00:08:46,727 line:-2
我强烈建议你们中
还没有查看的人去尝试


140
00:08:48,061 --> 00:08:49,830 line:-2
那么 如果你已经在使用
SceneKit了呢？


141
00:08:51,431 --> 00:08:54,301 line:-2
我们看看我如何在SceneKit
实现真人遮挡剔除功能


142
00:08:56,703 --> 00:09:00,440 line:-2
如果你已在使用ARSCNView
就能用一个和RealityKit


143
00:09:00,674 --> 00:09:03,310 line:-2
一样非常相似的方式启用
实现真人遮挡剔除功能


144
00:09:04,044 --> 00:09:07,981 line:-2
我们要做是在我们的配置中设置
FrameSemantics


145
00:09:08,048 --> 00:09:10,617 line:-2
ARSCNView会自动地
使用这个功能


146
00:09:11,752 --> 00:09:14,988 line:-2
但实现SceneKit和
RealityKit方式略有不同


147
00:09:15,055 --> 00:09:19,993 line:-2
SceneKit做了一个
后处理效果图


148
00:09:20,260 --> 00:09:21,962 line:-1
那对你具体意味着什么呢


149
00:09:22,029 --> 00:09:24,731 line:-1
基于你设置的深度 它可能不会


150
00:09:24,798 --> 00:09:26,333 line:-1
非常好地处理透明的物体


151
00:09:28,502 --> 00:09:31,171 line:-2
最后 假如我有我
自己的渲染引擎呢？


152
00:09:32,239 --> 00:09:36,076 line:-2
我们想要让你在你自己
或一个第三方的渲染引擎


153
00:09:36,143 --> 00:09:39,213 line:-1
实现真人遮挡剔除功能


154
00:09:40,747 --> 00:09:43,884 line:-1
它能让你完全控制效果图


155
00:09:45,886 --> 00:09:48,722 line:-1
我们想要给你更多的自由度


156
00:09:48,889 --> 00:09:52,559 line:-2
为你提供简单易用的API的同时
也提供了


157
00:09:52,626 --> 00:09:53,627 line:-1
这个很棒的功能


158
00:09:54,228 --> 00:09:57,364 line:-2
那么 在我为你展示如何做之前
我们来快速回顾一下


159
00:09:58,498 --> 00:10:02,836 line:-2
我们的这个来自神经网络的
segmentationBuffer


160
00:10:03,070 --> 00:10:05,272 line:-1
处理一张比较小的图片


161
00:10:05,339 --> 00:10:06,540 line:-1
接下来 我们介绍了遮片提取


162
00:10:06,607 --> 00:10:08,976 line:-1
为了修复一些遗失的细节


163
00:10:10,277 --> 00:10:15,249 line:-2
当我们自定义效果图时
我们提供了一个新的类型


164
00:10:15,315 --> 00:10:18,285 line:-2
它通过使用Metal
会为你实现遮片提取


165
00:10:18,485 --> 00:10:22,122 line:-2
为你提供遮片的纹理
集成至你自己的管线中


166
00:10:22,656 --> 00:10:25,192 line:-1
我们来用一个例子来看如何实现


167
00:10:27,194 --> 00:10:29,496 line:-1
那么 这里是我的自定义构建函数


168
00:10:29,830 --> 00:10:34,468 line:-2
我要做的第一件事是确认这个功能
是否被支持


169
00:10:35,235 --> 00:10:39,173 line:-2
只要我实现后 我只需调用
generateMatte


170
00:10:39,239 --> 00:10:41,241 line:-2
我为它提供frame
和commandBuffer的值


171
00:10:41,475 --> 00:10:45,746 line:-2
它返回给我Metal纹理
当我自定义效果图时


172
00:10:45,812 --> 00:10:49,917 line:-2
我能使用它们 最后
为GPU规划所有的进程


173
00:10:51,585 --> 00:10:55,822 line:-2
我们为ARKit增加的类叫做
ARMatteGenerator


174
00:10:55,889 --> 00:10:59,059 line:-2
就像你在例子中看到的那样 它获取
ARFrame和commandBuffer


175
00:10:59,226 --> 00:11:01,628 line:-1
返回一张你能用到的纹理


176
00:11:02,362 --> 00:11:03,830 line:-1
不过 我们还没做完


177
00:11:05,365 --> 00:11:08,769 line:-2
和segmentationBuffer类似
它的分辨率很低


178
00:11:09,036 --> 00:11:13,473 line:-2
estimatedDepthData也低
如果我们放大它


179
00:11:13,540 --> 00:11:15,642 line:-1
覆盖在我们的遮片图片上


180
00:11:16,844 --> 00:11:18,846 line:-1
我们看到它们可能不太一致


181
00:11:19,346 --> 00:11:22,516 line:-2
我们可以使用遮片的没有阿尔法值的
深度值


182
00:11:22,683 --> 00:11:25,586 line:-2
更重要的是
我们也能在遮片中获取没有


183
00:11:25,652 --> 00:11:27,554 line:-1
相应深度值的阿尔法值


184
00:11:31,191 --> 00:11:34,294 line:-2
现在 虽然遮片已经拥有了一些
遗失的细节


185
00:11:34,361 --> 00:11:36,630 line:-1
我们不能真的修正它自己的阿尔法值


186
00:11:36,697 --> 00:11:39,266 line:-2
取而代之的是 我们需要修正
深度缓冲


187
00:11:41,468 --> 00:11:44,771 line:-2
那么 我们回到我之前的例子
看看我们要如何实现


188
00:11:45,873 --> 00:11:49,376 line:-2
这里我们有我添加的一行代码
生成matte的定义


189
00:11:49,443 --> 00:11:51,512 line:-1
为了让我使用我自定义的效果图


190
00:11:52,045 --> 00:11:56,750 line:-2
所以我加入了额外的函数
调用generateDilatedDepth


191
00:11:57,084 --> 00:12:00,254 line:-2
非常相似地 它也获取frame和
commandBuffer参数


192
00:12:00,454 --> 00:12:02,022 line:-1
返回一张纹理图


193
00:12:04,157 --> 00:12:08,128 line:-2
你看它的API 它和我们生成
遮片的非常类似


194
00:12:08,762 --> 00:12:12,933 line:-2
返回纹理图 获取frame和
commandBuffer参数


195
00:12:14,001 --> 00:12:18,338 line:-2
这能保证遮片中
每个我们发现的阿尔法值


196
00:12:18,405 --> 00:12:20,874 line:-1
都有一个相应的深度值


197
00:12:20,941 --> 00:12:23,677 line:-1
我们可以用它来处理最终效果图


198
00:12:25,345 --> 00:12:29,850 line:-2
利用这个扩大了的深度和遮片
我们终于可以进入到


199
00:12:29,917 --> 00:12:30,884 line:-1
效果图中


200
00:12:32,853 --> 00:12:36,056 line:-2
效果图通常在片段着色器
由GPU完成


201
00:12:36,523 --> 00:12:39,960 line:-2
那么 我们来看一个示例着色器
了解我是如何整合一切的


202
00:12:41,962 --> 00:12:45,799 line:-2
我从通常需要怎么
创建一个AR体验开始


203
00:12:46,033 --> 00:12:49,736 line:-2
我创建了一个相机图片
和一个渲染纹理示例


204
00:12:50,370 --> 00:12:53,841 line:-2
因为我们要做遮挡剔除
我也创建了一个渲染深度示例


205
00:12:55,342 --> 00:12:58,278 line:-2
接下来 我通常会
这样在AR里实现


206
00:12:58,345 --> 00:13:01,648 line:-2
我在一张真实图片上覆盖
渲染的内容


207
00:13:02,082 --> 00:13:03,817 line:-1
传给它渲染阿尔法值


208
00:13:05,219 --> 00:13:08,522 line:-2
下一个部分 为了实现
真人遮挡剔除


209
00:13:08,589 --> 00:13:12,526 line:-2
我也创建了一个遮片和
dilatedDepth


210
00:13:13,527 --> 00:13:18,632 line:-2
接着我确保对比dilatedDepth
和renderedDepth


211
00:13:18,866 --> 00:13:22,469 line:-2
如我发现dilatedDepth里的
东西离照像机更近


212
00:13:22,536 --> 00:13:25,339 line:-1
则意味着那里可能有一个人


213
00:13:25,672 --> 00:13:29,776 line:-2
接下来 我通过混合了照像机
返回遮片的值


214
00:13:30,244 --> 00:13:34,348 line:-1
不过 如果渲染内容离照相机更近


215
00:13:34,414 --> 00:13:38,585 line:-2
如以往一样 将渲染内容
放在它的上面


216
00:13:39,520 --> 00:13:44,458 line:-2
这样 我们终于在我们自定义
的渲染器上


217
00:13:44,525 --> 00:13:46,260 line:-1
实现了真人遮挡剔除功能


218
00:13:55,169 --> 00:13:59,706 line:-2
因为这个功能运用了神经网络引擎和
机器学习


219
00:13:59,773 --> 00:14:02,442 line:-1
它支持使用了A12及之后的设备


220
00:14:03,810 --> 00:14:06,613 line:-1
我也在室内环境中运行良好


221
00:14:07,381 --> 00:14:10,784 line:-2
你看到的这些所有视频中
你看到场景中站着的人


222
00:14:10,851 --> 00:14:13,787 line:-2
但这个功能也适用于
你自己的手和脚


223
00:14:15,022 --> 00:14:17,291 line:-1
它也适用于多人场景


224
00:14:18,358 --> 00:14:21,628 line:-2
在我邀请Tanmay上台为大家
展示动作捕捉之前


225
00:14:21,695 --> 00:14:22,863 line:-1
我们来快速回顾一下


226
00:14:24,598 --> 00:14:29,636 line:-2
使用真人遮挡剔除功能
我们能为大家在渲染内容


227
00:14:29,703 --> 00:14:31,305 line:-1
和真实内容间正确处理遮挡关系


228
00:14:33,440 --> 00:14:35,843 line:-2
如果你要创建一个新的app
我们推荐你


229
00:14:35,909 --> 00:14:40,848 line:-2
使用RealityKit和ARView
因为它能进行深度融合


230
00:14:41,682 --> 00:14:44,184 line:-2
如果你已经在你的app中使用了
SceneKit


231
00:14:44,251 --> 00:14:47,888 line:-2
我们也在ARSCNView中
加入了真人遮挡剔除的支持


232
00:14:49,223 --> 00:14:52,659 line:-2
如果你有你自己的渲染器
我们为你提供了一个新的API


233
00:14:52,726 --> 00:14:56,463 line:-2
调用ARMatteGenerator
你就能自己合成图片


234
00:14:56,530 --> 00:14:58,632 line:-1
至你自己的渲染器中


235
00:14:59,333 --> 00:15:02,970 line:-2
下面 有请Tanmay
带大家深入了解


236
00:15:03,036 --> 00:15:04,037 line:-1
动作捕捉


237
00:15:06,340 --> 00:15:07,341 line:0
（动作捕捉）


238
00:15:10,878 --> 00:15:12,045 line:-1
谢谢Adrian


239
00:15:12,679 --> 00:15:16,650 line:-2
大家好 我是Tanmay
今天我为大家介绍


240
00:15:16,717 --> 00:15:19,720 line:-1
我们今年引入的新技术


241
00:15:20,087 --> 00:15:21,388 line:-1
动作捕捉


242
00:15:25,826 --> 00:15:28,095 line:-1
那么 什么是动作捕捉呢？


243
00:15:28,729 --> 00:15:31,865 line:-1
它就是用来捕捉人的动作的


244
00:15:33,200 --> 00:15:36,537 line:-2
你看到了一个人
你捕捉了所有的动作


245
00:15:36,603 --> 00:15:41,909 line:-2
你用一个虚拟的角色
来模拟那个动作


246
00:15:41,975 --> 00:15:45,546 line:-2
使那个角色表现出和你看到的一样的
一套动作


247
00:15:45,746 --> 00:15:49,283 line:-2
我们试着能让你的app
可以实现这项技术


248
00:15:50,484 --> 00:15:51,685 line:-1
现在 我们开始深入了解吧


249
00:15:52,486 --> 00:15:56,156 line:-2
我们想要这个角色来模仿
你看到的这个人


250
00:15:56,356 --> 00:16:00,160 line:-2
但在我们开始前
我们需要了解我们要做


251
00:16:00,227 --> 00:16:03,463 line:-2
怎样的动画效果
这个角色需要怎样实现？


252
00:16:04,631 --> 00:16:07,034 line:-1
所以 这是一个虚拟角色的示例


253
00:16:07,434 --> 00:16:10,270 line:-2
我们给它照一张X光片子
来看看它里面有什么


254
00:16:12,005 --> 00:16:14,474 line:-1
你能看到它有两个主要部分


255
00:16:14,708 --> 00:16:17,244 line:-1
它的外层 叫做一个网格


256
00:16:17,644 --> 00:16:20,781 line:-2
骨性结构里面
称为一个骨架


257
00:16:21,782 --> 00:16:25,519 line:-1
结合这两者就是这个完整的角色


258
00:16:26,787 --> 00:16:30,791 line:-1
骨架是整个角色背后的驱动力


259
00:16:31,191 --> 00:16:34,695 line:-2
它包含了我们用来控制它动作的
所有的四肢 就像人那样


260
00:16:35,596 --> 00:16:39,833 line:-2
所以 为了让角色动起来
我们需要让这个骨架遵循


261
00:16:39,900 --> 00:16:40,968 line:-1
一样的动作动起来


262
00:16:42,102 --> 00:16:43,804 line:-1
所以 第一步怎么做呢？


263
00:16:43,971 --> 00:16:48,809 line:-2
我们有一个人 第一步我们
让这个骨架模拟这个人


264
00:16:50,777 --> 00:16:52,346 line:-1
这就是它的看上去的样子


265
00:16:52,579 --> 00:16:56,550 line:-2
这要这个骨架动了
这个角色就会接着动


266
00:16:57,985 --> 00:17:02,990 line:-1
整个虚拟角色在自动地模仿你


267
00:17:05,325 --> 00:17:06,393 line:-1
所以 我们如何实现呢？


268
00:17:07,794 --> 00:17:09,396 line:-1
我们如何让这个骨架动起来呢？


269
00:17:09,762 --> 00:17:13,165 line:-1
基于这张图片 我们使用机器学习


270
00:17:14,134 --> 00:17:16,936 line:-1
来首次估量图片中人的姿势


271
00:17:17,304 --> 00:17:22,643 line:-2
我们使用这个姿势来创建一个完整
高保真的骨架


272
00:17:24,344 --> 00:17:27,481 line:-2
最后 我们使用这个骨架
结合一个网格


273
00:17:27,548 --> 00:17:29,216 line:-1
给你最终的角色


274
00:17:30,083 --> 00:17:34,821 line:-2
我们在ARKit整合了你看到的
所有的这些


275
00:17:36,924 --> 00:17:38,492 line:-1
为了完整的概括


276
00:17:39,226 --> 00:17:43,096 line:-2
我们在今年的ARKit加入了
动作捕捉的科技


277
00:17:43,764 --> 00:17:48,802 line:-2
使用它 你能在你的设备中实时
追踪人的动作


278
00:17:49,636 --> 00:17:54,675 line:-2
它和RealityKit一起能完美工作
为你提供了驱动角色的动画效果


279
00:17:54,942 --> 00:17:58,278 line:-1
接着渲染在你的屏幕中


280
00:17:58,912 --> 00:18:03,016 line:-2
它由机器学习驱动
在Apple神经网络引擎中平稳运行


281
00:18:04,051 --> 00:18:07,888 line:-1
我们在A12及之后设备支持这功能


282
00:18:09,223 --> 00:18:12,626 line:-2
那么 现在利用这项技术
你可以做些什么呢？


283
00:18:12,693 --> 00:18:14,328 line:-1
你能用它做什么？


284
00:18:15,796 --> 00:18:18,866 line:-2
对于新手来说 你可以创建
一个可以跟随一个人的


285
00:18:18,932 --> 00:18:20,300 line:-1
虚拟角色


286
00:18:20,501 --> 00:18:22,369 line:-2
你可以在AR中拥有一个你自己的
木偶


287
00:18:23,136 --> 00:18:25,873 line:-1
这是可以立刻实现的


288
00:18:26,306 --> 00:18:29,843 line:-1
除此之外 你可以在其他的


289
00:18:30,043 --> 00:18:31,245 line:-1
一些使用案例中用到它


290
00:18:32,212 --> 00:18:37,017 line:-1
比如 你可以创建你自己的监测


291
00:18:37,084 --> 00:18:38,752 line:-1
人们动作的模型来进一步强化它


292
00:18:40,387 --> 00:18:43,690 line:-1
你能使用它来创建分析动作的工具


293
00:18:43,757 --> 00:18:47,294 line:-2
比如高尔夫挥杆动作
或是你的正确姿势


294
00:18:47,361 --> 00:18:50,497 line:-1
或是在锻炼中的姿势是否正确


295
00:18:52,799 --> 00:18:56,670 line:-2
现在 因为人在场景中有了一个
虚拟的表示物件


296
00:18:56,904 --> 00:19:00,741 line:-2
你可以和任何你喜欢的虚拟物件
产生交互活动


297
00:19:02,242 --> 00:19:06,213 line:-1
它支持场景中所有的虚拟物件


298
00:19:07,681 --> 00:19:12,186 line:-2
最后 你也能使用它来分析图片
和视频


299
00:19:12,419 --> 00:19:16,857 line:-2
因为我们也在图像空间
提供骨架的2D版本


300
00:19:17,024 --> 00:19:21,662 line:-2
你可以创建它来编辑工具
或是进行图像语义理解


301
00:19:24,798 --> 00:19:29,703 line:-1
这甚至不需要涵盖全部的可能性


302
00:19:29,903 --> 00:19:31,972 line:-1
你可以远程启用它


303
00:19:32,406 --> 00:19:34,675 line:-1
你还可以用它来做其他很多事情


304
00:19:36,176 --> 00:19:38,879 line:-2
现在 我来向你展示如何
在你的app中引入


305
00:19:38,946 --> 00:19:40,013 line:-1
动作捕捉


306
00:19:40,814 --> 00:19:44,651 line:-2
基于不同的使用场景
我们有三种不同的方式来使用它


307
00:19:45,786 --> 00:19:49,223 line:-2
第一种
是RealityKit的动作捕捉


308
00:19:50,324 --> 00:19:52,860 line:-2
如果你只是想快速地让一个角色
动起来


309
00:19:53,760 --> 00:19:55,996 line:-1
这个高级的API能帮你


310
00:19:56,630 --> 00:20:02,102 line:-2
如果你想启用高级的使用场景
比如活动识别 分析


311
00:20:02,169 --> 00:20:07,774 line:-2
或在场景中与3D物件交互
我们也提供了低级别的API


312
00:20:07,841 --> 00:20:10,944 line:-1
来单独取出骨架中的每个元素


313
00:20:12,112 --> 00:20:14,081 line:-1
它是为你而设计的


314
00:20:15,115 --> 00:20:19,453 line:-1
最后 如果你的使用场景需要


315
00:20:19,686 --> 00:20:22,856 line:-2
位于2D版本的图片空间的骨架
可能用于分析动态图片


316
00:20:22,923 --> 00:20:25,492 line:-1
或用来编辑工具 或用于其他事情


317
00:20:25,559 --> 00:20:27,728 line:-1
我们也提供了相关的入口


318
00:20:29,796 --> 00:20:32,799 line:-2
那么 我们从RealityKit
的动作捕捉开始


319
00:20:34,201 --> 00:20:37,204 line:-2
你知道的 我们今年
引入了RealityKit


320
00:20:37,738 --> 00:20:43,010 line:-2
使用RealityKit的API
你只需写几行代码


321
00:20:43,076 --> 00:20:46,113 line:-2
就能追踪一个人
以及添加一个角色来模仿


322
00:20:47,514 --> 00:20:51,685 line:-2
为此我们提供了一个
非常简单易用的API


323
00:20:52,486 --> 00:20:55,289 line:-1
你也可以加入你自定义的角色


324
00:20:55,455 --> 00:20:58,058 line:-1
基于我们提供的示例结构


325
00:20:58,525 --> 00:21:02,429 line:-2
所以 基于我们提供的示例结构
在示例包里


326
00:21:02,496 --> 00:21:07,801 line:-1
你可以使用任意的网格


327
00:21:07,868 --> 00:21:09,036 line:-1
你想要的任意的角色


328
00:21:11,238 --> 00:21:15,909 line:-2
最后 被追踪的人非常容易通过一个
叫做AnchorEntities的元素


329
00:21:15,976 --> 00:21:20,113 line:-2
在这里被访问到
它们只是在场景中创建的块


330
00:21:20,747 --> 00:21:24,551 line:-1
它会自动地收集我们在动作捕捉


331
00:21:24,618 --> 00:21:25,619 line:-1
中需要的所有转换信息


332
00:21:28,522 --> 00:21:29,690 line:-1
那么 首先


333
00:21:31,825 --> 00:21:36,330 line:-2
你在ARView获取的
动作捕捉的每个元素


334
00:21:36,597 --> 00:21:37,698 line:-1
你门知道的


335
00:21:37,764 --> 00:21:42,069 line:-2
它是结合了AR和RealityKit
主要的UI元素


336
00:21:42,436 --> 00:21:47,741 line:-2
它由一个新的配置
ARBodyTrackingConfiguration来驱动


337
00:21:48,342 --> 00:21:53,947 line:-2
只要你启用了它
你可以开始加入包含了身体锚点和


338
00:21:54,181 --> 00:21:58,385 line:-2
封装在锚点整体里的一个叫做
bodyTrackedEntity


339
00:21:58,452 --> 00:22:00,220 line:-1
的特殊类型


340
00:22:00,821 --> 00:22:02,322 line:-1
那么 我来为你简单介绍一下


341
00:22:02,389 --> 00:22:05,259 line:-2
bodyTrackedEntity
实际上是什么呢？


342
00:22:06,627 --> 00:22:09,997 line:-2
一个身体被追踪的整体代表着
一个单独的人


343
00:22:10,731 --> 00:22:14,268 line:-1
它包含下面的骨骼和它的位置


344
00:22:15,736 --> 00:22:19,606 line:-1
它被实时跟踪 每帧都会更新


345
00:22:20,174 --> 00:22:25,045 line:-2
最后 它结合了骨骼来
自动地拼凑出网格


346
00:22:25,512 --> 00:22:27,381 line:-1
和为你提供了完整的角色


347
00:22:28,949 --> 00:22:32,119 line:-2
现在 我们来快速过一遍让角色
动起来的代码


348
00:22:32,186 --> 00:22:33,754 line:-1
你会发现非常简单


349
00:22:33,820 --> 00:22:35,622 line:-1
你只需跟随这3个步骤


350
00:22:36,590 --> 00:22:38,759 line:-1
第一步是加载一个角色


351
00:22:40,160 --> 00:22:45,632 line:-2
为了自动地异步加载一个被追踪的人
你需要调用


352
00:22:45,699 --> 00:22:48,869 line:-2
entity.loadBodyTrackedAsync
函数


353
00:22:50,003 --> 00:22:55,209 line:-2
以及你可以使用.sync来捕捉
在完成接收块的错误


354
00:22:55,275 --> 00:22:56,877 line:-1
或者 如果没有任何问题的话


355
00:22:56,944 --> 00:22:59,513 line:-1
你会在返回值块中获取你的角色


356
00:22:59,980 --> 00:23:03,283 line:-2
这个角色是
bodyTrackedEntity类型


357
00:23:07,087 --> 00:23:11,458 line:-2
在我们示例代码的包中 我们有一个
叫做robot.usdz的文件


358
00:23:11,625 --> 00:23:15,596 line:-2
如果你为这个文件提供文件机器人
它会自动地


359
00:23:15,662 --> 00:23:19,066 line:-2
为机器人网格添加骨骼
以及提供一个角色给你


360
00:23:20,501 --> 00:23:24,571 line:-1
接下来 第二部是来获取


361
00:23:24,638 --> 00:23:25,739 line:-1
你想要放置角色的位置信息


362
00:23:27,107 --> 00:23:30,744 line:-2
比如 如果你想要将角色放在追踪人
的上方


363
00:23:30,811 --> 00:23:34,515 line:-2
你可以通过使用
AnchorEntity


364
00:23:34,581 --> 00:23:37,184 line:-1
的参数.body来获取位置信息


365
00:23:37,684 --> 00:23:40,420 line:-1
请留意这只是一个例子


366
00:23:40,487 --> 00:23:42,890 line:-1
它也能被放置到其他位置


367
00:23:43,223 --> 00:23:47,661 line:-2
在地板上 在桌面上或其他任意地方
你只需提供


368
00:23:47,728 --> 00:23:49,997 line:-1
位置的一个锚点


369
00:23:51,164 --> 00:23:53,467 line:-1
角色会一直模仿这个人


370
00:23:55,903 --> 00:23:59,940 line:-2
最后 将你的角色放在这里
<i>看吧!</i>


371
00:24:01,341 --> 00:24:04,077 line:-2
你可以驱动你的角色了
就是这么简单


372
00:24:08,448 --> 00:24:12,653 line:-2
那么 你可能会思考
你要如何用你喜欢的其他自定义


373
00:24:12,719 --> 00:24:14,855 line:-1
角色来替换这个机器人呢


374
00:24:16,857 --> 00:24:20,928 line:-2
就像我之前说的 我们有一个
叫做robot.usdz的文件


375
00:24:21,195 --> 00:24:24,531 line:-1
这个usdz文件有一个完整的结构


376
00:24:24,598 --> 00:24:29,236 line:-2
如果你自定义的模型遵循了和它
一样的结构 接着你就能使用它


377
00:24:30,070 --> 00:24:33,941 line:-1
另外 我们提供的下面的骨骼


378
00:24:34,274 --> 00:24:38,178 line:-2
是一个极高保真的包含了91个
关节点的骨骼


379
00:24:38,712 --> 00:24:42,382 line:-1
这就是你要了解的所有信息了


380
00:24:45,485 --> 00:24:46,486 line:-1
有很多 对吧？


381
00:24:46,954 --> 00:24:51,425 line:-2
这些只是一般的关节点
包含了手臂 腿等等


382
00:24:52,759 --> 00:24:54,895 line:-1
如果你的角色遵循了这个命名方案


383
00:24:54,962 --> 00:24:56,763 line:-2
你可以直接在RealityKit
中使用它


384
00:25:00,767 --> 00:25:04,872 line:-2
这是一个快速简单的方式来载入
一个被追随的人和驱动角色


385
00:25:05,472 --> 00:25:09,009 line:-2
现在 我们来看低级别的API
作为高级的用途


386
00:25:10,410 --> 00:25:14,615 line:-2
这里 我们提供了一些接触骨骼中
每个元素的方式


387
00:25:16,250 --> 00:25:19,086 line:-2
我们通过了一个非常简单易用的
API连接了它


388
00:25:21,054 --> 00:25:23,257 line:-2
你能启用所有的这些高阶的
使用案例


389
00:25:23,323 --> 00:25:26,727 line:-2
我们之前讨论过
它需要使用骨骼数据来分析


390
00:25:26,793 --> 00:25:31,265 line:-1
或是为你模型提供一个输入


391
00:25:33,967 --> 00:25:38,639 line:-2
最后 我们提供的骨骼包含了
我们介绍的一个新的锚点类型


392
00:25:38,705 --> 00:25:41,408 line:-1
叫做ARBodyAnchor


393
00:25:42,543 --> 00:25:46,580 line:-2
ARBodyAnchor
是我们提供的


394
00:25:46,780 --> 00:25:47,781 line:-1
整个数据结构的初始点


395
00:25:48,849 --> 00:25:50,984 line:-1
这就是数据结构的样子


396
00:25:52,519 --> 00:25:57,424 line:-2
上面有ARBodyAnchor
它包含了所有骨骼的元素


397
00:26:00,360 --> 00:26:03,397 line:-2
那么 我们来看看这个结构
从上面开始


398
00:26:05,799 --> 00:26:08,135 line:-2
ARBodyAnchor
是一个常规的锚点


399
00:26:08,202 --> 00:26:09,870 line:-1
它包含一个几何物体


400
00:26:10,170 --> 00:26:13,307 line:-1
现在 这个几何物体本身是一个骨架


401
00:26:13,473 --> 00:26:15,542 line:-1
这就是骨架的样子


402
00:26:15,976 --> 00:26:19,780 line:-2
它包含了节点和边界
就像其他几何物体一样


403
00:26:21,048 --> 00:26:23,050 line:-1
它也包含一个转换


404
00:26:24,084 --> 00:26:26,854 line:-1
这个转换只是锚点在


405
00:26:26,920 --> 00:26:29,489 line:-1
旋转和转换矩阵形式中的位置


406
00:26:30,257 --> 00:26:33,026 line:-2
在这里 访问骨架是我们主要
感兴趣的点


407
00:26:33,227 --> 00:26:34,595 line:-1
那么 我们开始吧


408
00:26:36,096 --> 00:26:38,131 line:-1
我们展现给你的这个骨架是什么呢？


409
00:26:38,432 --> 00:26:39,466 line:-1
它是一个由节点组成的几何体


410
00:26:39,666 --> 00:26:43,170 line:-1
它代表节点


411
00:26:43,637 --> 00:26:46,273 line:-1
你看到的绿色的点和黄色的点


412
00:26:46,607 --> 00:26:49,276 line:-1
它包含了表示着骨头的边界


413
00:26:49,343 --> 00:26:50,811 line:-1
你看到的白色的线


414
00:26:51,245 --> 00:26:53,413 line:-1
向你展示了节点被链接


415
00:26:55,382 --> 00:26:59,086 line:-2
只要所有的节点被连接了
你就能组建整个几何体了


416
00:27:00,287 --> 00:27:05,259 line:-2
我们称这个骨架为ARSkeleton
你可以通过骨架的属性


417
00:27:05,425 --> 00:27:07,661 line:-1
ARBodyAnchor来访问它


418
00:27:11,798 --> 00:27:15,836 line:-1
骨架的根节点 几何体最上面的点


419
00:27:15,903 --> 00:27:19,940 line:-2
在几何体的层级中
它是髋关节 你在这可以看到


420
00:27:21,074 --> 00:27:24,178 line:-2
那么 我们继续
来看一下它的结构的定义


421
00:27:25,145 --> 00:27:28,148 line:-2
这里所谓的定义
只是骨架的一个属性


422
00:27:28,482 --> 00:27:30,083 line:-1
它包含了两个组件


423
00:27:30,584 --> 00:27:33,487 line:-1
骨架中展示的所有关节点的名字


424
00:27:33,687 --> 00:27:36,223 line:-1
和它们之间的连接


425
00:27:36,290 --> 00:27:37,291 line:-1
它展示了如何将关节连接在一起


426
00:27:37,991 --> 00:27:40,260 line:-1
那么 我们来看一下这些属性


427
00:27:42,296 --> 00:27:46,166 line:-2
这里 我们在骨架上标记
了一些关节点 如你所见


428
00:27:46,233 --> 00:27:50,571 line:-2
这些关节点有具有意义的语义名字
比如左肩


429
00:27:50,637 --> 00:27:54,641 line:-1
右肩 左手 右手等等 和人类似


430
00:27:55,809 --> 00:27:58,812 line:-2
这里我想说明一下
绿色的点是我们控制的


431
00:27:58,879 --> 00:28:02,749 line:-1
它们是你看到的人身上估量出来的


432
00:28:03,016 --> 00:28:05,152 line:-1
黄色的是没有被跟随的


433
00:28:05,485 --> 00:28:08,488 line:-2
它们只是跟随离它们
最近的绿色的父节点


434
00:28:11,158 --> 00:28:15,429 line:-2
放大 关注右手
查看这三个关节点


435
00:28:15,495 --> 00:28:17,731 line:-1
右手 右胳膊肘和右肩


436
00:28:18,432 --> 00:28:20,601 line:-1
它们遵循了父子关系


437
00:28:21,134 --> 00:28:25,439 line:-2
你的手是胳膊肘的孩子
胳膊肘是肩的孩子


438
00:28:25,973 --> 00:28:28,742 line:-1
这也包含了骨架的剩余部分


439
00:28:29,009 --> 00:28:30,978 line:-1
以此来为你提供完整的层级


440
00:28:36,350 --> 00:28:39,953 line:-2
我们现在知道了所有被调用的关节点
以及如何连接它们


441
00:28:40,354 --> 00:28:42,289 line:-1
但我们如何确定它们的位置？


442
00:28:43,023 --> 00:28:46,293 line:-2
我们提供了两种方式来访问所有
关节点的位置


443
00:28:47,261 --> 00:28:49,830 line:-1
第一个是和它父辈有关联的


444
00:28:50,397 --> 00:28:53,667 line:-2
如果你想要你右手的位置关联到
胳膊肘


445
00:28:54,001 --> 00:28:57,237 line:-2
你可以通过调用localTransform
方法来转换


446
00:28:57,304 --> 00:29:00,107 line:-2
为它提供
.rightHand的参数


447
00:29:01,675 --> 00:29:04,945 line:-2
但如果你想要改变相关的根节点
在我们的示例中


448
00:29:05,012 --> 00:29:08,248 line:-2
是髋关节 你可以调用
modelTransform函数


449
00:29:08,315 --> 00:29:11,685 line:-2
以及再一次提供相同的
.rightHand参数


450
00:29:13,520 --> 00:29:17,057 line:-2
现在 如果你不想单独访问
所有关节点的转换


451
00:29:17,124 --> 00:29:21,962 line:-2
但你想要一个包含了所有
关节点转换的列表


452
00:29:22,362 --> 00:29:26,466 line:-2
你一可以通过使用
localTransforms


453
00:29:26,533 --> 00:29:28,402 line:-2
和modelTransforms
属性来实现


454
00:29:28,769 --> 00:29:33,874 line:-2
它会返回给你一个包含所有关节点
转换的列表


455
00:29:34,508 --> 00:29:37,311 line:-1
如果你想要它关联至你的父辈


456
00:29:37,377 --> 00:29:39,046 line:-2
你可以使用
localTransforms


457
00:29:39,213 --> 00:29:42,349 line:-2
如你想要它关联至根节点 你可使用
modelTransforms


458
00:29:44,618 --> 00:29:46,987 line:-2
所以 现在我们来仔细观察这个
骨架


459
00:29:47,221 --> 00:29:50,791 line:-2
我们来通过代码了解如何使用
其中的每一个元素


460
00:29:52,492 --> 00:29:55,529 line:-1
你从遍历场景中所有的锚点开始


461
00:29:56,263 --> 00:29:57,998 line:-1
只要找到bodyAnchor


462
00:29:59,099 --> 00:30:01,702 line:-2
只要你有了bodyAnchor
你可能会想要知道


463
00:30:01,768 --> 00:30:04,004 line:-2
bodyAnchor位于
世界坐标系中的哪个位置


464
00:30:05,305 --> 00:30:08,609 line:-2
你能使用bodyAnchor的
变换属性来获取


465
00:30:09,943 --> 00:30:13,046 line:-2
因为在我们的几何体中
髋是根部


466
00:30:14,047 --> 00:30:18,185 line:-2
所以bodyAnchor.transform会
返回髋关节在世界坐标系的位置


467
00:30:19,453 --> 00:30:22,523 line:-1
只要你有了锚点的变换


468
00:30:22,589 --> 00:30:25,792 line:-1
你可能需要访问几何体的锚点


469
00:30:25,859 --> 00:30:26,860 line:-1
可以通过使用锚点骨架属性来完成


470
00:30:28,295 --> 00:30:31,265 line:-2
当你有了这个几何体
你需要所有的关节点


471
00:30:31,331 --> 00:30:32,466 line:-1
你需要所有的节点


472
00:30:32,799 --> 00:30:35,836 line:-1
来获取所有关节点转换的列表


473
00:30:36,170 --> 00:30:39,506 line:-2
就是所有关节点的位置列表
你可以通过骨架的


474
00:30:39,573 --> 00:30:42,376 line:-2
jointModelTransforms
属性来获取


475
00:30:43,310 --> 00:30:48,248 line:-2
在这种情况下 当你有了列表之后
你就能遍历整个列表


476
00:30:48,315 --> 00:30:51,151 line:-2
以及访问每个元素或
每个关节点的转换


477
00:30:52,186 --> 00:30:57,024 line:-2
所以遍历所有的关节点
你可以通过定义中的


478
00:30:57,090 --> 00:31:00,661 line:-2
parentIndices
的属性来访问


479
00:31:00,727 --> 00:31:01,828 line:-2
每个关节点的
parentIndex


480
00:31:02,296 --> 00:31:05,098 line:-2
只需检查父节点是否为根节点
因为根节点


481
00:31:05,165 --> 00:31:07,968 line:-2
使整个层级中最顶部的点
所以它没有父节点


482
00:31:08,702 --> 00:31:14,441 line:-2
所以当父节点不是根节点时 你可以
使用相同的jointTransforms


483
00:31:14,675 --> 00:31:20,547 line:-2
列表访问父节点的转换 但需要使用
parentIndex来做索引


484
00:31:21,114 --> 00:31:22,115 line:-1
就是这些了


485
00:31:22,316 --> 00:31:26,653 line:-2
它为你提供整个层级中对应的
每个孩子


486
00:31:26,820 --> 00:31:30,891 line:-2
当你在层级中有了每个孩子和父亲
的配对


487
00:31:30,958 --> 00:31:32,860 line:-1
你就有了骨架的整个层级


488
00:31:34,194 --> 00:31:36,263 line:-1
你现在可以随时使用它


489
00:31:36,897 --> 00:31:38,365 line:-1
那么 我们来运行这段代码


490
00:31:38,432 --> 00:31:43,170 line:-2
使用它 我们来简单画一下这个骨架
来看它是什么样子


491
00:31:44,905 --> 00:31:46,206 line:-1
这是它的样子


492
00:31:46,640 --> 00:31:49,209 line:-2
我们所做的是
我们获取了整个层级


493
00:31:49,276 --> 00:31:53,380 line:-2
我们获取了所有的父亲和孩子的点
我们只是画了骨架 就是这样


494
00:31:53,914 --> 00:31:56,183 line:-1
它开始自动地模仿人了


495
00:31:56,783 --> 00:32:00,387 line:-2
这是你能做的最基本的事情
因为只要你有了


496
00:32:00,454 --> 00:32:04,358 line:-2
整个骨架层级
你可以使用它用在多种使用案例上


497
00:32:04,424 --> 00:32:05,626 line:-1
就如我们之前讲的那样


498
00:32:06,360 --> 00:32:10,430 line:-2
无论你的想象力把你带到哪里
你都可以用这个技术来实现


499
00:32:14,368 --> 00:32:19,206 line:-2
到现在为止 我们只了解了
世界坐标系中的3D物体


500
00:32:19,840 --> 00:32:24,344 line:-2
但 如果你想要平面空间的
2D版本的骨架


501
00:32:24,511 --> 00:32:27,314 line:-1
我们也有一个API能实现


502
00:32:28,115 --> 00:32:33,720 line:-2
这里 我们为你提供了在二维空间
访问每个元素详细的入口


503
00:32:35,355 --> 00:32:39,993 line:-2
我们也在一般的平面空间
提供了所有骨架关节点


504
00:32:41,361 --> 00:32:44,164 line:-1
使用这个API同样非常简单


505
00:32:45,766 --> 00:32:48,235 line:-1
你可以使用它做语义图片分析


506
00:32:48,302 --> 00:32:51,839 line:-1
或者为图片和视屏创建编辑工具


507
00:32:53,774 --> 00:32:57,511 line:-1
最后 整个结构都连接了一个物体


508
00:32:57,711 --> 00:32:59,713 line:-1
叫做ARBody2D


509
00:33:00,881 --> 00:33:04,985 line:-2
这就是视觉上ARBody2D
物件的样子


510
00:33:05,719 --> 00:33:09,790 line:-2
ARBody2D物件包含了整个
骨架的结构


511
00:33:13,861 --> 00:33:15,495 line:-1
这就是结构的样子


512
00:33:16,129 --> 00:33:18,599 line:-1
所以你在上面有了这个物件本身


513
00:33:18,799 --> 00:33:22,469 line:-2
接着 还是和3D响应的
所有骨架的元素


514
00:33:22,536 --> 00:33:23,537 line:-1
都在物件的下面


515
00:33:24,571 --> 00:33:25,873 line:-1
我们来看一下这个结构


516
00:33:25,939 --> 00:33:27,941 line:-1
快速了解这些元素


517
00:33:29,543 --> 00:33:32,012 line:-1
从上面的ARBody2D物件开始


518
00:33:33,547 --> 00:33:36,016 line:-1
你有两种访问这个物件的方式


519
00:33:36,850 --> 00:33:40,487 line:-2
如果你已经在二维空间了
默认访问它的方式是通过


520
00:33:40,554 --> 00:33:41,688 line:-1
vARFrame


521
00:33:41,955 --> 00:33:47,194 line:-2
你可以使用ARFrame的
detectedBody属性来实现


522
00:33:48,495 --> 00:33:51,999 line:-2
这里 这个人是ARBody2D物件
的一个实例


523
00:33:53,734 --> 00:33:57,204 line:-2
为了你使用方便
如果你在使用3D空间


524
00:33:57,404 --> 00:34:01,008 line:-2
因为某些原因
如果你已经在做一个3D的骨架了


525
00:34:01,074 --> 00:34:04,945 line:-1
你也想要二维空间对应的2D骨架


526
00:34:05,779 --> 00:34:11,185 line:-2
通过使用ARBodyAnchor的
referenceBody属性


527
00:34:11,351 --> 00:34:12,585 line:-2
我们为你提供了一个
直接的方式来访问


528
00:34:13,719 --> 00:34:16,523 line:-2
它也会为你返回
ARBody2D物件


529
00:34:18,725 --> 00:34:22,862 line:-2
访问完ARBody2D物件时
我们可以使用骨架的属性


530
00:34:22,929 --> 00:34:24,864 line:-1
从中取出骨架


531
00:34:26,099 --> 00:34:28,902 line:-1
这就是那个骨架的视觉图


532
00:34:30,070 --> 00:34:34,107 line:-2
就像我说过的
这个骨架画在了一般的二维空间里


533
00:34:34,608 --> 00:34:38,745 line:-2
所以 如果这是你的图像网格
左上的坐标是0,0


534
00:34:38,812 --> 00:34:40,447 line:-1
右下是1,1


535
00:34:40,514 --> 00:34:45,018 line:-2
示例图中的所有位置坐标都
在0到1之间


536
00:34:45,085 --> 00:34:47,087 line:-1
x和y方向都是


537
00:34:49,956 --> 00:34:52,926 line:-1
你看到的绿色的点被称为标记点


538
00:34:53,293 --> 00:34:56,730 line:-2
请注意 我们在这里并不称他们为
关节点 虽然它们代表着关节


539
00:34:56,797 --> 00:35:00,200 line:-2
我们称它们为标记点的原因是
因为它们是图片上的像素坐标


540
00:35:03,103 --> 00:35:07,241 line:-2
和3D版本一样 它包含一个
definition对象


541
00:35:07,307 --> 00:35:10,577 line:-1
描述哪一个标记点在骨架中被调用


542
00:35:10,644 --> 00:35:12,546 line:-1
以及如何连接这些标记点


543
00:35:14,348 --> 00:35:16,750 line:-1
在这个骨架中 有16个关节点


544
00:35:17,017 --> 00:35:20,621 line:-2
和3D类似 它们具有语义上有意义
的名字


545
00:35:20,687 --> 00:35:24,892 line:-1
比如左肩 右肩 左手和右手等等


546
00:35:25,192 --> 00:35:29,263 line:-2
根节点还在髋关节这里
还是和3D非常类似


547
00:35:31,965 --> 00:35:34,568 line:-1
那么 请关注右手


548
00:35:34,635 --> 00:35:37,437 line:-2
我们看到这只手是右胳膊肘
的一个孩子


549
00:35:37,504 --> 00:35:39,573 line:-1
胳膊肘是右肩的一个孩子


550
00:35:39,806 --> 00:35:43,010 line:-1
再一次 这和你在3D版本中看到的


551
00:35:43,076 --> 00:35:45,412 line:-1
父子关系类似


552
00:35:46,079 --> 00:35:48,682 line:-1
这个类似的层级关系在这里构成


553
00:35:50,951 --> 00:35:54,988 line:-2
那么 了解了所有的信息
现在我们来快速通过代码


554
00:35:55,055 --> 00:35:56,056 line:-1
了解一下这个结构


555
00:35:57,057 --> 00:35:59,826 line:-1
我们从访问ARBody2D物件开始


556
00:36:01,695 --> 00:36:06,066 line:-2
所以 只要你有了ARFRame
你就能使用detectedBody属性


557
00:36:06,133 --> 00:36:07,701 line:-1
来获取ARBody2D物件


558
00:36:08,168 --> 00:36:10,737 line:-1
现在 只要你有了ARBody2D物件


559
00:36:10,804 --> 00:36:14,141 line:-1
你就能访问整个内部的骨架结构


560
00:36:14,842 --> 00:36:18,111 line:-2
你可以通过使用person.skeleton
来提取几何体


561
00:36:18,979 --> 00:36:22,182 line:-2
现在 person指向了ARBody2D
物件


562
00:36:22,950 --> 00:36:26,987 line:-2
骨架的definition再一次包含了
所有关节点的名字


563
00:36:27,054 --> 00:36:31,058 line:-1
和如何连接这些关节点的信息


564
00:36:31,124 --> 00:36:34,761 line:-2
它在definition中被展示
你可以通过使用骨架的


565
00:36:34,828 --> 00:36:36,830 line:-1
definition属性来访问它


566
00:36:38,365 --> 00:36:40,000 line:-1
只要你有了这些信息


567
00:36:40,167 --> 00:36:43,604 line:-2
你可能需要知道这些标记点
的定位在哪里


568
00:36:43,871 --> 00:36:47,641 line:-2
和3D版本类似 我们有一个叫做
jointLandmarks属性


569
00:36:47,708 --> 00:36:52,646 line:-2
它提供了你看到的所有绿色点的
坐标列表


570
00:36:53,547 --> 00:36:56,383 line:-2
但请注意 这里的绿色的点都是
2D的


571
00:36:56,450 --> 00:36:57,718 line:-1
所以它们处于二维空间


572
00:36:57,784 --> 00:37:00,621 line:-1
它们是标准的像素坐标


573
00:37:01,688 --> 00:37:05,225 line:-2
只要你有了那个列表
你可以遍历所有的这些标记点


574
00:37:05,692 --> 00:37:08,428 line:-2
对于每一个标记点来说
你通过调用definition的


575
00:37:08,795 --> 00:37:11,932 line:-2
parentIndices属性
来访问它的父节点


576
00:37:12,399 --> 00:37:15,269 line:-2
再一次 只需检查它的父节点是否为
根节点


577
00:37:15,335 --> 00:37:17,704 line:-1
因为根节点处于点的层级中的最顶层


578
00:37:18,205 --> 00:37:23,010 line:-2
如果父节点不是根节点 你可以通过
jointLandmarks


579
00:37:23,076 --> 00:37:26,547 line:-2
列表访问它的转换
使用parentIndex来索引


580
00:37:27,314 --> 00:37:29,349 line:-1
这种方式下


581
00:37:29,583 --> 00:37:34,721 line:-1
你在骨架层级中


582
00:37:34,788 --> 00:37:36,089 line:-1
有了每个父子对的转换


583
00:37:36,390 --> 00:37:38,692 line:-2
如果你想的话
你可以再一次使用它


584
00:37:38,759 --> 00:37:41,461 line:-1
你可以在这里继续完成你想做的事


585
00:37:42,863 --> 00:37:44,731 line:-1
我们来总结一下


586
00:37:47,668 --> 00:37:51,371 line:-1
我们介绍了今年AR的动作捕捉


587
00:37:52,873 --> 00:37:56,844 line:-2
我们提供了实时追踪人的访问
方式


588
00:37:58,912 --> 00:38:04,952 line:-1
我们同时提供了3D和2D骨架


589
00:38:05,886 --> 00:38:08,755 line:-1
这就是我们连接人体姿势的方式


590
00:38:10,591 --> 00:38:13,861 line:-1
我们支持了立刻同步的角色动画


591
00:38:14,661 --> 00:38:16,964 line:-2
它在RealityKit中
运行平稳


592
00:38:18,498 --> 00:38:20,367 line:-2
我们有了一个之前讨论过的
RealityKit API


593
00:38:20,634 --> 00:38:24,004 line:-1
来快速让角色动起来


594
00:38:24,338 --> 00:38:27,808 line:-2
就像我之前提到的
你也可以使用你自定义的角色


595
00:38:27,875 --> 00:38:32,412 line:-2
只要它是基于我们提供的示例结构
构建的


596
00:38:34,047 --> 00:38:38,118 line:-2
我们介绍了一个你可能会想到的
用于识别任务或分析任务


597
00:38:38,185 --> 00:38:42,689 line:-1
的高阶用法的ARKit API


598
00:38:46,260 --> 00:38:48,061 line:-1
今天的演讲就要结束了


599
00:38:48,629 --> 00:38:51,198 line:-1
我们介绍了两个功能


600
00:38:51,431 --> 00:38:53,433 line:-1
真人遮挡剔除和动作捕捉


601
00:38:53,700 --> 00:38:57,838 line:-1
我们为这些功能都提供了API


602
00:38:59,072 --> 00:39:03,110 line:-2
了解今天演讲的更多信息
请访问我们的网站


603
00:39:03,343 --> 00:39:06,513 line:-1
请随意下载示例代码并使用它


604
00:39:07,314 --> 00:39:08,782 line:-1
我们明天会在实验室


605
00:39:08,849 --> 00:39:11,418 line:-1
你可以带着你的问题来找我们


606
00:39:15,522 --> 00:39:16,523 line:-1
谢谢


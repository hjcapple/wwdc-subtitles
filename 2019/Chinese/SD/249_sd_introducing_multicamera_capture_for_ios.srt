1
00:00:06,939 --> 00:00:10,844 line:0
（为iOS引入多镜头捕捉）


2
00:00:10,911 --> 00:00:12,212 line:-1
多镜头捕捉


3
00:00:12,646 --> 00:00:14,481 line:-1
或者像我们圈内一样喜欢把它叫做


4
00:00:15,048 --> 00:00:16,183 line:-1
MultiCam


5
00:00:18,151 --> 00:00:21,955 line:-2
MultiCam是呼声最高的一个
第三方功能


6
00:00:22,022 --> 00:00:24,324 line:-1
我们年复一年地听到关于它的消息


7
00:00:24,725 --> 00:00:25,826 line:-1
那么我们今天要讲的是


8
00:00:25,893 --> 00:00:28,795 line:-1
同时使用多镜头和多路麦克


9
00:00:29,296 --> 00:00:33,634 line:-1
同时捕捉视频、


10
00:00:33,934 --> 00:00:36,737 line:-1
音频、元数据、深度和照片


11
00:00:39,173 --> 00:00:41,708 line:-2
不过第三方并不是唯一一个
受益于此的


12
00:00:42,109 --> 00:00:46,480 line:-2
我们还收到了许多来自
第一方的客户们反复提出的需求


13
00:00:46,780 --> 00:00:48,015 line:-1
针对MultiCam捕捉


14
00:00:48,348 --> 00:00:50,083 line:-1
其中主要的因素是ARKit


15
00:00:50,384 --> 00:00:52,085 line:-1
如果你看了主题演讲


16
00:00:52,452 --> 00:00:55,022 line:-2
你一定听到了
对ARKit 3的介绍


17
00:00:55,522 --> 00:00:59,626 line:-2
这些API使用前置镜头
追踪面部和鼻子


18
00:00:59,893 --> 00:01:02,462 line:-1
并同时使用后置镜头追踪周边环境


19
00:01:02,796 --> 00:01:05,899 line:-2
这会帮助他们了解
要把虚拟角色放在场景中的哪个位置


20
00:01:06,099 --> 00:01:07,401 line:-1
通过了解你正在凝望什么


21
00:01:08,836 --> 00:01:11,338 line:-2
那么自AVFoundation的
首次出现起


22
00:01:11,505 --> 00:01:14,374 line:-2
我们就已经
在Mac上支持MultiCam了


23
00:01:14,675 --> 00:01:16,543 line:-1
可以一直追溯到Lion操作系统


24
00:01:17,077 --> 00:01:18,178 line:-1
但在iOS上


25
00:01:18,412 --> 00:01:22,449 line:-2
AVFoundation客户们
每次都只能有一个激活的镜头


26
00:01:22,850 --> 00:01:24,685 line:-1
并不是因为我们打算这样做


27
00:01:24,885 --> 00:01:26,386 line:-1
我们有许多很好的理由


28
00:01:26,753 --> 00:01:28,822 line:-1
第一个理由是硬件限制


29
00:01:29,289 --> 00:01:32,192 line:-1
我指的是镜头分担了电源的电量


30
00:01:32,593 --> 00:01:35,562 line:-1
它本身不能提供足够的电量


31
00:01:35,629 --> 00:01:38,131 line:-1
同时全量支持两个摄像头


32
00:01:38,799 --> 00:01:40,200 line:-1
第二个理由是


33
00:01:40,834 --> 00:01:43,470 line:-1
我们期待发布一个可靠的API


34
00:01:43,704 --> 00:01:46,240 line:-1
可以帮助你不让手机发烫


35
00:01:46,473 --> 00:01:51,144 line:-1
当多个摄像头同时消耗电源时


36
00:01:51,345 --> 00:01:53,180 line:-2
因此我们希望能确保
我们给你们提交的东西


37
00:01:53,247 --> 00:01:54,481 line:-1
可以帮助你


38
00:01:54,681 --> 00:01:57,718 line:-1
处理硬件、热量和带宽约束


39
00:01:57,885 --> 00:01:59,686 line:-1
这在我们的世界中是现实存在的


40
00:02:00,354 --> 00:02:04,758 line:-2
好的 在iOS 13中有个好消息
我们支持MultiCam捕捉了


41
00:02:05,058 --> 00:02:06,994 line:-2
并且我们在当前全部硬件上
都实现了这个功能


42
00:02:07,160 --> 00:02:12,533 line:-2
iPhone XS、XS Max
XR和新出的iPad Pro


43
00:02:13,100 --> 00:02:14,635 line:-1
在全部平台上


44
00:02:14,701 --> 00:02:17,771 line:-2
前面提到的硬件限制已经解决了
谢天谢地


45
00:02:19,473 --> 00:02:21,441 line:-1
那么让我们直接来看有趣的部分吧


46
00:02:21,508 --> 00:02:25,546 line:-2
我们开发了一些新API
用于创建MultiCam会话


47
00:02:26,880 --> 00:02:29,583 line:-2
如你曾用AVFoundation
来进行镜头捕捉


48
00:02:29,650 --> 00:02:32,286 line:-2
你一定了解我们有四个主要的类
分别是输入、


49
00:02:32,586 --> 00:02:37,090 line:-1
输出、会话和连接


50
00:02:38,725 --> 00:02:41,562 line:-2
AVCaptureSession
是重中之重


51
00:02:41,628 --> 00:02:43,463 line:-1
它用于排列数据


52
00:02:43,797 --> 00:02:46,133 line:-1
它用于表明开始或停止运行


53
00:02:47,501 --> 00:02:50,971 line:-2
你把它添加到一个或多个输入中
AVCapture输入


54
00:02:51,271 --> 00:02:53,674 line:-2
其中一个这样的是
AVCaptureDeviceInput


55
00:02:53,740 --> 00:02:57,044 line:-1
它要么是摄像头要么是麦克的包装器


56
00:02:58,378 --> 00:03:02,316 line:-2
你还需要添加一个或多个
AVCapture输出 用于接收数据


57
00:03:02,549 --> 00:03:04,618 line:-1
否则那些生产出来的东西就无处安置


58
00:03:05,619 --> 00:03:08,589 line:-2
然后会话以你的名义
以兼容的媒体类型


59
00:03:08,856 --> 00:03:12,092 line:-1
在输入和输出之间自动创建连接


60
00:03:12,793 --> 00:03:16,096 line:-2
请注意 我展示的是iOS上的传统
AVCaptureSession


61
00:03:16,296 --> 00:03:20,167 line:-1
每个会话只允许一个镜头输入


62
00:03:21,768 --> 00:03:25,873 line:-2
iOS 13中的新功能是我们引入了
AVCaptureSession的一个子类


63
00:03:25,939 --> 00:03:28,509 line:-1
叫做AVCaptureMultiCamSession


64
00:03:29,009 --> 00:03:32,145 line:-1
允许你执行多个输入和输出


65
00:03:33,480 --> 00:03:36,917 line:-2
我们并没有抛弃
AVCaptureSession


66
00:03:36,984 --> 00:03:38,218 line:-1
它不会消失


67
00:03:38,552 --> 00:03:42,356 line:-2
当前的AVCaptureSession
仍然是优选类


68
00:03:42,422 --> 00:03:44,224 line:-1
当你执行单镜头捕捉时


69
00:03:44,625 --> 00:03:47,094 line:-2
原因是
MultiCamSession


70
00:03:47,361 --> 00:03:50,430 line:-1
作为一个强大的工具有一些限制


71
00:03:50,664 --> 00:03:51,965 line:-1
我稍后会讲到


72
00:03:53,800 --> 00:03:56,937 line:-1
好的 那么让我给你一个基本用例


73
00:03:57,171 --> 00:03:59,406 line:-2
针对新的
AVCaptureMultiCamSession


74
00:03:59,806 --> 00:04:01,608 line:-2
假如你想对MultiCamSession
添加两个设备


75
00:04:01,675 --> 00:04:03,677 line:-2
一个用作前置镜头
一台个用作后置镜头


76
00:04:04,278 --> 00:04:07,848 line:-1
并同时执行两个视频数据输出


77
00:04:08,148 --> 00:04:11,752 line:-2
一个设备收到后置镜头的视频流
而另一个收到前置镜头的视频流


78
00:04:12,819 --> 00:04:15,889 line:-1
那么假设如果你想执行实时预览


79
00:04:15,956 --> 00:04:19,760 line:-2
你可添加独立的VideoPreviewLayers
一个给前置镜头 一个给后置镜头


80
00:04:20,793 --> 00:04:22,095 line:-1
不过不要止步于此


81
00:04:22,963 --> 00:04:23,897 line:-1
你可以执行


82
00:04:25,399 --> 00:04:28,502 line:-1
同时元数据输出 如果你希望


83
00:04:28,569 --> 00:04:31,705 line:-1
同时执行条形码扫描或面部检测的话


84
00:04:32,506 --> 00:04:34,808 line:-1
你可以执行电影文件的多个输出


85
00:04:34,875 --> 00:04:37,211 line:-2
如果你想用一个设备记录前景
一个设备记录后景的话


86
00:04:37,477 --> 00:04:40,414 line:-2
你可以添加照片的多个输出
如果你想


87
00:04:40,747 --> 00:04:43,550 line:-2
从不同的摄像头
执行实时照片捕捉的话


88
00:04:43,884 --> 00:04:46,954 line:-2
那么正如你所看到的
这些图表开始看起来很复杂


89
00:04:47,287 --> 00:04:50,190 line:-2
大量输入和大量输出中
掺杂着大量箭头


90
00:04:51,425 --> 00:04:54,061 line:-2
那些小箭头叫做
AVCaptureConnections


91
00:04:54,228 --> 00:04:57,164 line:-1
它们定义了输入到输出的数据流


92
00:04:57,698 --> 00:05:00,067 line:-1
让我暂时关注一下设备输入


93
00:05:00,434 --> 00:05:02,169 line:-1
从而剖析一下连接


94
00:05:03,570 --> 00:05:07,908 line:-2
捕捉输入有AVCapture
输入端口


95
00:05:08,375 --> 00:05:11,512 line:-1
我喜欢把它看作是个小插座


96
00:05:12,012 --> 00:05:16,316 line:-2
每个媒体类型都有一个插座
可以容纳输入


97
00:05:17,184 --> 00:05:18,919 line:-1
如果端口上没有插入任何东西


98
00:05:19,119 --> 00:05:22,122 line:-2
那个端口就没有视频流
就像插座一样


99
00:05:22,189 --> 00:05:24,057 line:-1
你必须插点东西才能获得电流


100
00:05:24,558 --> 00:05:27,895 line:-2
为了找出每个特定的输入
有哪些可用的端口


101
00:05:28,228 --> 00:05:31,098 line:-1
你可以查询那个输入的端口属性


102
00:05:31,265 --> 00:05:34,868 line:-2
它会告诉你说“我有这些
AVCapture输入端口”


103
00:05:35,169 --> 00:05:36,370 line:-1
对于双摄像头来说


104
00:05:36,637 --> 00:05:40,140 line:-2
它们是以下这些的端口
一个捕捉视频 一个捕捉深度


105
00:05:40,574 --> 00:05:43,977 line:-2
一个捕捉元数据对象
比如二维码扫描和面部扫描


106
00:05:44,144 --> 00:05:48,048 line:-2
一个捕捉元数据项
元数据项可以被钩到电影文件输出上


107
00:05:49,550 --> 00:05:52,586 line:-2
现在无论何时当你使用
AVCaptureSession的添加输入方法


108
00:05:52,653 --> 00:05:54,454 line:-1
来向会话中添加输入


109
00:05:54,721 --> 00:05:58,258 line:-2
或使用它的添加输出方法
来向会话中添加输出时


110
00:05:58,926 --> 00:06:01,595 line:-1
会话都会查找兼容的媒体类型


111
00:06:01,662 --> 00:06:04,231 line:-1
和隐藏的表单连接 如果可以的话


112
00:06:04,631 --> 00:06:06,500 line:-2
因此我们在这里有一个
VideoDataOutput


113
00:06:06,867 --> 00:06:09,736 line:-2
VideoDataOutput
收到视频并接受视频


114
00:06:10,103 --> 00:06:13,040 line:-1
并且我们有一个电插头可以产生视频


115
00:06:13,240 --> 00:06:15,309 line:-1
因此会自动进行连接


116
00:06:15,809 --> 00:06:17,945 line:-1
大多数人都习惯那样


117
00:06:18,178 --> 00:06:20,180 line:-2
处理
AVCaptureSession


118
00:06:20,247 --> 00:06:21,915 line:-1
如果你之前曾用过我们的类的话


119
00:06:22,549 --> 00:06:24,818 line:-2
MultiCamSession
完全不一样


120
00:06:25,452 --> 00:06:28,088 line:-1
那是因为输入和输出


121
00:06:28,322 --> 00:06:30,991 line:-1
现在多个输入可以配备多个输出


122
00:06:31,058 --> 00:06:33,560 line:-1
你很可能想要确保连接发生在


123
00:06:33,861 --> 00:06:37,998 line:-2
A到A和B到B 而不会在
你不希望它们交叉的地方发生交叉


124
00:06:38,398 --> 00:06:41,168 line:-2
因此当创建
MultiCamSession时


125
00:06:41,435 --> 00:06:45,672 line:-2
我们强烈要求你不要使用
隐藏的连接表单


126
00:06:46,173 --> 00:06:50,711 line:-2
而是使用这些特别目的的添加工具
添加不带任何连接的输入


127
00:06:51,011 --> 00:06:53,380 line:-1
或添加不带任何连接的输出


128
00:06:53,847 --> 00:06:57,017 line:-1
同样地


129
00:06:57,551 --> 00:06:59,720 line:-1
你可以用于视频预览层的那些


130
00:07:00,053 --> 00:07:02,723 line:-1
就是setSessionWithNoConnections


131
00:07:03,056 --> 00:07:06,326 line:-2
当你使用它们时
它们基本上只会告诉会话说


132
00:07:06,627 --> 00:07:08,495 line:-1
“这是输入 这是输出


133
00:07:08,562 --> 00:07:11,265 line:-1
你现在知道它们了 但请不要碰它们


134
00:07:11,431 --> 00:07:14,568 line:-2
我稍后会手动添加
我所想要添加的连接”


135
00:07:15,469 --> 00:07:19,239 line:-2
添加方式就是自己创建
AVCaptureConnection


136
00:07:19,473 --> 00:07:23,710 line:-2
并告诉它“我希望你把这个
或这些端口连接到这个输出上”


137
00:07:24,044 --> 00:07:26,513 line:-1
然后告诉会话说“请添加这个连接”


138
00:07:26,780 --> 00:07:27,814 line:-1
然后一切就绪


139
00:07:29,049 --> 00:07:30,184 line:-1
我太啰嗦了


140
00:07:30,384 --> 00:07:34,154 line:-2
演示比口头说说要好的多
因此我想让Nik Gelo上台


141
00:07:34,621 --> 00:07:38,425 line:-2
他同样来自摄像头软件组 将给大家
演示AVMultiCamPIP


142
00:07:38,759 --> 00:07:39,593 line:-1
Nik？


143
00:07:40,827 --> 00:07:41,662 line:-1
谢谢Brad


144
00:07:42,362 --> 00:07:43,797 line:-2
AVMultiCamPIP
是一款app


145
00:07:43,864 --> 00:07:46,733 line:-2
用于演示
同时来自前后镜头的视频流


146
00:07:46,934 --> 00:07:48,669 line:-1
在这里我们有两个视频预览


147
00:07:48,735 --> 00:07:51,471 line:-2
一个显示的是前置镜头
一个显示的是后置镜头


148
00:07:51,772 --> 00:07:55,075 line:-2
当我双击屏幕时
我可以切换让哪个镜头呈现全屏


149
00:07:55,409 --> 00:07:56,710 line:-1
哪个镜头呈现PIP


150
00:08:07,754 --> 00:08:11,692 line:-2
现在我们可以在这里看到Brad
正在Apple Park里


151
00:08:12,392 --> 00:08:14,561 line:-1
在我向他提问之前


152
00:08:14,995 --> 00:08:16,897 line:-1
我会按下底部的录制按钮


153
00:08:17,130 --> 00:08:18,532 line:-1
稍后再观看这段采访


154
00:08:19,366 --> 00:08:22,169 line:-2
嘿Brad 那么告诉我
Apple Park怎么样？


155
00:08:22,636 --> 00:08:25,038 line:-2
Nik
Apple Park一片嘈杂


156
00:08:25,239 --> 00:08:26,874 line:-1
你可以看到在倒影池前边


157
00:08:26,940 --> 00:08:28,909 line:-1
正在举办各种活动


158
00:08:29,176 --> 00:08:30,511 line:-1
我听到了水流的哗哗声


159
00:08:30,811 --> 00:08:32,980 line:-1
听起来我随时都会被淋湿


160
00:08:33,413 --> 00:08:36,650 line:-2
我听到我后边有野兽的声音
像是鸭子之类的东西


161
00:08:36,850 --> 00:08:38,919 line:-1
老实说我非常担忧我在这里的生活


162
00:08:39,186 --> 00:08:40,821 line:-1
嗯 Brad 那听起来太恐怖了


163
00:08:40,888 --> 00:08:41,889 line:-1
希望你保持安全


164
00:08:41,955 --> 00:08:42,923 line:-1
好的 谢谢


165
00:08:43,222 --> 00:08:44,057 line:-1
录好了


166
00:08:44,558 --> 00:08:46,493 line:-1
那么现在 我们完成了对电影的录制


167
00:08:46,560 --> 00:08:48,462 line:-1
让我们看看我们刚刚录下了什么


168
00:08:51,331 --> 00:08:52,499 line:-1
这是电影


169
00:08:53,033 --> 00:08:56,670 line:-2
正如你所看到的
当我在两个镜头之间切换时


170
00:08:56,737 --> 00:09:00,073 line:-1
就跟我们使用app时一样


171
00:09:00,874 --> 00:09:02,209 line:-1
这就是AVMultiCamPIP


172
00:09:02,476 --> 00:09:03,343 line:-1
舞台交还给Brad


173
00:09:07,948 --> 00:09:09,483 line:-1
谢谢Nik 演示很棒


174
00:09:11,385 --> 00:09:14,922 line:-2
好的 那么让我们看看
在AVMultiCamPIP后台发生了什么


175
00:09:15,422 --> 00:09:18,158 line:-2
那么我们有两个设备输入
一个是前置镜头的输入


176
00:09:18,425 --> 00:09:19,626 line:-1
一个是后置镜头的输入


177
00:09:20,427 --> 00:09:23,697 line:-2
不带任何连接
就像我刚才所提到的那样


178
00:09:23,864 --> 00:09:28,302 line:-2
我们还有两个视频数据输出
前后镜头各一个


179
00:09:28,735 --> 00:09:30,204 line:-2
还有两个VideoPreviewLayers
现在要把它们放在屏幕上


180
00:09:30,437 --> 00:09:32,940 line:-2
只需要把那些
VideoPreviewLayers排序


181
00:09:33,207 --> 00:09:37,845 line:-2
把一个放在另一个的上层
并且有一个尺寸较小


182
00:09:38,212 --> 00:09:39,780 line:-1
当Nik双击它们时


183
00:09:40,013 --> 00:09:43,217 line:-2
我们要简单地重定位它们
并颠倒Z排序


184
00:09:43,617 --> 00:09:47,221 line:-2
在Metal Shader Compositor代码中
有一些神奇的事情


185
00:09:47,554 --> 00:09:50,090 line:-2
它合成了那两个
VideoDataOutputs


186
00:09:50,357 --> 00:09:54,127 line:-2
从而把较小的PIP
安排到一个框架内


187
00:09:54,394 --> 00:09:57,531 line:-2
因此它是把它们
合成到单一视频缓冲器中


188
00:09:57,831 --> 00:09:59,900 line:-2
然后把它们发送给
AVAssetWriter


189
00:10:00,133 --> 00:10:03,036 line:-2
在AVAssetWriter中
它们被记录到电影的一个视频轨道中


190
00:10:03,637 --> 00:10:06,173 line:-1
你现在可以使用这段示例代码了


191
00:10:06,440 --> 00:10:07,774 line:-1
它与研讨会相关联


192
00:10:07,841 --> 00:10:10,978 line:-2
你可以查看并开始做
自己的MultiCam捕捉了


193
00:10:12,446 --> 00:10:15,048 line:-1
好的 是时候谈谈限制了


194
00:10:15,115 --> 00:10:18,352 line:-2
虽然AVMultiCamSession
是一个强大的工具


195
00:10:18,652 --> 00:10:19,987 line:-1
但它并不能实现一切功能


196
00:10:20,287 --> 00:10:22,689 line:-1
让我来告诉你它不能做什么


197
00:10:24,057 --> 00:10:28,061 line:-1
首先你不能假装一个镜头是两个镜头


198
00:10:28,595 --> 00:10:32,065 line:-1
AVCaptureDeviceInput API会让你


199
00:10:32,132 --> 00:10:35,302 line:-1
创建为比如说后置镜头创建多个实例


200
00:10:35,536 --> 00:10:36,970 line:-1
如果你想的话 你可以做十个


201
00:10:37,271 --> 00:10:41,475 line:-2
但如果你尝试把全部实例添加到一个
MultiCamSession中 它会说“哼哼”


202
00:10:42,176 --> 00:10:43,810 line:-1
并且会引发异常


203
00:10:44,578 --> 00:10:47,948 line:-2
在会话中
请对每个镜头仅做一个输入


204
00:10:48,916 --> 00:10:51,185 line:-1
同时你也不可以克隆镜头


205
00:10:51,451 --> 00:10:53,487 line:-1
来获得两个同样类型的输出


206
00:10:53,954 --> 00:10:59,126 line:-2
比如把一个镜头的信号
拆分为两个视频数据输出


207
00:10:59,593 --> 00:11:01,895 line:-1
当然了 你可以添加多个镜头


208
00:11:01,962 --> 00:11:04,631 line:-2
并把每一个都连接到一个
VideoDataOutput


209
00:11:04,865 --> 00:11:07,100 line:-1
但你不能散开 不能一对多


210
00:11:10,537 --> 00:11:13,974 line:-1
反之也不可以


211
00:11:14,508 --> 00:11:18,145 line:-2
AVCapture在iOS上
的输出不支持媒体混合


212
00:11:18,545 --> 00:11:21,715 line:-2
因此所有数据输出
只能处理一个单一的输入


213
00:11:22,282 --> 00:11:24,818 line:-2
你不可以 比如说
尝试把两个镜头源


214
00:11:24,885 --> 00:11:27,621 line:-1
挤压为一个单一数据输出


215
00:11:27,688 --> 00:11:29,456 line:-1
它不知道如何处理第二个视频


216
00:11:29,523 --> 00:11:31,225 line:-1
因为它不知道如何混合它们


217
00:11:31,792 --> 00:11:34,828 line:-2
当然 你可以使用
独立的视频数据输出


218
00:11:35,195 --> 00:11:37,865 line:-2
然后在你自己的代码中
合成那些缓冲器


219
00:11:37,931 --> 00:11:41,101 line:-2
比如我们在MultiCamPIP中
使用的Metal Shader Compositor


220
00:11:41,568 --> 00:11:42,970 line:-1
不管怎样做都可以


221
00:11:43,237 --> 00:11:45,172 line:-1
只要考虑到会话创建就好


222
00:11:45,239 --> 00:11:48,041 line:-2
不要尝试把多镜头
挤压到一个单一输出中


223
00:11:49,443 --> 00:11:51,311 line:-1
好的 提一下预设


224
00:11:52,412 --> 00:11:56,149 line:-2
传统的AVCaptureSession
对于会话预设的概念是


225
00:11:56,483 --> 00:12:00,320 line:-2
给整个会话规定一个
一般性的服务质量


226
00:12:01,255 --> 00:12:04,191 line:-2
它应用于那个会话内的
全部输入和输出


227
00:12:04,424 --> 00:12:07,828 line:-2
比如当你把
sessionPreset设为高时


228
00:12:08,795 --> 00:12:12,232 line:-1
会话配置设备的分辨率和帧频


229
00:12:12,666 --> 00:12:15,068 line:-1
以及全部输出 从而使它们提交


230
00:12:15,135 --> 00:12:18,672 line:-2
一个高频时的视频体验
比如1080p30


231
00:12:21,742 --> 00:12:24,111 line:-2
预设对于MultiCamSession来说
是个问题


232
00:12:24,878 --> 00:12:27,347 line:-1
再想想一些诸如此类的东西


233
00:12:29,082 --> 00:12:33,086 line:-2
MultiCamSession
配置是混合配置 具有多样化


234
00:12:33,387 --> 00:12:35,923 line:-1
整个都是高品质意味着什么呢？


235
00:12:36,223 --> 00:12:38,458 line:-1
你可能想在图表的不同分支上


236
00:12:38,792 --> 00:12:40,460 line:-1
执行不同的服务品质


237
00:12:40,928 --> 00:12:43,664 line:-1
比如在前置镜头上 你可能只想执行


238
00:12:43,931 --> 00:12:46,466 line:-2
一个低分辨率预览
比如640乘480


239
00:12:47,334 --> 00:12:49,636 line:-1
而同时在后置镜头上执行


240
00:12:49,803 --> 00:12:53,073 line:-2
非常高品质的预览
比如1080p60


241
00:12:53,440 --> 00:12:57,077 line:-2
嗯 很明显 我们没有针对
这些混合状况的预设


242
00:12:57,911 --> 00:13:00,981 line:-2
我们决定在MultiCamSession中
让事情变得简单一些


243
00:13:01,381 --> 00:13:02,950 line:-1
它不支持预设


244
00:13:03,016 --> 00:13:07,487 line:-2
它支持一个也只支持一个预设
就是输入优先


245
00:13:07,855 --> 00:13:11,892 line:-2
那么那意味着当你添加输入和输出时
它不会处理输入和输出


246
00:13:12,192 --> 00:13:14,828 line:-1
你必须自己配置活动格式


247
00:13:17,798 --> 00:13:20,234 line:-1
好的 再谈谈消耗函数


248
00:13:20,868 --> 00:13:24,238 line:-2
我在开始就提到过我们花了很多时间
来做这个MultiCam支持


249
00:13:24,304 --> 00:13:27,407 line:-2
因为我们希望提交一个
非常可靠的API


250
00:13:27,841 --> 00:13:31,445 line:-1
可以帮助你解决在运行多摄像头


251
00:13:31,678 --> 00:13:34,214 line:-1
和点亮手机上每一个区块


252
00:13:34,581 --> 00:13:35,883 line:-1
所付出的各种代价


253
00:13:39,520 --> 00:13:41,321 line:-1
那么这很老套 但却是事实


254
00:13:42,022 --> 00:13:43,924 line:-1
天下没有免费的午餐


255
00:13:43,991 --> 00:13:47,528 line:-2
那么在研讨会的这个部分
我是你的前辈


256
00:13:48,362 --> 00:13:49,730 line:-2
我要跟你们做一次
长辈对后辈的经验传授


257
00:13:50,898 --> 00:13:53,567 line:-2
在接下来的经验传授中
我会解释信用卡是如何运作的


258
00:13:54,034 --> 00:13:56,170 line:-1
以及你要如何对你的钱负责


259
00:13:56,436 --> 00:14:00,407 line:-2
并用你的方式生活
以及比如这种东西


260
00:14:01,508 --> 00:14:05,512 line:-2
那么生活的现实就是我们在iOS上
拥有有限的硬件带宽


261
00:14:06,113 --> 00:14:09,516 line:-2
然而我们有多个摄像头
因此我们有多个传感器


262
00:14:09,850 --> 00:14:13,654 line:-2
但我们只有一个ISP
或图像信号处理器


263
00:14:14,388 --> 00:14:17,591 line:-1
因此流过这些传感器的全部像素


264
00:14:17,925 --> 00:14:21,995 line:-1
都要由一个ISP来处理


265
00:14:22,462 --> 00:14:27,234 line:-2
它限制了每个给定的时钟频率
可以运行多少像素


266
00:14:28,068 --> 00:14:30,637 line:-1
因此限制了


267
00:14:30,704 --> 00:14:32,973 line:-1
一次可以运行多少像素


268
00:14:33,507 --> 00:14:37,878 line:-2
硬件增加消耗正如你所期待的那样
是视频分辨率


269
00:14:38,312 --> 00:14:41,014 line:-2
较高的分辨率意味着可以填充
更多的像素


270
00:14:41,615 --> 00:14:42,950 line:-1
最大帧频


271
00:14:43,217 --> 00:14:45,152 line:-1
如果像素提交的速度更快


272
00:14:45,219 --> 00:14:47,654 line:-1
那么时钟频率的像素就越多


273
00:14:48,956 --> 00:14:51,091 line:-2
还有第三个
你可能听说过或也可能没听说过


274
00:14:51,158 --> 00:14:52,526 line:-1
叫做传感器装箱


275
00:14:53,694 --> 00:14:56,930 line:-2
传感器装箱指的是在相邻像素中
合并信息的一种方式


276
00:14:56,997 --> 00:14:59,266 line:-1
用于减少带宽


277
00:14:59,766 --> 00:15:01,435 line:-1
那么比如说这里有一张图片


278
00:15:01,768 --> 00:15:03,504 line:-1
我们执行一次二乘二装箱


279
00:15:03,804 --> 00:15:07,741 line:-2
它会拥有四个像素方格
并把它们合并为一个方格


280
00:15:08,275 --> 00:15:11,345 line:-1
从而尺寸上就缩减了4x


281
00:15:11,778 --> 00:15:13,614 line:-1
它为你减少了噪音


282
00:15:14,147 --> 00:15:15,949 line:-1
它为你减少了带宽


283
00:15:16,350 --> 00:15:19,086 line:-1
它为每个像素提供4x强度


284
00:15:19,520 --> 00:15:21,522 line:-1
那么传感器装箱还有许多好处


285
00:15:21,889 --> 00:15:25,592 line:-1
负面消息是图像品质稍差


286
00:15:25,859 --> 00:15:28,295 line:-2
那么对角线可能看起来
有点像阶梯式


287
00:15:28,962 --> 00:15:33,767 line:-2
但它做出了补偿
就是装箱格式的功率超级低


288
00:15:34,501 --> 00:15:37,171 line:-2
事实上无论何时
当你使用ARKit的摄像头时


289
00:15:37,371 --> 00:15:41,408 line:-2
你就正在使用装箱格式
因为装箱格式是ARKit专用的


290
00:15:41,475 --> 00:15:44,211 line:-1
从而节省功率去实现


291
00:15:44,278 --> 00:15:45,212 line:-1
你想要实现的其它有意思的AR功能


292
00:15:46,246 --> 00:15:50,184 line:-2
好的 我们该如何解释消耗
或者说我们该如何报告消耗呢？


293
00:15:50,751 --> 00:15:52,953 line:-2
MultiCamSession
会在你配置会话时


294
00:15:53,020 --> 00:15:54,555 line:-1
结算硬件消耗


295
00:15:54,988 --> 00:15:56,557 line:-1
因此每一次当你修改了什么东西时


296
00:15:56,957 --> 00:16:00,160 line:-2
它会持续追踪它
就像填满一辆购物车


297
00:16:00,527 --> 00:16:02,396 line:-1
或逛在线商店一样


298
00:16:02,462 --> 00:16:04,765 line:-2
把东西都放在购物车上
然后再付款


299
00:16:05,032 --> 00:16:07,668 line:-1
你知道何时会接近预算的极限


300
00:16:07,901 --> 00:16:09,703 line:-1
并且你可以比如说把东西丢掉


301
00:16:10,137 --> 00:16:12,239 line:-1
然后放进新东西或去掉那些旧东西


302
00:16:12,306 --> 00:16:14,241 line:-1
你在付款之前能看到费用


303
00:16:14,708 --> 00:16:16,710 line:-2
这对于MultiCamSession来说
也一样


304
00:16:17,177 --> 00:16:20,647 line:-2
我们有一个新属性叫做
hardwareCost


305
00:16:21,381 --> 00:16:24,451 line:-2
当你创建一个全新的会话时
hardwareCost从零开始


306
00:16:25,485 --> 00:16:29,590 line:-2
并随着你添加更多的功能、
更多的输入、更多的输出而增加


307
00:16:30,390 --> 00:16:32,826 line:-1
只要处于1.0以下就没什么问题


308
00:16:33,293 --> 00:16:35,395 line:-1
1.0以下一切都运行无误


309
00:16:36,496 --> 00:16:39,733 line:-2
只要你达到1.0及以上
你就有麻烦了


310
00:16:40,167 --> 00:16:43,036 line:-1
那是因为ISP带宽限制很严格


311
00:16:43,437 --> 00:16:44,571 line:-1
它不像你可以


312
00:16:45,372 --> 00:16:47,341 line:-1
你知道的 每隔一帧提交一次


313
00:16:47,641 --> 00:16:49,843 line:-2
不 这是一个
要么全部要么全无的提案


314
00:16:50,043 --> 00:16:51,879 line:-1
你要么能实现或要么不能实现


315
00:16:52,179 --> 00:16:54,581 line:-2
因此如果你超过了1.0
并且你尝试运行


316
00:16:54,915 --> 00:16:57,718 line:-2
AVCaptureMultiCamSession
它会说“哼哼”


317
00:16:58,085 --> 00:16:59,753 line:-1
它会给你一个通知


318
00:17:00,020 --> 00:17:03,490 line:-2
关于运行时间报错的通知
表明不得不停止的原因


319
00:17:03,557 --> 00:17:05,459 line:-1
是因为硬件消耗超出


320
00:17:08,628 --> 00:17:11,064 line:-2
现在你很可能正在思考
“我该如何缩减那个消耗呢？”


321
00:17:11,732 --> 00:17:15,002 line:-2
最显而易见的方式就是
选择较低的分辨率


322
00:17:15,636 --> 00:17:18,739 line:-2
另一种方式就是
如果你想保持同样的分辨率


323
00:17:19,540 --> 00:17:21,842 line:-1
如果在同一个分辨率上有装箱格式


324
00:17:21,909 --> 00:17:23,042 line:-1
那就选那个


325
00:17:23,277 --> 00:17:26,246 line:-1
虽然品质有点儿低 但功率也低


326
00:17:27,714 --> 00:17:30,951 line:-1
接下来你可能会想降低帧频会有帮助


327
00:17:32,085 --> 00:17:32,953 line:-1
但并不会


328
00:17:33,720 --> 00:17:35,822 line:-2
原因是
AVCaptureDevice


329
00:17:36,223 --> 00:17:39,293 line:-1
允许你在运行时修改帧频


330
00:17:39,693 --> 00:17:42,095 line:-1
并且我认为是自iOS 4起


331
00:17:42,596 --> 00:17:44,898 line:-1
就开始允许你这样做了


332
00:17:45,566 --> 00:17:49,036 line:-2
那么你可能会说
“把活动格式设为60”


333
00:17:49,736 --> 00:17:52,706 line:-2
你仍然需要付出120的消耗
而不是60的消耗


334
00:17:52,773 --> 00:17:54,508 line:-1
因为在运行时的任意一点上


335
00:17:54,575 --> 00:17:56,910 line:-1
你可能会把帧频增加到120


336
00:17:56,977 --> 00:17:58,645 line:-1
我们必须考虑到最糟糕的情况


337
00:17:59,613 --> 00:18:01,315 line:-1
但有个好消息


338
00:18:01,381 --> 00:18:05,352 line:-2
我们在AVCaptureDeviceInput上
提供一个覆盖属性


339
00:18:06,086 --> 00:18:10,224 line:-2
通过设置它
你可以把高帧频格式转换成


340
00:18:10,290 --> 00:18:12,125 line:-1
低帧频格式


341
00:18:12,559 --> 00:18:17,231 line:-2
通过承诺
你不会超过某个指定帧频来实现


342
00:18:19,900 --> 00:18:23,570 line:-1
这是API中的混淆点


343
00:18:23,637 --> 00:18:25,572 line:-1
我们不会把帧频看作是比率


344
00:18:25,639 --> 00:18:27,107 line:-1
我们把它们看作是持续时间


345
00:18:27,341 --> 00:18:31,411 line:-2
因此要设置帧频
你要设置持续时间为1


346
00:18:31,478 --> 00:18:32,813 line:-1
这对于帧频来说一样


347
00:18:33,146 --> 00:18:36,950 line:-2
如果你想把60 FPS格式
变成30 FPS格式


348
00:18:37,518 --> 00:18:41,889 line:-2
通过做一个CMTime实现
1比30 其中1是持续时间


349
00:18:42,256 --> 00:18:47,895 line:-2
然后把deviceInput的videoMinFram
DurationOverride设为30 FPS


350
00:18:48,228 --> 00:18:50,731 line:-1
恭喜你 你已经把60 FPS格式


351
00:18:50,797 --> 00:18:53,901 line:-2
转成30 FPS格式 你只付出了
30 FPS格式的硬件消耗


352
00:18:56,336 --> 00:19:00,007 line:-2
我还要提一下 在AVMultiCamPIP
app中有一个很棒的功能


353
00:19:00,073 --> 00:19:04,278 line:-1
演示了如何反复地缩减消耗


354
00:19:04,578 --> 00:19:07,981 line:-2
它是个递归函数
它会选择对它来说最重要的东西


355
00:19:08,382 --> 00:19:11,251 line:-1
并限制那些比较不重要的东西


356
00:19:11,318 --> 00:19:12,819 line:-1
直到低于硬件消耗为止


357
00:19:13,787 --> 00:19:15,756 line:-1
接下来是系统压力消耗


358
00:19:15,822 --> 00:19:18,959 line:-1
这是我们发现的第二个消耗


359
00:19:19,760 --> 00:19:20,761 line:-1
相信也你清楚地知道


360
00:19:21,628 --> 00:19:23,730 line:-1
手机是非常强大的计算器


361
00:19:23,797 --> 00:19:27,034 line:-1
它是一个小的、面临热挑战的设备


362
00:19:27,668 --> 00:19:32,739 line:-2
并在在iOS 11中我们引入了
摄像系统压力状态


363
00:19:33,307 --> 00:19:35,809 line:-1
帮助你监控摄像头的当前状况


364
00:19:37,044 --> 00:19:39,112 line:-1
设想系统压力包含


365
00:19:39,713 --> 00:19:43,083 line:-2
系统温度 全部OS热峰值功率
都会要求系统温度


366
00:19:44,351 --> 00:19:47,221 line:-1
那与电池有关


367
00:19:47,554 --> 00:19:49,456 line:-1
它目前有多少余额呢？


368
00:19:49,957 --> 00:19:52,693 line:-2
它是否可以足够迅速地
升高它的电压


369
00:19:52,759 --> 00:19:56,063 line:-2
以满足运行需求呢
无论你现在想要做什么


370
00:19:56,964 --> 00:19:59,299 line:-1
还有红外线探照灯温度


371
00:19:59,933 --> 00:20:02,202 line:-2
在支持TrueDepth摄像头
的设备上


372
00:20:02,269 --> 00:20:04,938 line:-1
我们有红外摄像头以及RGB摄像头


373
00:20:05,105 --> 00:20:06,640 line:-1
嗯 那会产生它自己的热量


374
00:20:06,707 --> 00:20:09,676 line:-2
那也给系统压力状态
造成了一部分负担


375
00:20:12,346 --> 00:20:15,749 line:-2
其中五个自始至终
从“标准”变成 “关机”


376
00:20:16,283 --> 00:20:19,453 line:-2
当系统压力状态是标准时
你的状态不错


377
00:20:19,520 --> 00:20:20,487 line:-1
你可以做一切你想做的事


378
00:20:20,921 --> 00:20:23,857 line:-2
当它是合理时 你还可以继续
做几乎一切你想做的事


379
00:20:24,291 --> 00:20:26,660 line:-2
但如果它是严重时
你开始进入这样一种情况


380
00:20:26,727 --> 00:20:28,262 line:-1
系统即将减慢速度


381
00:20:28,896 --> 00:20:31,431 line:-1
意味着GPU循环数量变少了


382
00:20:31,498 --> 00:20:35,035 line:-1
可能会降低品质


383
00:20:35,502 --> 00:20:38,272 line:-1
如果是危险状态 会有一大堆节制


384
00:20:38,338 --> 00:20:42,776 line:-2
如果是关机
我们就再也不能运行摄像头了


385
00:20:43,110 --> 00:20:44,912 line:-1
害怕会伤害硬件


386
00:20:45,078 --> 00:20:48,482 line:-2
在关机情况下我们将自动中断
你的会话 停止它


387
00:20:48,949 --> 00:20:52,085 line:-2
并告诉你由于系统压力状态不好
而导致中断


388
00:20:52,452 --> 00:20:55,689 line:-2
然后我们等待设备
完全恢复到标准状态


389
00:20:55,756 --> 00:20:57,424 line:-1
然后才会让你再次运行摄像头


390
00:20:58,725 --> 00:21:01,195 line:-1
那么这是iOS 11


391
00:21:01,595 --> 00:21:03,130 line:-1
在iOS 13中


392
00:21:03,497 --> 00:21:08,202 line:-2
我们提供一种可以提前了解
系统压力消耗的方法 OK？


393
00:21:08,602 --> 00:21:11,171 line:-1
我们不仅告诉你当前所发生的


394
00:21:11,505 --> 00:21:13,640 line:-1
可能会受到重启摄像头之前


395
00:21:14,007 --> 00:21:16,376 line:-1
产生冲突的事实的影响


396
00:21:17,044 --> 00:21:19,913 line:-2
我们现在有一种方式
可以告诉你摄像头消耗


397
00:21:19,980 --> 00:21:23,383 line:-2
以及系统压力是多少
不依赖任何其它因素


398
00:21:23,884 --> 00:21:25,986 line:-1
因此增加消耗的东西


399
00:21:26,053 --> 00:21:27,855 line:-1
与硬件消耗增加一样


400
00:21:28,055 --> 00:21:29,456 line:-1
还有许多其它的东西


401
00:21:29,857 --> 00:21:32,359 line:-1
比如视频图像稳定性


402
00:21:32,860 --> 00:21:34,628 line:-1
或光学图像稳定性


403
00:21:34,695 --> 00:21:36,363 line:-1
这些全都需要消耗电源


404
00:21:36,697 --> 00:21:38,799 line:-2
我们有一个
Smart HDR功能等等


405
00:21:39,032 --> 00:21:40,601 line:-1
这里所列出的一切


406
00:21:40,667 --> 00:21:43,303 line:-1
都会增加整个系统的压力消耗


407
00:21:45,272 --> 00:21:49,142 line:-2
MultiCamSession可提前计算该分值
就像它对硬件所做的那样


408
00:21:50,010 --> 00:21:53,046 line:-1
并且它会考虑到它所了解的因素


409
00:21:53,280 --> 00:21:56,917 line:-2
因此如果你要同时执行一些
原生的GPU处理


410
00:21:57,317 --> 00:21:58,752 line:-1
分值将不会包含它


411
00:21:58,819 --> 00:22:00,921 line:-1
它只包含你通过摄像头所做的事


412
00:22:03,023 --> 00:22:03,924 line:-1
这是如何使用它


413
00:22:05,125 --> 00:22:07,327 line:-1
通过查询系统压力消耗


414
00:22:07,528 --> 00:22:10,831 line:-1
你可以了解在一个否则静态系统中


415
00:22:11,131 --> 00:22:13,534 line:-1
还能正常运行多久


416
00:22:13,834 --> 00:22:17,104 line:-2
因此如果它小于1.0
你可以无限期运行


417
00:22:17,171 --> 00:22:18,205 line:-1
你是一个很酷的客户


418
00:22:18,839 --> 00:22:20,240 line:-1
如果它在1和2之间


419
00:22:20,507 --> 00:22:22,576 line:-1
你应该最多可以运行15分钟


420
00:22:22,943 --> 00:22:25,078 line:-1
如果在2和3之间 最多10分钟


421
00:22:25,512 --> 00:22:29,149 line:-2
如果高于3
你可能只能运行很短的时间


422
00:22:29,783 --> 00:22:33,287 line:-2
事实上 即使你超过3
我们仍然会允许你运行摄像头


423
00:22:33,720 --> 00:22:36,823 line:-1
但你必须了解它不会保持太长时间


424
00:22:37,124 --> 00:22:39,626 line:-1
一旦它上升到危险或关机层级


425
00:22:39,860 --> 00:22:41,662 line:-1
会话将被中断


426
00:22:41,728 --> 00:22:43,830 line:-1
我们要拯救硬件 即使你不想


427
00:22:43,897 --> 00:22:46,733 line:-2
但 嘿 这很棒
如果你可以在30秒的运行时间内


428
00:22:47,034 --> 00:22:49,036 line:-1
完成你想要完成的事情的话


429
00:22:49,102 --> 00:22:51,405 line:-1
但要付出非常非常高的系统压力消耗


430
00:22:51,605 --> 00:22:52,739 line:-1
务必要这样做


431
00:22:55,108 --> 00:22:57,711 line:-2
现在该如何在运行时
减少系统压力呢？


432
00:22:57,778 --> 00:22:59,913 line:-1
我不是指会话的配置


433
00:22:59,980 --> 00:23:02,616 line:-1
我说的是一旦你已经开始运行


434
00:23:02,783 --> 00:23:05,552 line:-2
并且你注意到你已经开始增加
系统压力的情况


435
00:23:05,719 --> 00:23:08,288 line:-2
最迅速、最简单的方法就是
降低帧频


436
00:23:08,622 --> 00:23:11,258 line:-1
那会立即减轻系统压力


437
00:23:11,725 --> 00:23:14,261 line:-2
同时 如果你正在做一些
我们不知道的事 比如


438
00:23:14,528 --> 00:23:17,531 line:-2
耗费GPU或CPU的操作
你可以有所节制


439
00:23:18,465 --> 00:23:21,802 line:-1
还有最后一招 你可以尝试


440
00:23:21,869 --> 00:23:24,204 line:-2
禁用你正在使用的一个
或多个摄像头


441
00:23:25,005 --> 00:23:28,976 line:-2
AVMultiCamSession
有一个很灵巧的小功能


442
00:23:29,343 --> 00:23:30,410 line:-1
当运行时


443
00:23:30,777 --> 00:23:34,481 line:-2
你可以禁用其中一个摄像头
而不影响另一个摄像头的预览


444
00:23:34,548 --> 00:23:35,949 line:-1
我们不会把所有摄像头都关掉


445
00:23:36,316 --> 00:23:38,352 line:-1
因此如果你正在运行前后摄像头


446
00:23:38,418 --> 00:23:41,788 line:-2
你注意到你即将超出预算
并且很快会进入危险状态


447
00:23:42,322 --> 00:23:44,491 line:-1
你可以选择关闭前置摄像头


448
00:23:44,558 --> 00:23:46,026 line:-1
后置摄像头将继续预览


449
00:23:46,093 --> 00:23:48,428 line:-1
它不会丢失它的焦点、曝光或白平衡


450
00:23:48,929 --> 00:23:53,166 line:-1
并且当你关闭你想禁用的摄像头上的


451
00:23:53,567 --> 00:23:55,569 line:-1
最后一个活跃的输入端口时


452
00:23:55,769 --> 00:23:59,206 line:-2
通过把它输入端口的启动属性
设为假来实现


453
00:23:59,773 --> 00:24:02,743 line:-2
我们将终止摄像头的视频流
并节约大量电源


454
00:24:03,010 --> 00:24:04,912 line:-1
为系统提供一次冷却的机会


455
00:24:07,014 --> 00:24:09,950 line:-2
好的 那么我刚讲了
两个非常重要的消耗


456
00:24:10,017 --> 00:24:11,451 line:-1
硬件消耗和系统压力消耗


457
00:24:12,152 --> 00:24:14,188 line:-2
还有一些
我们没有报告出来的其它消耗


458
00:24:14,254 --> 00:24:16,256 line:-1
如果你不希望欺骗自己相信


459
00:24:16,323 --> 00:24:18,091 line:-1
没有其它可以增加消耗的功能


460
00:24:18,158 --> 00:24:20,661 line:-1
当然会有其它消耗 比如内存


461
00:24:21,061 --> 00:24:23,964 line:-1
但在iOS 13中我们人为地限制


462
00:24:24,231 --> 00:24:27,067 line:-1
设备组合 我们允许你运行


463
00:24:27,301 --> 00:24:31,638 line:-2
我们认为可以正常运行
并且不会让你陷入麻烦中的组合


464
00:24:32,873 --> 00:24:36,877 line:-2
因此我们对所支持的
设备组合的数量有限制


465
00:24:37,144 --> 00:24:40,214 line:-2
我列出了在iPhone XS上
所支持的组合


466
00:24:40,514 --> 00:24:42,850 line:-2
这有点儿像视力表
我不期待你能记住它


467
00:24:42,916 --> 00:24:44,117 line:-1
你稍后可以暂停视频


468
00:24:44,418 --> 00:24:46,053 line:-1
但有六个所支持的配置


469
00:24:46,119 --> 00:24:48,088 line:-1
并且你要记住的简单规则就是


470
00:24:48,155 --> 00:24:50,824 line:-1
你不可以同时运行两个物理摄像头


471
00:24:51,258 --> 00:24:54,661 line:-2
你可能会问比如
Brad 配置一个怎么样？


472
00:24:54,728 --> 00:24:56,129 line:-1
只有一个复选框


473
00:24:56,496 --> 00:24:58,065 line:-1
因为它是双摄像头


474
00:24:58,131 --> 00:25:00,234 line:-1
并且双摄像头是一种软件摄像头


475
00:25:00,300 --> 00:25:02,769 line:-1
它实际上包含广角和远距镜头


476
00:25:02,836 --> 00:25:04,438 line:-1
因此它是两个物理摄像头


477
00:25:05,739 --> 00:25:07,808 line:-2
如何了解是否支持
MultiCam呢？


478
00:25:08,008 --> 00:25:10,344 line:-2
正如我所说过的那样
只有较先进的硬件才支持它


479
00:25:10,677 --> 00:25:15,249 line:-2
因此你需要查看
MultiCamSession


480
00:25:15,315 --> 00:25:16,416 line:-2
是否允许你在你所拥有的设备上
运行多个摄像头


481
00:25:16,984 --> 00:25:19,419 line:-2
有一个类方法叫做
isMultiCamSupported


482
00:25:19,486 --> 00:25:21,955 line:-1
你可以立刻决定是或否


483
00:25:22,856 --> 00:25:24,324 line:-1
然后进一步当你想决定


484
00:25:24,391 --> 00:25:27,027 line:-1
我是否可以一起运行这个设备组合时


485
00:25:27,427 --> 00:25:30,130 line:-2
你可以对你感兴趣的设备创建一个
AVCaptureDeviceDiscoverySession


486
00:25:30,364 --> 00:25:33,901 line:-1
然后请求它的新属性


487
00:25:34,168 --> 00:25:36,503 line:-1
supportedMultiCamDeviceSets


488
00:25:36,737 --> 00:25:39,573 line:-1
这将生成大量无序集合


489
00:25:39,773 --> 00:25:42,109 line:-1
那会告诉你可以同时使用哪些组合


490
00:25:42,976 --> 00:25:47,614 line:-1
接下来是我们人为限制


491
00:25:47,681 --> 00:25:48,682 line:-1
你可以运行的格式


492
00:25:49,316 --> 00:25:53,120 line:-2
我刚查看的在iPhone XS中
所支持的格式


493
00:25:53,320 --> 00:25:55,756 line:-1
与后置摄像头有关的有40多种格式


494
00:25:55,822 --> 00:25:57,257 line:-1
因此可以选择的格式有很多很多


495
00:25:57,457 --> 00:25:59,393 line:-1
但我们限制


496
00:25:59,459 --> 00:26:01,195 line:-2
有MultiCamSession
的视频格式


497
00:26:01,395 --> 00:26:03,830 line:-1
因为这些是我们可以随心所欲地


498
00:26:03,897 --> 00:26:06,466 line:-1
在终端设备上同时运行的格式


499
00:26:06,934 --> 00:26:08,302 line:-1
这有点儿像视力表


500
00:26:08,368 --> 00:26:10,204 line:-1
但我希望你们注意一下组


501
00:26:11,071 --> 00:26:12,906 line:-1
第一组是装箱格式


502
00:26:13,106 --> 00:26:15,509 line:-2
还记得吗？低功率 是的
这是我们的朋友


503
00:26:15,676 --> 00:26:18,212 line:-2
在传感器上
你得到了那个2乘2的装箱


504
00:26:18,278 --> 00:26:20,080 line:-1
因此你收获了非常低的功率


505
00:26:20,781 --> 00:26:22,983 line:-1
低于60 FPS时全部这些都可用


506
00:26:23,050 --> 00:26:27,354 line:-2
你可以从640乘480
一直到1920乘1440之间选择


507
00:26:28,789 --> 00:26:32,025 line:-2
下一组是
1920乘1080 30 FPS


508
00:26:32,092 --> 00:26:33,927 line:-1
这是一种非装箱格式


509
00:26:34,161 --> 00:26:35,863 line:-1
与你对常规传统会话


510
00:26:35,929 --> 00:26:39,299 line:-1
选择高预设时的格式一样


511
00:26:39,766 --> 00:26:41,835 line:-1
它对MultiCam可用


512
00:26:42,169 --> 00:26:46,607 line:-2
最后一组是1920乘1440
非装箱格式 30 FPS


513
00:26:47,007 --> 00:26:49,476 line:-1
这是一个很好的图片格式的替换格式


514
00:26:49,943 --> 00:26:53,680 line:-2
我们并不是在所有摄像头上
都支持1200万像素


515
00:26:53,981 --> 00:26:56,416 line:-1
那对于手机来说是一件糟糕的事


516
00:26:56,750 --> 00:27:00,587 line:-2
但我们允许你以30 FPS执行
1920乘1440


517
00:27:00,654 --> 00:27:04,358 line:-2
请注意 它仍然允许你执行
1200万像素高分辨率拍照


518
00:27:04,691 --> 00:27:06,360 line:-1
因此这是一个很好的代理


519
00:27:06,660 --> 00:27:11,031 line:-1
当你想通过多摄像头同时摄像时


520
00:27:13,100 --> 00:27:16,270 line:-2
现在你要如何了解某格式是否支持
MultiCam呢？


521
00:27:16,336 --> 00:27:17,304 line:-1
你只需要问它就可以了


522
00:27:17,604 --> 00:27:20,974 line:-2
因此当在格式之间迭代时 你可以说
“是否支持MultiCam？”


523
00:27:21,375 --> 00:27:22,876 line:-1
如果它支持 你就可以使用它


524
00:27:23,343 --> 00:27:26,513 line:-2
在这里的代码中
我在一台设备上进行格式迭代


525
00:27:26,780 --> 00:27:29,550 line:-2
并选择下一个支持
MultiCam的最低的分辨率


526
00:27:29,850 --> 00:27:33,287 line:-1
然后把它设置为我的活动格式


527
00:27:35,189 --> 00:27:37,357 line:-1
我们进行人为限制的还有最后一点


528
00:27:37,858 --> 00:27:40,727 line:-1
因为我们需要报告消耗


529
00:27:40,894 --> 00:27:43,830 line:-2
由MultiCamSession
报告消耗


530
00:27:44,331 --> 00:27:46,867 line:-1
在iOS上我们明确不支持


531
00:27:47,134 --> 00:27:50,404 line:-1
app中多摄像头的多重会话


532
00:27:50,470 --> 00:27:52,873 line:-1
并且同时我们也不支持


533
00:27:52,940 --> 00:27:55,008 line:-1
多个应用中的多摄像头


534
00:27:55,242 --> 00:27:59,580 line:-2
请注意iOS上的支持
仍然限制于一次一个会话


535
00:27:59,880 --> 00:28:02,249 line:-2
但当然了
你可以同时运行多个摄像头


536
00:28:03,984 --> 00:28:06,086 line:-1
那么我的经验传授结束了


537
00:28:06,587 --> 00:28:07,821 line:-1
好的 写不错的代码


538
00:28:08,288 --> 00:28:09,356 line:-1
11点前要回家


539
00:28:09,623 --> 00:28:11,058 line:-1
如果你的计划有变 给我打电话


540
00:28:12,125 --> 00:28:12,993 line:-1
好的


541
00:28:13,460 --> 00:28:15,162 line:-2
好的 现在让我们返回到
有意思的部分


542
00:28:16,363 --> 00:28:17,531 line:-1
同步视频流


543
00:28:19,833 --> 00:28:21,902 line:-1
我提到了软件摄像头


544
00:28:22,169 --> 00:28:25,939 line:-2
一个软件摄像头有两个镜头
我们在iPhone 7 Plus上引入了它


545
00:28:26,306 --> 00:28:29,409 line:-2
现在它也在iPhone XS
和XS Max上使用


546
00:28:29,977 --> 00:28:32,513 line:-2
TrueDepth摄像头
也是另外一种软件摄像头


547
00:28:32,579 --> 00:28:33,714 line:-1
因为它由


548
00:28:34,147 --> 00:28:36,683 line:-2
一个红外摄像头
和一个RGB摄像头组成


549
00:28:36,950 --> 00:28:40,921 line:-2
这就能利用这两者的不同之处
实现深度


550
00:28:42,022 --> 00:28:46,126 line:-2
我们从未
给摄像头的这些特殊类型命名


551
00:28:46,627 --> 00:28:47,728 line:-1
但我们现在要这样做


552
00:28:47,794 --> 00:28:50,297 line:-2
在iOS 13中我们把它们叫做
虚拟摄像头


553
00:28:50,864 --> 00:28:52,232 line:-1
DualCam就是其中一种


554
00:28:52,666 --> 00:28:55,369 line:-1
它一次呈现一个视频流


555
00:28:55,969 --> 00:29:00,073 line:-1
并根据你的缩放系数转换


556
00:29:00,140 --> 00:29:02,176 line:-1
如果你接近2x


557
00:29:02,242 --> 00:29:05,412 line:-2
它会转换成远距摄像头
而不是广角摄像头


558
00:29:05,779 --> 00:29:07,581 line:-1
它还能针对深度出奇招


559
00:29:07,748 --> 00:29:11,818 line:-2
因为它有两个图像
可以用于生成它们之间的不同点


560
00:29:12,319 --> 00:29:13,520 line:-1
但从你的角度来说


561
00:29:13,587 --> 00:29:16,089 line:-1
你依然只能一次获得一个视频流


562
00:29:17,791 --> 00:29:22,162 line:-2
因为它们现在有名字了 它们还可
作为API的属性 你可以查询它


563
00:29:22,229 --> 00:29:24,798 line:-2
因此当你查看摄像头设备时
你可以用编程方式查找


564
00:29:25,165 --> 00:29:27,568 line:-1
这是否是一个虚拟设备？


565
00:29:27,968 --> 00:29:31,471 line:-2
如果它是 你可以问它
“嗯 哪个是你的物理设备？”


566
00:29:31,905 --> 00:29:35,409 line:-2
在API中我们把这个叫做它的
constituentDevices


567
00:29:37,578 --> 00:29:39,780 line:-1
同步视频流就是


568
00:29:39,980 --> 00:29:41,715 line:-2
获取虚拟设备的
constituentDevices


569
00:29:42,216 --> 00:29:45,786 line:-1
并同步运行它们


570
00:29:46,086 --> 00:29:47,654 line:-1
换句话说就是


571
00:29:48,088 --> 00:29:50,424 line:-1
我们首次允许你同时拍摄


572
00:29:50,490 --> 00:29:52,826 line:-2
来自广角摄像头
和远距摄像头的同步视频


573
00:29:53,861 --> 00:29:56,930 line:-1
你要继续在虚拟设备上设置属性


574
00:29:56,997 --> 00:29:58,665 line:-1
而不是在constituentDevices上


575
00:30:00,234 --> 00:30:02,469 line:-1
那里有一些规则


576
00:30:03,170 --> 00:30:04,671 line:-1
当你运行虚拟设备时


577
00:30:04,938 --> 00:30:08,141 line:-2
constituentDevices
不允许你乱糟糟地运行


578
00:30:08,909 --> 00:30:10,911 line:-1
它们有相同的活动分辨率


579
00:30:10,978 --> 00:30:12,279 line:-1
它们有相同的帧频


580
00:30:12,846 --> 00:30:15,649 line:-1
从硬件层级上来说 它们是同步的


581
00:30:15,983 --> 00:30:21,321 line:-2
那意味着传感器
会同步读取那些视频流


582
00:30:21,388 --> 00:30:25,926 line:-1
因此视频流的中间行


583
00:30:26,260 --> 00:30:28,595 line:-1
一定对应着同样的时钟时间


584
00:30:29,429 --> 00:30:32,132 line:-1
那意味着它们的中心帧相匹配


585
00:30:32,332 --> 00:30:37,004 line:-2
它还意味着曝光、
白平衡和聚焦是相继发生的


586
00:30:37,070 --> 00:30:40,274 line:-1
非常棒 看起来几乎是同一个摄像头


587
00:30:40,440 --> 00:30:42,976 line:-1
只是碰巧是两个不同的视野


588
00:30:45,913 --> 00:30:49,716 line:-2
最好演示一下 而不是说说而已
让我们做一个演示


589
00:30:49,783 --> 00:30:51,785 line:-1
这个叫做AVDualCam


590
00:30:53,220 --> 00:30:54,054 line:-1
好了


591
00:30:55,055 --> 00:30:57,424 line:-2
好的
AVDualCam会让你了解


592
00:30:57,791 --> 00:30:59,793 line:-1
虚拟摄像头会看到那些东西


593
00:30:59,860 --> 00:31:03,263 line:-2
通过给你展示一个两个摄像头
同步运行的显示屏


594
00:31:04,665 --> 00:31:07,901 line:-2
它这样做是为了给你展示
摄像头所拍摄的不同场景


595
00:31:08,635 --> 00:31:09,803 line:-1
好的 在这里我们有


596
00:31:10,270 --> 00:31:13,941 line:-1
同步运行的双摄像头的广角摄像头


597
00:31:14,241 --> 00:31:15,375 line:-1
和远距摄像头所拍摄的视频流


598
00:31:15,609 --> 00:31:18,679 line:-2
左侧是广角摄像头拍摄的
右侧是远距摄像头拍摄的


599
00:31:19,379 --> 00:31:20,347 line:-1
不相信我吗？


600
00:31:20,914 --> 00:31:22,583 line:-1
在这里我要把我的手指放到一侧


601
00:31:24,218 --> 00:31:25,786 line:-2
然后再把我的手指放到另一侧
看到了吗？


602
00:31:26,119 --> 00:31:27,020 line:-1
它们是不同的摄像头


603
00:31:29,523 --> 00:31:31,925 line:-1
我对广角摄像头所做的事就是放大它


604
00:31:31,992 --> 00:31:33,894 line:-1
从而让它与远距摄像头的视野一样


605
00:31:34,394 --> 00:31:37,698 line:-2
但你可以注意到
它们完美地同步运行着


606
00:31:37,764 --> 00:31:39,233 line:-1
没什么问题


607
00:31:39,299 --> 00:31:41,401 line:-1
场消隐也没有什么古怪


608
00:31:41,902 --> 00:31:45,272 line:-1
它们的曝光和聚焦同时发生了改变


609
00:31:46,306 --> 00:31:49,409 line:-2
我们可以再把它变得更有意思点儿
我们把肩并肩视图


610
00:31:50,077 --> 00:31:51,578 line:-1
变为分屏视图


611
00:31:51,979 --> 00:31:53,747 line:-1
现在看起来有点儿困难


612
00:31:54,047 --> 00:31:57,818 line:-1
但左侧是广角摄像头的拍摄内容


613
00:31:58,252 --> 00:31:59,319 line:-1
右侧是远距摄像头的拍摄内容


614
00:31:59,386 --> 00:32:01,255 line:-1
那么我只展示了每帧的一半


615
00:32:02,256 --> 00:32:05,526 line:-2
现在如果我轻触三次
我会打开一个测距仪


616
00:32:06,193 --> 00:32:11,398 line:-2
可以让我修改
两个图像的深度汇聚平面


617
00:32:11,965 --> 00:32:16,103 line:-2
这个app知道如何注册
两个图像的相关性


618
00:32:16,336 --> 00:32:19,973 line:-1
因此它允许我修改深度汇聚平面


619
00:32:20,040 --> 00:32:21,308 line:-1
就像你的双眼一样


620
00:32:21,675 --> 00:32:24,244 line:-2
当你盯着一些非常近
或非常远的东西看时


621
00:32:24,311 --> 00:32:26,980 line:-1
你正在修改汇聚的深度平面


622
00:32:27,281 --> 00:32:29,616 line:-1
那么比如说近距离地看我的手


623
00:32:30,450 --> 00:32:34,321 line:-1
我可以发现哪里是适合的深度汇聚


624
00:32:34,388 --> 00:32:36,523 line:-1
好了 现在适合一只手了


625
00:32:37,024 --> 00:32:39,226 line:-1
但却不适合我后边的那辆汽车


626
00:32:39,293 --> 00:32:42,696 line:-1
因此我继续向远处移动


627
00:32:43,797 --> 00:32:46,466 line:-1
好了 但这不适合它后面的汽车


628
00:32:47,234 --> 00:32:49,970 line:-1
因此现在我可以把那家伙拉回来


629
00:32:51,405 --> 00:32:54,708 line:-2
那么这就是来自双摄像头的
同步双摄像头视频流


630
00:33:03,116 --> 00:33:05,953 line:-2
这个图表显示了
AVDualCam的图表


631
00:33:07,020 --> 00:33:10,190 line:-2
它没有使用两个独立的设备输入
而是一个


632
00:33:10,557 --> 00:33:12,893 line:-2
因此它使用的是
双摄像头的单一设备输入


633
00:33:13,126 --> 00:33:15,429 line:-1
但它以同步方式


634
00:33:15,829 --> 00:33:19,233 line:-2
把广角帧和远距帧转成了两个
VideoDataOutputs


635
00:33:20,033 --> 00:33:22,336 line:-1
你会注意到底部有一个小对象


636
00:33:22,402 --> 00:33:25,873 line:-1
叫做AVCaptureOutputSynchronizer


637
00:33:26,073 --> 00:33:27,174 line:-1
我不想混淆你们的视听


638
00:33:27,407 --> 00:33:30,410 line:-1
它所做的并不是我所谈到的硬件同步


639
00:33:30,611 --> 00:33:34,815 line:-2
它只是会话底部的一个对象
如果你想


640
00:33:35,249 --> 00:33:39,987 line:-2
它可以在一个单一回调中
为你提供多次回调


641
00:33:40,053 --> 00:33:42,923 line:-1
因此不要对广角和远距摄像头


642
00:33:42,990 --> 00:33:44,091 line:-2
使用单独的
VideoDataOutput回调


643
00:33:44,157 --> 00:33:46,760 line:-2
你可以在底部执行
DataOutputSynchronizer


644
00:33:46,827 --> 00:33:50,030 line:-2
并通过一个单一的回调
同时获得来自两个摄像头的图像


645
00:33:50,097 --> 00:33:51,532 line:-1
那样就非常方便了


646
00:33:52,132 --> 00:33:56,837 line:-2
它下边有一个Metal Shader
Filter Compositor 它也很神奇


647
00:33:57,304 --> 00:34:00,007 line:-2
正如我所说的那样 它知道如何
把两个来源的图像混合在一起


648
00:34:00,207 --> 00:34:02,376 line:-1
并决定预览中的哪里


649
00:34:02,442 --> 00:34:03,944 line:-1
是渲染这些图像的正确位置


650
00:34:04,244 --> 00:34:07,047 line:-2
并且它还把它们发送到
AVAssetWriter


651
00:34:07,114 --> 00:34:08,415 line:-1
以记录到一个视频轨道中


652
00:34:10,250 --> 00:34:12,286 line:-1
现在回顾一下之前的图表


653
00:34:14,021 --> 00:34:16,956 line:-2
我展示了
AVCaptureDeviceInput的全貌图


654
00:34:17,157 --> 00:34:18,859 line:-1
尤其是双摄像头的输入


655
00:34:19,560 --> 00:34:22,996 line:-1
双摄像头输入的端口属性揭露了


656
00:34:23,063 --> 00:34:24,364 line:-1
你能在那儿看到哪个端口


657
00:34:25,264 --> 00:34:27,134 line:-1
有人看到那儿有两个视频端口吗？


658
00:34:28,635 --> 00:34:29,937 line:-1
我没看到两个视频端口


659
00:34:30,204 --> 00:34:35,108 line:-2
因此如何从我们在这里看到的那些
输入端口中区分开广角和远距呢？


660
00:34:35,275 --> 00:34:37,744 line:-2
那一个视频端口
是设法为我们提供两个吗？


661
00:34:38,110 --> 00:34:40,112 line:-1
不 它没有给我们提供广角或远距


662
00:34:40,179 --> 00:34:43,016 line:-1
它为我们提供的是双摄像头决定的


663
00:34:43,083 --> 00:34:45,886 line:-1
适合指定缩放系数的端口


664
00:34:47,221 --> 00:34:50,257 line:-1
这不会帮助我们同时获得两个视频流


665
00:34:50,324 --> 00:34:51,257 line:-1
因此我们要如何做呢？


666
00:34:52,793 --> 00:34:53,760 line:-1
嗯 让我来告诉你


667
00:34:54,161 --> 00:34:57,664 line:-2
但这是个秘密 所以你必须保证
不告诉其他人 好吗？


668
00:34:58,265 --> 00:35:02,202 line:-1
虚拟设备有秘密端口 OK？


669
00:35:03,303 --> 00:35:07,341 line:-1
你之前不知道的秘密端口


670
00:35:07,407 --> 00:35:10,611 line:-2
现在变得可用了 但你没有把它们
从端口的数组中区分出来


671
00:35:10,878 --> 00:35:14,248 line:-1
你通过了解要请求什么来获得它们


672
00:35:14,848 --> 00:35:19,086 line:-2
因此你不仅会得到
端口的每个可能的类型的数组


673
00:35:19,152 --> 00:35:22,956 line:-1
包括不能用于单一摄像头会话的端口


674
00:35:23,557 --> 00:35:25,158 line:-1
你都可以通过名称进行请求


675
00:35:25,559 --> 00:35:27,394 line:-2
因此在这里我们有
dualCameraInput


676
00:35:27,661 --> 00:35:32,599 line:-2
我正在通过sourceDeviceType
WideAngleCamera来请求它的端口


677
00:35:33,000 --> 00:35:35,435 line:-2
以及源设备类型
TelephotoCamera


678
00:35:35,969 --> 00:35:38,272 line:-2
它告诉我说“啊哈
我知道那些秘密端口


679
00:35:38,338 --> 00:35:39,173 line:-1
我现在就提供给你”


680
00:35:39,473 --> 00:35:41,341 line:-1
一旦你得到那些输入端口


681
00:35:41,575 --> 00:35:44,545 line:-1
你就可以把它们钩在一个连接上


682
00:35:44,778 --> 00:35:47,281 line:-1
与你创建自己的手动连接时一样


683
00:35:47,748 --> 00:35:51,451 line:-2
然后你就可以在广角或远距
或广角和远距视频流之间切换


684
00:35:52,853 --> 00:35:57,090 line:-2
在AVDualCam演示中
我可以用正确的视角


685
00:35:57,524 --> 00:36:00,894 line:-2
修改广角和远距摄像头的
深度汇聚平面


686
00:36:01,295 --> 00:36:04,031 line:-1
你也看到了 那不只是移动和晃动


687
00:36:04,097 --> 00:36:06,567 line:-1
它沿着我希望它汇聚的平面上移动


688
00:36:06,633 --> 00:36:09,002 line:-1
就沿着基线所在的平面


689
00:36:09,536 --> 00:36:12,139 line:-2
我之所以能这样做是因为
AVFoundation


690
00:36:12,639 --> 00:36:14,074 line:-1
为我们提供了一些单应性辅助


691
00:36:14,474 --> 00:36:16,276 line:-2
单应性是指
如果你不熟悉这个术语的话


692
00:36:16,343 --> 00:36:18,946 line:-1
它与同一平面上的两个图像相关


693
00:36:19,680 --> 00:36:21,348 line:-1
它们是计算机视觉的基础


694
00:36:21,548 --> 00:36:26,787 line:-2
它们对于以下任务来说很常见
比如图像校正、图像注册


695
00:36:28,222 --> 00:36:30,557 line:-2
现在相机内联对于iOS来说
不是什么新技术


696
00:36:30,624 --> 00:36:32,492 line:-1
我们在iOS 11中就引入了它们


697
00:36:33,193 --> 00:36:35,229 line:-1
它们给我呈现一个3乘3的矩阵


698
00:36:35,295 --> 00:36:38,498 line:-1
描述了摄像头的几何学性能


699
00:36:38,732 --> 00:36:42,069 line:-1
也就是它的焦距和光学中心


700
00:36:42,436 --> 00:36:45,339 line:-1
看这里使用了针孔摄像头


701
00:36:45,639 --> 00:36:48,809 line:-2
你可以看到
它进入到针孔摄像头的哪个位置


702
00:36:49,009 --> 00:36:51,812 line:-1
并击中传感器 那是光学传感器


703
00:36:51,879 --> 00:36:54,147 line:-1
而把两者之间的距离作为焦距


704
00:36:55,415 --> 00:36:58,418 line:-1
现在你可以选择按帧接收内联


705
00:36:58,685 --> 00:37:01,955 line:-2
通过给AVCaptureConnection
发消息说


706
00:37:02,022 --> 00:37:04,191 line:-1
你希望内部提交


707
00:37:04,525 --> 00:37:05,492 line:-1
一旦完成


708
00:37:05,859 --> 00:37:08,462 line:-2
那么你所收到的每个
视频数据输出缓冲器


709
00:37:08,695 --> 00:37:10,130 line:-1
都带有这个


710
00:37:10,564 --> 00:37:13,734 line:-2
CameraIntrinsicMatrix
它还是一个NSData


711
00:37:13,967 --> 00:37:18,105 line:-2
包裹的是一个3乘3的浮点型矩阵
是一个SIMD类型


712
00:37:18,672 --> 00:37:20,574 line:-1
当你获得广角视频流时


713
00:37:20,874 --> 00:37:22,910 line:-1
你会得到广角摄像头的视频流矩阵


714
00:37:23,110 --> 00:37:26,713 line:-2
当你获得远距视频流时
你会得到远距摄像头的视频流矩阵


715
00:37:28,682 --> 00:37:33,120 line:-2
现在iOS 13中有个新功能
我们提供设备层级的摄像头外联


716
00:37:33,520 --> 00:37:37,524 line:-1
外联是一个旋转矩阵和一个平移矢量


717
00:37:37,591 --> 00:37:42,229 line:-1
就像是被一起填到一个矩阵中


718
00:37:42,629 --> 00:37:46,500 line:-2
它描述了摄像头
相对于参照摄像头的姿势


719
00:37:47,167 --> 00:37:50,270 line:-2
这会为你提供帮助
如果你想关联两个摄像头


720
00:37:50,537 --> 00:37:53,207 line:-2
所在的位置的斜度
以及它们之间的距离


721
00:37:54,041 --> 00:37:56,710 line:-1
那么AVDualCam使用外联


722
00:37:56,977 --> 00:38:00,347 line:-2
来了解如何对齐
广角和远距摄像头的视频流


723
00:38:00,514 --> 00:38:03,817 line:-1
相互协调 从而可以进行正常拍摄


724
00:38:05,853 --> 00:38:09,923 line:-2
它们是内联和外联上
非常非常简短的刷新工具


725
00:38:10,190 --> 00:38:13,460 line:-1
那么在两年前的第507号研讨会上


726
00:38:13,527 --> 00:38:15,696 line:-1
我对它们做了非常详细的介绍


727
00:38:15,929 --> 00:38:17,531 line:-1
因此我请你回顾那场研讨会


728
00:38:17,598 --> 00:38:19,833 line:-1
如果你的胃很强大的话


729
00:38:21,869 --> 00:38:27,174 line:-2
好的 关于MultiCam捕捉的
最后一个话题是多路麦克捕捉


730
00:38:28,575 --> 00:38:31,512 line:-1
好的 让我们回顾一下使用传统


731
00:38:31,879 --> 00:38:35,582 line:-2
AVCaptureSession
时的麦克捕捉的默认行为


732
00:38:37,751 --> 00:38:39,887 line:-1
麦克跟随摄像头


733
00:38:40,187 --> 00:38:41,455 line:-1
这是我能想到的最简单的描述了


734
00:38:41,688 --> 00:38:46,627 line:-1
因此如果会话中连接着前置摄像头


735
00:38:47,227 --> 00:38:49,763 line:-1
那么麦克将自动选择


736
00:38:49,830 --> 00:38:51,365 line:-2
与前置摄像头所指向的方向
一样的麦克


737
00:38:51,765 --> 00:38:52,766 line:-1
对于后置摄像头也一样


738
00:38:53,100 --> 00:38:55,802 line:-1
因此它是一个漂亮的心形图


739
00:38:56,103 --> 00:38:58,105 line:-1
它拒绝来自外部的你不想要的音频


740
00:38:58,505 --> 00:39:01,441 line:-1
那样你就可以跟随你的主体 前或后


741
00:39:01,909 --> 00:39:03,310 line:-1
如果你有只有音频的会话


742
00:39:03,377 --> 00:39:05,646 line:-1
我们不太确定该把音频指向哪个方向


743
00:39:05,712 --> 00:39:07,748 line:-1
因此我们只给你提供一个全方位字段


744
00:39:08,549 --> 00:39:11,685 line:-2
作为一个强大的功能
你可以禁用全部功能


745
00:39:11,952 --> 00:39:15,589 line:-2
通过声明“别动AVCaptureSession
我想使用我自己的AVCaptureSession


746
00:39:15,756 --> 00:39:18,692 line:-2
并配置自己的音频”
并且我们会遵守承诺


747
00:39:20,961 --> 00:39:23,697 line:-1
那么是时候告诉你们另一个小秘密了


748
00:39:26,733 --> 00:39:28,502 line:-1
根本就没有什么前置麦克


749
00:39:28,869 --> 00:39:30,204 line:-1
我刚才完全是在骗你


750
00:39:31,104 --> 00:39:32,739 line:-1
事实上iPhone


751
00:39:33,273 --> 00:39:35,209 line:-1
包含许多麦克


752
00:39:35,275 --> 00:39:37,778 line:-1
设备不同麦克的数量也不同


753
00:39:38,078 --> 00:39:39,680 line:-2
最近几代iPhone
恰巧有四个麦克


754
00:39:39,746 --> 00:39:43,851 line:-2
iPad有五个麦克
并且它们的位置不一样


755
00:39:44,218 --> 00:39:47,487 line:-2
在最近几代iPhone上
恰巧有两个麦克直接指向屏幕底部


756
00:39:47,721 --> 00:39:50,023 line:-1
顶部有两个麦克分别指向两侧


757
00:39:50,457 --> 00:39:52,226 line:-1
它们都是全方位麦克


758
00:39:52,726 --> 00:39:55,762 line:-2
现在顶部麦克会产生一些
听觉上的分离


759
00:39:55,829 --> 00:39:58,966 line:-1
因为它们之间有设备充当挡板


760
00:39:59,333 --> 00:40:02,503 line:-2
但它仍然不能为你提供一个
你想要的方向图


761
00:40:03,303 --> 00:40:06,306 line:-1
那么你具体到如何获得


762
00:40:06,373 --> 00:40:08,308 line:-1
接近前或后麦克的效果呢？


763
00:40:09,343 --> 00:40:11,912 line:-1
你要做的叫做麦克波束形成


764
00:40:12,312 --> 00:40:15,682 line:-1
这是处理原生音频信号


765
00:40:15,749 --> 00:40:17,384 line:-1
并让它们具备方向性的一种方式


766
00:40:17,684 --> 00:40:19,920 line:-2
Core Audio
会以你的名义替你实现


767
00:40:20,487 --> 00:40:22,623 line:-1
在这里我们有两个蓝色点


768
00:40:22,689 --> 00:40:26,026 line:-1
代表iPhone上两侧的两个麦克


769
00:40:26,360 --> 00:40:27,995 line:-1
圆形大概就是


770
00:40:28,061 --> 00:40:29,963 line:-1
它们所听到的音频图


771
00:40:30,030 --> 00:40:32,299 line:-1
请记住 它们都是全方位麦克


772
00:40:32,833 --> 00:40:35,402 line:-1
如果我们去掉这两个信号


773
00:40:35,602 --> 00:40:38,305 line:-1
我们就会形成八字形图 那很酷


774
00:40:38,372 --> 00:40:40,007 line:-2
那并不是我们想要的
但它的确很酷


775
00:40:40,974 --> 00:40:42,476 line:-1
如果我们想进一步塑造它


776
00:40:43,010 --> 00:40:47,147 line:-2
我们可以给我们想要保留的麦克
添加一些增益 在去掉它们之前


777
00:40:47,381 --> 00:40:49,449 line:-1
现在我们得到了一个小吃豆人


778
00:40:49,883 --> 00:40:52,619 line:-2
很好 现在我们拒绝了
我们不希望保留的那一个麦克


779
00:40:53,053 --> 00:40:55,422 line:-1
但很遗憾 我们也把信号削弱了


780
00:40:55,489 --> 00:40:57,457 line:-1
因此它比我们想的要安静多了


781
00:40:58,625 --> 00:41:03,363 line:-2
但如果我们做了那些之后
又给那个信号应用了一些增益


782
00:41:03,530 --> 00:41:05,165 line:-1
我们会得到一个很漂亮的大吃豆人


783
00:41:05,332 --> 00:41:07,734 line:-2
并且现在我们得到了
我们想要的那个漂亮的心形图


784
00:41:07,968 --> 00:41:11,605 line:-1
我们拒绝了我们不想要的那个摄像头


785
00:41:12,306 --> 00:41:14,908 line:-1
现在这个极其简化了


786
00:41:15,175 --> 00:41:18,345 line:-2
其中进行着大量筛选
从而确保不会增益白噪声


787
00:41:18,879 --> 00:41:20,614 line:-1
但从根本上来说那就是所发生的事


788
00:41:20,948 --> 00:41:24,818 line:-2
到目前为止
一次只支持一个麦克波束形成


789
00:41:25,118 --> 00:41:27,287 line:-2
但Core Audio的
那些好伙计们


790
00:41:27,354 --> 00:41:30,624 line:-2
针对这个MultiCam功能
做了一些很伟大的事


791
00:41:31,024 --> 00:41:35,796 line:-2
在iOS 13中我们现在同时支持
多个波束形成


792
00:41:41,368 --> 00:41:43,770 line:-2
那么返回到老版
AVCaptureSession


793
00:41:44,271 --> 00:41:48,208 line:-2
当你获得麦克设备输入时
你发现它的音频端口


794
00:41:48,642 --> 00:41:51,745 line:-1
那个端口有许多种音频


795
00:41:51,812 --> 00:41:53,614 line:-1
可能是前、后或全方位音频


796
00:41:53,680 --> 00:41:55,782 line:-1
取决于会话所找到的摄像头


797
00:41:56,450 --> 00:41:58,886 line:-2
但当你使用
MultiCamSession时


798
00:41:59,119 --> 00:42:01,288 line:-1
行为非常严格


799
00:42:01,889 --> 00:42:05,659 line:-2
你找到的第一个音频端口
总是全方位的端口


800
00:42:06,059 --> 00:42:09,329 line:-1
然后你会发现我提到的那些秘密端口


801
00:42:09,730 --> 00:42:13,133 line:-2
得到了专门的后方波束
或专门的前方波束


802
00:42:13,800 --> 00:42:16,770 line:-1
实现方式是使用同样的


803
00:42:18,939 --> 00:42:21,241 line:-1
设备输入端口接收器


804
00:42:21,608 --> 00:42:24,344 line:-2
这一次通过
指定你所感兴趣的方位实现


805
00:42:24,745 --> 00:42:27,948 line:-1
因此你可以请求前方或后方位置


806
00:42:28,115 --> 00:42:30,083 line:-1
那会为你提供你所感兴趣的端口


807
00:42:30,150 --> 00:42:32,186 line:-1
并且你将获得很好的后或前波束形成


808
00:42:34,421 --> 00:42:35,355 line:-1
这是前波束形成


809
00:42:36,523 --> 00:42:37,491 line:-1
这是后波束形成


810
00:42:39,459 --> 00:42:42,729 line:-2
现在返回到Nik的
MultiCamPIP演示中


811
00:42:43,297 --> 00:42:44,898 line:-1
我们只关注了视频


812
00:42:44,965 --> 00:42:47,901 line:-1
我们展示了技术先进的部分功能


813
00:42:48,302 --> 00:42:51,071 line:-2
现在我要返回去并告诉你
我们在音频处理上做了哪些努力


814
00:42:51,905 --> 00:42:53,774 line:-1
我们总是运行


815
00:42:55,242 --> 00:42:58,512 line:-2
一台单一设备输入
带有两个波束形成


816
00:42:58,579 --> 00:42:59,980 line:-2
一个是前方波束形成
一个是后方波束形成


817
00:43:00,380 --> 00:43:03,183 line:-2
我们对它们执行
两个不同的音频数据输出


818
00:43:03,417 --> 00:43:05,419 line:-2
这张幻灯片应该说
AudioDataOutputs


819
00:43:05,819 --> 00:43:08,589 line:-1
然后在运行时从两者之间选择一个


820
00:43:08,655 --> 00:43:11,692 line:-1
那么根据两者谁更大


821
00:43:11,959 --> 00:43:13,927 line:-1
我们可以切换为前或后麦克


822
00:43:14,261 --> 00:43:16,396 line:-1
并为你提供我们所期待的波形形成


823
00:43:18,365 --> 00:43:21,468 line:-2
关于多路麦克捕捉有一些规则
需要了解


824
00:43:21,835 --> 00:43:23,837 line:-1
波形形成只能对内置麦克有效


825
00:43:23,904 --> 00:43:27,107 line:-2
如果你有外置麦克 USB麦克
我们不知道它是个什么东西


826
00:43:27,174 --> 00:43:28,609 line:-1
我们不知道如何用它形成波束


827
00:43:30,077 --> 00:43:33,380 line:-2
如果你恰巧插入其它东西
包括AirPods


828
00:43:33,780 --> 00:43:36,717 line:-1
我们当然会捕捉音频


829
00:43:36,984 --> 00:43:40,153 line:-2
但我们不知道如何形成波束
因此我们只给那个麦克


830
00:43:40,420 --> 00:43:42,589 line:-1
传输你所连接的全部输入


831
00:43:42,656 --> 00:43:44,625 line:-1
从而确保你不会丢失信号


832
00:43:47,828 --> 00:43:52,199 line:-2
今天对于多摄像头捕捉的演讲
就到此结束了


833
00:43:52,266 --> 00:43:53,467 line:-1
让我们快速总结一下


834
00:43:55,202 --> 00:44:01,475 line:-2
MultiCam捕捉会话是在
iOS上同时运行多摄像头的新方式


835
00:44:02,476 --> 00:44:04,678 line:-1
它是个强大的工具 但有一些限制


836
00:44:04,745 --> 00:44:05,612 line:-1
了解一下这些限制


837
00:44:06,680 --> 00:44:09,583 line:-1
当你编程时要考虑好


838
00:44:10,050 --> 00:44:11,485 line:-1
如何处理硬件和系统压力消耗


839
00:44:13,420 --> 00:44:15,022 line:-1
如果你想执行同步视频流


840
00:44:15,289 --> 00:44:18,158 line:-2
使用那些有必不可少的
设备端口的虚拟设备


841
00:44:18,959 --> 00:44:21,328 line:-1
最后如果你想执行多路麦克捕捉


842
00:44:21,395 --> 00:44:24,198 line:-2
请注意你可以使用前、
后或全方位波束形成


843
00:44:24,665 --> 00:44:25,532 line:-1
谢谢大家


1
00:00:00,506 --> 00:00:04,500
[音乐]


2
00:00:07,516 --> 00:00:14,196
[掌声]


3
00:00:14,696 --> 00:00:15,336
>> 早上好！


4
00:00:15,676 --> 00:00:16,976
我叫 Brittany Weinert


5
00:00:16,976 --> 00:00:18,866
我是来自 Vision 框架团队的软件工程师


6
00:00:19,566 --> 00:00:20,596
今年 我们团队


7
00:00:20,596 --> 00:00:22,086
带来了许多新技术和新产品


8
00:00:22,086 --> 00:00:23,316
相信你会感到兴奋并且爱上它们


9
00:00:23,916 --> 00:00:24,846
因为我们有太多


10
00:00:24,846 --> 00:00:26,076
新东西要介绍


11
00:00:26,076 --> 00:00:27,106
所以我们接下来


12
00:00:27,106 --> 00:00:27,516
将深入研究这些新特点


13
00:00:27,876 --> 00:00:29,256
如果你对我们完全不了解


14
00:00:29,256 --> 00:00:30,706
也不要担心


15
00:00:30,706 --> 00:00:31,466
我想你依然可以跟得上


16
00:00:31,466 --> 00:00:33,096
我们希望


17
00:00:33,096 --> 00:00:34,216
我们今天推出的这些新功能


18
00:00:34,216 --> 00:00:35,956
能够激励你对我们团队有更多了解


19
00:00:35,956 --> 00:00:39,606
并且能够将其运用到你的 App 当中


20
00:00:39,836 --> 00:00:40,906
今天我们将谈论到


21
00:00:40,906 --> 00:00:44,206
四个全新的主题 


22
00:00:44,416 --> 00:00:46,066
显着性 图像分类  


23
00:00:46,066 --> 00:00:47,556
图像相似性和面部质量


24
00:00:47,556 --> 00:00:49,456
我们也做了一些技术升级


25
00:00:49,456 --> 00:00:50,796
包括目标跟踪程序 面部标记


26
00:00:50,796 --> 00:00:52,826
新的探测器以及


27
00:00:52,826 --> 00:00:55,196
优化了的 Core ML 机器学习框架支持系统


28
00:00:55,196 --> 00:00:59,276
今天 我要和大家分享的是显著性


29
00:00:59,736 --> 00:01:00,836
让我们从定义开始


30
00:01:01,386 --> 00:01:02,806
我要给你们看一张照片


31
00:01:02,806 --> 00:01:06,426
需要你们注意


32
00:01:06,426 --> 00:01:08,036
第一眼就吸引你们的地方


33
00:01:08,526 --> 00:01:13,046
当你第一次看到这张照片


34
00:01:13,046 --> 00:01:14,126
三只海雀坐在悬崖上


35
00:01:14,126 --> 00:01:15,696
你有没有注意到


36
00:01:15,696 --> 00:01:16,366
自己首先看到的是什么


37
00:01:17,326 --> 00:01:19,956
根据我们的模型


38
00:01:19,956 --> 00:01:21,226
大部分人首先看到的


39
00:01:21,226 --> 00:01:21,476
是海雀的脸


40
00:01:22,436 --> 00:01:23,556
这就是显著性


41
00:01:24,006 --> 00:01:26,366
它包含两种类型


42
00:01:26,766 --> 00:01:28,156
基于注意力类


43
00:01:28,216 --> 00:01:28,566
以及基于客体性类型


44
00:01:29,086 --> 00:01:30,976
我们在海雀图像上


45
00:01:30,976 --> 00:01:32,566
看到的覆盖层叫做热图


46
00:01:32,566 --> 00:01:35,686
是基于注意力的显著性产生的


47
00:01:36,146 --> 00:01:37,216
但在我们探讨更多的可视化示例之前


48
00:01:37,216 --> 00:01:40,246
我想先介绍每个算法的基础知识


49
00:01:41,776 --> 00:01:45,186
基于注意力的显著性


50
00:01:45,186 --> 00:01:47,786
是人类预期的显著性


51
00:01:47,786 --> 00:01:49,436
我的意思是


52
00:01:49,436 --> 00:01:51,006
基于注意力的显著性模型


53
00:01:51,006 --> 00:01:53,566
是产生于人们在看到一系列图像时


54
00:01:53,906 --> 00:01:55,446
所看到的东西


55
00:01:56,326 --> 00:01:57,646
这意味着当人们看到图像时 


56
00:01:57,646 --> 00:02:00,026
热图会反射


57
00:02:00,026 --> 00:02:01,646
并突出显示他们首先看到的地方


58
00:02:02,676 --> 00:02:04,556
另一方面


59
00:02:04,556 --> 00:02:06,006
基于对象的显著性


60
00:02:06,056 --> 00:02:07,976
在图像中进行主题分割训练


61
00:02:08,515 --> 00:02:13,176
以突出前景对象或图像的主题


62
00:02:13,766 --> 00:02:15,996
因此在热图中


63
00:02:15,996 --> 00:02:17,656
主体或前景对象会突出显示


64
00:02:18,226 --> 00:02:19,916
现在我们来看一些例子


65
00:02:20,496 --> 00:02:23,946
这是早先的海雀


66
00:02:24,636 --> 00:02:26,956
这是覆盖在图像上的


67
00:02:26,956 --> 00:02:28,436
基于注意力的热图


68
00:02:29,446 --> 00:02:33,046
这是基于对象的热图


69
00:02:33,046 --> 00:02:34,806
正如我所说 


70
00:02:34,806 --> 00:02:36,056
人们往往先看海雀的脸


71
00:02:36,346 --> 00:02:37,686
所以海雀头部周围的区域


72
00:02:37,686 --> 00:02:39,956
在基于注意力的热图上非常明显


73
00:02:40,896 --> 00:02:42,286
对于对象


74
00:02:42,286 --> 00:02:43,676
我们只是试图找到主题


75
00:02:43,676 --> 00:02:45,246
在这种情况下


76
00:02:45,246 --> 00:02:45,636
这是三个海雀


77
00:02:45,786 --> 00:02:47,266
所以所有的海雀都会突出显示
 
00:02:48,486 --> 00:02:50,206
下面让我们来看看


78
00:02:50,206 --> 00:02:51,206
显著性在人物图像中的表现


79
00:02:51,746 --> 00:02:56,556
基于注意力的显著性


80
00:02:57,046 --> 00:02:58,656
人民面部周围的区域


81
00:02:58,656 --> 00:02:59,926
往往是最突出的


82
00:03:00,536 --> 00:03:02,146
不出所料


83
00:03:02,146 --> 00:03:04,146
因为我们倾向于先看人的脸


84
00:03:04,846 --> 00:03:06,616
对于基于对象的显著性


85
00:03:06,616 --> 00:03:08,196
如果这个人是形象的主体


86
00:03:08,196 --> 00:03:10,386
整个人应该突出显示


87
00:03:12,596 --> 00:03:14,896
因此我要说


88
00:03:14,896 --> 00:03:16,266
基于注意力的显著性


89
00:03:16,266 --> 00:03:17,846
在两种显著性中更为复杂


90
00:03:18,436 --> 00:03:19,926
因为它是由许多


91
00:03:19,926 --> 00:03:21,386
人为因素决定的


92
00:03:23,846 --> 00:03:25,126
决定注意力的显着性


93
00:03:25,126 --> 00:03:26,236
以及突出与否的主要因素


94
00:03:26,236 --> 00:03:28,426
是对比度 面部


95
00:03:28,916 --> 00:03:31,046
主体 视野和光线


96
00:03:32,416 --> 00:03:33,666
但有趣的是


97
00:03:33,666 --> 00:03:36,026
它也可能受到感知运动的影响


98
00:03:36,556 --> 00:03:39,816
在这个例子中


99
00:03:39,816 --> 00:03:42,436
伞的颜色真的很亮眼


100
00:03:42,436 --> 00:03:43,716
所以伞周围的区域是显著的


101
00:03:44,026 --> 00:03:45,636
不过路也很突出


102
00:03:46,006 --> 00:03:47,666
因为我们的眼睛


103
00:03:47,666 --> 00:03:48,796
试图追踪伞的前进方向


104
00:03:50,706 --> 00:03:52,216
对于基于对象的显著性


105
00:03:52,216 --> 00:03:55,096
我们只是找出了打伞的人


106
00:03:55,316 --> 00:03:56,876
我可以整天这样做


107
00:03:56,876 --> 00:03:58,456
给你们展示更多的例子


108
00:03:58,456 --> 00:03:59,456
但老实说


109
00:03:59,456 --> 00:04:00,966
想要理解显著性


110
00:04:00,966 --> 00:04:01,766
最好的方法还是自己尝试一下


111
00:04:02,476 --> 00:04:03,776
在这里我鼓励大家


112
00:04:03,776 --> 00:04:05,726
下载 Saliency App 


113
00:04:05,726 --> 00:04:07,136
并在你们自己的照片库中试试


114
00:04:07,796 --> 00:04:10,536
接下来让我们来研究一下


115
00:04:10,536 --> 00:04:12,236
从显著性请求中返回的内容


116
00:04:12,236 --> 00:04:15,726
主要是热图


117
00:04:16,055 --> 00:04:17,375
到目前为止我向你展示的图像里


118
00:04:17,375 --> 00:04:19,596
热图已经缩放


119
00:04:19,596 --> 00:04:22,356
重叠 着色


120
00:04:22,356 --> 00:04:25,996
并放到图像上


121
00:04:25,996 --> 00:04:27,536
但实际上 


122
00:04:27,536 --> 00:04:29,656
热图是一个非常小的


123
00:04:29,656 --> 00:04:32,206
CV 像素缓冲


124
00:04:32,206 --> 00:04:34,186
由 0 到 1 的浮点数组成


125
00:04:34,186 --> 00:04:37,246
0 表示不存在


126
00:04:37,526 --> 00:04:39,746
1 表示最显著


127
00:04:40,256 --> 00:04:44,066
还需要进行额外的处理


128
00:04:44,066 --> 00:04:45,206
才能做得到


129
00:04:45,206 --> 00:04:46,746
和你在这里看到的效果完全一样


130
00:04:47,526 --> 00:04:48,896
不过现在我们一起来看


131
00:04:48,946 --> 00:04:51,716
如何提出最基本的请求


132
00:04:53,256 --> 00:04:56,686
好了  现在首先我们开始


133
00:04:56,686 --> 00:04:58,776
使用一个 VNImageRequestHandler


134
00:04:58,776 --> 00:05:00,206
来处理一个单一的图像


135
00:05:01,266 --> 00:05:04,346
下一步 选择你想要运行的算法


136
00:05:04,346 --> 00:05:06,516
在这种情况下选择 AttentionBasedSaliency


137
00:05:06,566 --> 00:05:09,256
如果你一直想使用同样的算法


138
00:05:09,256 --> 00:05:10,976
则设置 revision


139
00:05:12,976 --> 00:05:14,896
接下来


140
00:05:15,196 --> 00:05:17,466
你可以像通常那样执行请求


141
00:05:17,466 --> 00:05:19,476
如果成功了


142
00:05:19,476 --> 00:05:20,996
那么就应用 VNSaliencyImageObservation


143
00:05:20,996 --> 00:05:24,266
填充请求中的结果属性


144
00:05:25,326 --> 00:05:28,396
要访问热图


145
00:05:28,396 --> 00:05:30,086
你需要像这样


146
00:05:30,086 --> 00:05:32,906
调用 VNSaliencyImageObservation上的


147
00:05:35,256 --> 00:05:35,356
pixelBuffer 属性


148
00:05:35,596 --> 00:05:37,516
如果你想做基于对象的显著性


149
00:05:37,516 --> 00:05:39,316
那你就 必须


150
00:05:39,316 --> 00:05:44,246
要将请求名称和 revision 更改为 Objectness


151
00:05:45,426 --> 00:05:46,676
因此 值得注意的是


152
00:05:46,736 --> 00:05:48,206
基于注意力的是


153
00:05:48,206 --> 00:05:50,106
VNGenerateAttentionBasedSaliencyImageRequest


154
00:05:50,106 --> 00:05:50,836
基于对象的事


155
00:05:50,836 --> 00:05:53,076
VNGenerateObjectnessBasedSaliencyImageRequest


156
00:05:53,796 --> 00:05:56,856
那么现在 除了热图之外


157
00:05:56,856 --> 00:05:58,226
我们来看另一个工具


158
00:05:58,226 --> 00:05:58,856
即边界框


159
00:05:59,476 --> 00:06:03,196
边界框能够标记出来


160
00:06:03,196 --> 00:06:04,796
图像的所有显著区域
 
00:06:05,156 --> 00:06:06,726
对于以注意力为基础的显著性来说


161
00:06:06,726 --> 00:06:08,006
你应该始终有一个边界框


162
00:06:08,006 --> 00:06:09,866
那就以对象为基础的显著性来说


163
00:06:09,866 --> 00:06:11,116
你可以增加到用三个边框


164
00:06:11,116 --> 00:06:12,616
加以标记


165
00:06:13,726 --> 00:06:15,826
边界框在


166
00:06:15,826 --> 00:06:18,546
相对于图像的标准化坐标空间中


167
00:06:18,546 --> 00:06:23,316
原始图像和左下角是原点


168
00:06:23,316 --> 00:06:25,066
非常类似于 Vision 中


169
00:06:25,066 --> 00:06:27,186
其他算法返回的边界框


170
00:06:27,956 --> 00:06:30,716
所以我写了一个小方法


171
00:06:30,716 --> 00:06:32,266
演示如何得到边界框


172
00:06:32,266 --> 00:06:33,146
并且使用它们


173
00:06:33,916 --> 00:06:35,346
这里我们有一个


174
00:06:35,346 --> 00:06:37,386
VNSaliencyImageObservation


175
00:06:37,826 --> 00:06:40,156
你只需进入


176
00:06:40,156 --> 00:06:41,706
访问其 salientObjects 属性


177
00:06:41,706 --> 00:06:43,876
之后就会得到


178
00:06:44,036 --> 00:06:46,456
一个边界框列表


179
00:06:46,456 --> 00:06:48,296
你可以像这样进行使用


180
00:06:48,296 --> 00:06:50,566
好了 现在你已经知道了


181
00:06:50,566 --> 00:06:53,136
如何去制定一个请求


182
00:06:53,136 --> 00:06:54,906
也明白了什么是显著性


183
00:06:55,686 --> 00:06:57,736
那接下来我们就一起来看一些用例


184
00:06:58,346 --> 00:07:02,606
首先为了有趣一点


185
00:07:02,816 --> 00:07:05,816
你可以使用显著性作为一个图形蒙版


186
00:07:06,076 --> 00:07:07,056
来编辑你的照片


187
00:07:07,056 --> 00:07:09,496
这里你拥有热图


188
00:07:10,216 --> 00:07:14,306
在左侧


189
00:07:14,306 --> 00:07:15,886
我已经降低了非显著区域的饱和度


190
00:07:15,886 --> 00:07:17,716
而在右侧


191
00:07:17,716 --> 00:07:19,416
我为所有非显著区域


192
00:07:19,416 --> 00:07:20,636
添加了高斯模糊


193
00:07:21,006 --> 00:07:22,576
它真的让主体显眼了
 
00:07:25,536 --> 00:07:27,246
另一个显著性的例子是


194
00:07:27,246 --> 00:07:29,156
你可以增强你照片的观看体验


195
00:07:30,106 --> 00:07:32,036
就比如你现在在家里


196
00:07:32,036 --> 00:07:33,846
坐在沙发 


197
00:07:33,946 --> 00:07:35,236
哪怕你的电视或者电脑


198
00:07:35,236 --> 00:07:38,216
已经进入待机模式


199
00:07:38,216 --> 00:07:39,906
它也在进入你的照片库


200
00:07:40,336 --> 00:07:41,706
很多时候


201
00:07:41,706 --> 00:07:44,446
这些照片显示的算法


202
00:07:44,446 --> 00:07:45,306
可能看似有点笨拙


203
00:07:45,306 --> 00:07:47,006
它们放大图像中看似随机的部分


204
00:07:47,006 --> 00:07:51,346
而不是 总是你所期望的


205
00:07:52,086 --> 00:07:53,546
但是有了显著性的帮助 


206
00:07:53,546 --> 00:07:56,026
你总是能够知道主题在哪里


207
00:07:56,026 --> 00:07:57,036
所以 你可以得到


208
00:07:57,136 --> 00:08:02,576
像这样更像纪录片的效果


209
00:08:02,766 --> 00:08:04,616
最后


210
00:08:04,616 --> 00:08:06,206
显著性对其他视觉算法非常有用


211
00:08:07,486 --> 00:08:09,246
假设我们有一个图像


212
00:08:09,246 --> 00:08:11,236
我们想要在图像中


213
00:08:11,236 --> 00:08:11,806
对对象进行分类


214
00:08:12,966 --> 00:08:14,426
我们通过可以运行基于对象的显著性


215
00:08:14,426 --> 00:08:16,226
来获取图像中的对象


216
00:08:16,226 --> 00:08:19,196
将图像裁剪到


217
00:08:19,196 --> 00:08:22,456
基于对象的显著性返回的边界框


218
00:08:22,456 --> 00:08:25,006
并通过运行算法


219
00:08:25,366 --> 00:08:26,816
以及运行图像分类算法


220
00:08:26,886 --> 00:08:31,536
找出对象是什么


221
00:08:32,316 --> 00:08:33,996
因此 由于边界框


222
00:08:34,285 --> 00:08:35,576
你不仅能够知道


223
00:08:35,576 --> 00:08:36,885
它们在图像中的位置


224
00:08:36,885 --> 00:08:39,905
而且还可以通过


225
00:08:39,905 --> 00:08:42,035
挑选其中包含这些对象的边界框


226
00:08:45,286 --> 00:08:46,756
现在 你已经可以使用 Core ML 对事物进行分类


227
00:08:46,756 --> 00:08:48,996
但今年


228
00:08:48,996 --> 00:08:50,606
Vision 已经为提供了新的图像分类技术


229
00:08:50,606 --> 00:08:54,126
由 Rohan 来介绍


230
00:08:55,516 --> 00:09:02,736
[掌声]


231
00:09:03,236 --> 00:09:04,696
>> 早上好


232
00:09:05,136 --> 00:09:06,806
我叫 Rohan Chandra


233
00:09:06,806 --> 00:09:08,476
是一名来自 Vision 团队的研究员


234
00:09:09,266 --> 00:09:10,846
今天 我将要跟大家谈论


235
00:09:10,846 --> 00:09:12,106
今年 Vision API 提供的


236
00:09:12,106 --> 00:09:13,476
一些新的


237
00:09:13,476 --> 00:09:15,606
图像分类请求


238
00:09:16,806 --> 00:09:18,856
现在作为一项任务 


239
00:09:18,856 --> 00:09:20,626
图像分类基本上是回答了这个问题


240
00:09:20,626 --> 00:09:23,916
我的图像中的对象是什么


241
00:09:25,066 --> 00:09:28,206
许多人已经对图像分类比较熟悉


242
00:09:28,626 --> 00:09:30,146
你可能已经根据自己的数据


243
00:09:30,146 --> 00:09:31,776
使用过 Create ML 或者 Core ML 


244
00:09:31,776 --> 00:09:33,306
来训练自己的分类网络


245
00:09:33,356 --> 00:09:35,366
正如我们去年


246
00:09:35,366 --> 00:09:37,046
在 Vision with Core ML 中展示的


247
00:09:38,156 --> 00:09:39,306
其他人


248
00:09:39,356 --> 00:09:40,456
可能对图像分类感兴趣


249
00:09:40,456 --> 00:09:41,636
但觉得自己


250
00:09:41,636 --> 00:09:45,116
缺乏开发自己网络的资源或专业知识


251
00:09:45,896 --> 00:09:48,716
在实践中 


252
00:09:48,716 --> 00:09:50,026
从零开始开发一个大规模的分类网络


253
00:09:50,026 --> 00:09:51,576
可能需要数百万张图像的标注


254
00:09:51,676 --> 00:09:55,246
数千小时的培训


255
00:09:55,246 --> 00:09:57,566
以及非常专业的开发领域知识


256
00:09:58,736 --> 00:10:00,606
我们这里 Apple 


257
00:10:00,606 --> 00:10:02,296
已经完成了这个过程


258
00:10:02,336 --> 00:10:03,646
所以我们希望与你们


259
00:10:03,646 --> 00:10:06,706
分享我们的大规模终端分类网络


260
00:10:07,176 --> 00:10:08,206
以便你可以利用这项技术


261
00:10:08,206 --> 00:10:09,936
而无需投入


262
00:10:09,936 --> 00:10:11,396
大量时间或资源


263
00:10:11,396 --> 00:10:13,306
来自己开发


264
00:10:14,296 --> 00:10:17,346
我们还努力在 API 中添加工具


265
00:10:17,346 --> 00:10:19,216
以帮助你


266
00:10:19,216 --> 00:10:20,646
用有意义的方式


267
00:10:20,646 --> 00:10:21,806
对 App 的结果进行推测和理解


268
00:10:22,986 --> 00:10:24,116
现在我们在这里


269
00:10:24,116 --> 00:10:25,796
讨论的网络


270
00:10:25,846 --> 00:10:27,326
实际上是我们自己


271
00:10:27,326 --> 00:10:29,306
用来为照片搜索体验提供支持的网络


272
00:10:30,096 --> 00:10:31,136
这是我们专门开发的网络


273
00:10:31,136 --> 00:10:32,646
为了在设备上高效运行


274
00:10:32,646 --> 00:10:36,236
而不需要任何服务端处理


275
00:10:36,956 --> 00:10:38,446
我们还开发了它


276
00:10:38,446 --> 00:10:39,746
来识别超过一千种


277
00:10:39,746 --> 00:10:41,136
不同类别的对象


278
00:10:42,516 --> 00:10:44,396
现在值得注意的是


279
00:10:44,516 --> 00:10:45,776
这是一个多标签网络


280
00:10:45,776 --> 00:10:49,936
能够识别单个图像中的多个对象


281
00:10:49,936 --> 00:10:51,986
而不是更典型的单标签网络


282
00:10:51,986 --> 00:10:53,976
试图专注于识别图像中的


283
00:10:53,976 --> 00:10:58,226
单个大型中心对象


284
00:10:59,586 --> 00:11:01,076
现在


285
00:11:01,076 --> 00:11:03,226
当我们谈论这个新的分类 API 时


286
00:11:03,226 --> 00:11:04,186
我认为首先想到的问题之一


287
00:11:04,186 --> 00:11:07,426
是它可以实际识别的对象是什么


288
00:11:08,336 --> 00:11:09,916
那么 


289
00:11:09,916 --> 00:11:12,426
分类器可以预测的对象集称为分类系统


290
00:11:13,436 --> 00:11:15,106
分类系统具有层次结构


291
00:11:15,106 --> 00:11:17,936
在类之间具有方向关系


292
00:11:19,006 --> 00:11:22,226
这些关系基于共享的语义含义


293
00:11:22,916 --> 00:11:26,436
例如狗类可能会有像比格犬
 
00:11:26,616 --> 00:11:29,156
卷毛狗 哈士奇狗和其他子类


294
00:11:30,256 --> 00:11:31,866
从这个意义上来说


295
00:11:31,896 --> 00:11:33,226
父类倾向于更一般化


296
00:11:33,226 --> 00:11:36,106
而子类则是父类更具体的实例


297
00:11:37,046 --> 00:11:38,466
当然你也可以使用


298
00:11:38,466 --> 00:11:41,756
ImageRequest.known 分类查看整个分类


299
00:11:43,136 --> 00:11:44,416
现在


300
00:11:44,416 --> 00:11:46,276
在我们构建分类法时


301
00:11:46,276 --> 00:11:47,226
我们应用了一些特定的规则


302
00:11:48,656 --> 00:11:49,816
首先这些分类


303
00:11:49,856 --> 00:11:51,516
必须在视觉上可识别


304
00:11:52,836 --> 00:11:54,456
也就是说 我们要避免更多抽象概念


305
00:11:54,456 --> 00:11:56,306
如假日或节日


306
00:11:57,426 --> 00:11:59,196
我们也避免任何


307
00:11:59,196 --> 00:11:59,876
可能被认为


308
00:11:59,876 --> 00:12:01,746
具有争议性或冒犯性的类别


309
00:12:01,746 --> 00:12:03,066
以及与名字 名词


310
00:12:03,066 --> 00:12:06,256
形容词或基本形状有关的类别


311
00:12:07,346 --> 00:12:09,266
最后我们省略了职业


312
00:12:09,526 --> 00:12:11,076
这在一开始可能看起来很奇怪


313
00:12:11,716 --> 00:12:12,856
但是如果


314
00:12:12,856 --> 00:12:14,266
我们询问工程师的样子


315
00:12:14,266 --> 00:12:16,216
请考虑你会得到什么样的答案


316
00:12:16,556 --> 00:12:18,496
除了睡眠不足


317
00:12:18,496 --> 00:12:19,546
和通常粘在电脑屏幕上之外


318
00:12:19,546 --> 00:12:20,856
可能没有一个更简单的描述


319
00:12:20,856 --> 00:12:24,476
可以适用于每一位工程师


320
00:12:25,556 --> 00:12:26,646
让我们看一下你需要使用的代码


321
00:12:26,646 --> 00:12:28,706
以便对图像进行分类


322
00:12:31,636 --> 00:12:33,526
为原图像添加 ImageRequestHandler


323
00:12:34,196 --> 00:12:35,026
下一步


324
00:12:35,026 --> 00:12:36,836
执行 VNClassifyImageRequest


325
00:12:36,836 --> 00:12:38,066
并检索你的观察结果


326
00:12:38,746 --> 00:12:40,036
现在 在本例中


327
00:12:40,036 --> 00:12:41,406
你实际上得到了一组观察结果


328
00:12:41,696 --> 00:12:42,976
分类系统中的每个类


329
00:12:42,976 --> 00:12:45,456
都有一个观察结果及其相关的置信度


330
00:12:46,366 --> 00:12:47,876
在单标签问题中


331
00:12:47,876 --> 00:12:48,946
你可能会期望


332
00:12:48,946 --> 00:12:50,906
这些概率总和为 1


333
00:12:50,906 --> 00:12:51,896
但这是一个多标签分类网络


334
00:12:51,936 --> 00:12:53,876
每个预测都是


335
00:12:53,876 --> 00:12:55,376
与特定类相关的


336
00:12:55,376 --> 00:12:57,416
独立置信度


337
00:12:58,416 --> 00:13:00,096
因此它们不会总和为 1


338
00:13:00,406 --> 00:13:01,466
而是要在一个类中


339
00:13:01,466 --> 00:13:02,956
进行比较


340
00:13:02,956 --> 00:13:04,256
而不是跨越不同的类


341
00:13:04,496 --> 00:13:06,196
所以我们不能简单地


342
00:13:06,196 --> 00:13:08,816
用最大值来确定我们的最终预测


343
00:13:09,696 --> 00:13:11,166
你可能想知道


344
00:13:11,166 --> 00:13:12,806
我如何处理这么多类别


345
00:13:12,856 --> 00:13:13,756
这么多的数字


346
00:13:14,516 --> 00:13:15,836
我们在 API 中


347
00:13:15,836 --> 00:13:17,066
实现了一些关键工具


348
00:13:17,066 --> 00:13:19,086
来帮助你理解结果


349
00:13:20,376 --> 00:13:22,026
现在为了在 API 中
 
00:13:22,086 --> 00:13:23,916
讨论这些工具


350
00:13:23,916 --> 00:13:25,586
我们首先需要定义一些基本术语


351
00:13:26,266 --> 00:13:28,716
第一个是


352
00:13:28,716 --> 00:13:30,576
当你对一个类有信心时


353
00:13:30,576 --> 00:13:31,936
我们通常会将它


354
00:13:31,936 --> 00:13:33,606
与特定于类的阈值进行比较


355
00:13:33,606 --> 00:13:35,186
我们将其称为操作点


356
00:13:35,976 --> 00:13:37,976
如果类的置信度高于阈值


357
00:13:37,976 --> 00:13:39,466
那么我们就说


358
00:13:39,466 --> 00:13:40,796
类存在于图像中


359
00:13:41,266 --> 00:13:42,996
如果类置信度低于类阈值


360
00:13:42,996 --> 00:13:44,516
那么我们就说


361
00:13:44,516 --> 00:13:46,756
对象不在图像中


362
00:13:47,656 --> 00:13:49,316
从这个意义上说 


363
00:13:49,316 --> 00:13:50,866
我们想要选取阈值


364
00:13:50,866 --> 00:13:52,346
使得具有目标类的对象


365
00:13:52,346 --> 00:13:53,456
通常具有高于阈值的置信度


366
00:13:53,456 --> 00:13:55,646
而没有目标类的图像


367
00:13:55,736 --> 00:13:59,406
通常具有低于阈值的分数


368
00:14:00,376 --> 00:14:02,216
然而


369
00:14:02,216 --> 00:14:03,866
机器学习并非一帆风顺


370
00:14:03,866 --> 00:14:05,636
而且在某些情况下


371
00:14:05,636 --> 00:14:07,136
网络不确定


372
00:14:07,136 --> 00:14:08,276
置信度也相应降低


373
00:14:09,056 --> 00:14:10,496
例如


374
00:14:10,536 --> 00:14:12,156
当对象所处光线奇怪


375
00:14:12,156 --> 00:14:14,006
或者出现角度奇怪时


376
00:14:14,006 --> 00:14:14,506
就可能发生这种情况


377
00:14:15,176 --> 00:14:16,426
那么 我们如何选择阈值呢


378
00:14:17,596 --> 00:14:18,306
根据我们对阈值的选择


379
00:14:18,436 --> 00:14:19,756
产生三种不同类型的搜索


380
00:14:19,756 --> 00:14:20,926
我们可以有三种不同的制度


381
00:14:20,926 --> 00:14:23,326
和方法


382
00:14:24,416 --> 00:14:25,426
为了使这


383
00:14:25,426 --> 00:14:26,816
更具体一点


384
00:14:26,876 --> 00:14:28,696
假设我有一个图像库


385
00:14:28,806 --> 00:14:30,346
我已经对其进行了分类


386
00:14:30,386 --> 00:14:31,246
并存储了结果


387
00:14:32,026 --> 00:14:33,296
假设在这个特定的案例中


388
00:14:33,296 --> 00:14:35,866
我正在寻找摩托车的图像


389
00:14:36,726 --> 00:14:37,626
现在我要选择阈值


390
00:14:37,626 --> 00:14:39,466
这样摩托车图像的置信度


391
00:14:39,466 --> 00:14:40,776
通常高于此阈值


392
00:14:40,776 --> 00:14:41,826
而没有摩托车图像的


393
00:14:41,876 --> 00:14:46,946
置信度通常低于此阈值


394
00:14:47,596 --> 00:14:49,186
那么如果我选择一个低阈值


395
00:14:49,186 --> 00:14:50,006
会发生什么


396
00:14:50,826 --> 00:14:52,286
正如你在我身后看到的


397
00:14:52,286 --> 00:14:54,246
当我应用这个低阈值时


398
00:14:54,246 --> 00:14:55,356
我确实得到了


399
00:14:55,356 --> 00:14:56,926
我的摩托车图像


400
00:14:56,926 --> 00:14:58,696
但我也得到了右下角的


401
00:14:58,696 --> 00:14:59,246
这些轻型摩托的图像


402
00:14:59,246 --> 00:15:01,106
如果我的用户


403
00:15:01,106 --> 00:15:02,106
是摩托车爱好者


404
00:15:02,106 --> 00:15:03,386
他们可能会对这个结果感到有点恼火


405
00:15:04,516 --> 00:15:06,186
当我们谈论


406
00:15:06,186 --> 00:15:07,806
试图最大化


407
00:15:07,806 --> 00:15:09,346
在整个库检索的


408
00:15:09,346 --> 00:15:11,156
目标类别的百分比的搜索时


409
00:15:11,246 --> 00:15:12,656
并不关心


410
00:15:12,656 --> 00:15:14,196
这些错过的预测


411
00:15:14,196 --> 00:15:15,596
我们就说摩托车存在时


412
00:15:15,596 --> 00:15:16,266
实际上不存在


413
00:15:16,656 --> 00:15:18,036
我们通常谈论的


414
00:15:18,036 --> 00:15:19,076
是高召回搜索


415
00:15:20,156 --> 00:15:22,236
现在我可以通过


416
00:15:22,236 --> 00:15:23,806
简单地返回尽可能多的图像


417
00:15:23,806 --> 00:15:25,086
来最大程度地召回


418
00:15:25,176 --> 00:15:26,526
但是我会得到


419
00:15:26,526 --> 00:15:27,516
大量的这些错误预测


420
00:15:27,516 --> 00:15:28,806
我是说我的目标类


421
00:15:28,806 --> 00:15:30,436
实际不存在但显示存在


422
00:15:30,436 --> 00:15:31,666
所以我们需要


423
00:15:31,666 --> 00:15:32,676
找到一个更平衡的阈值来召回


424
00:15:33,586 --> 00:15:34,956
让我们来看看


425
00:15:34,956 --> 00:15:36,596
如何更改代码


426
00:15:36,596 --> 00:15:38,256
以执行此高召回搜索


427
00:15:39,716 --> 00:15:41,566
这里我有和以前


428
00:15:41,566 --> 00:15:43,466
相同的代码片段


429
00:15:43,606 --> 00:15:45,516
但这次我执行了


430
00:15:45,516 --> 00:15:47,206
hasMinimumPrecision


431
00:15:47,206 --> 00:15:48,666
和特定召回值的筛选


432
00:15:49,496 --> 00:15:51,526
对于我的观察数组中的


433
00:15:51,526 --> 00:15:53,646
每个观察


434
00:15:53,646 --> 00:15:55,166
只有当与类相关的置信度


435
00:15:55,166 --> 00:15:56,336
达到我指定的召回级别时


436
00:15:56,656 --> 00:15:58,896
它才能够被保留下来


437
00:15:59,646 --> 00:16:01,526
现在确定这一点


438
00:16:01,526 --> 00:16:02,856
所需的实际操作点


439
00:16:02,856 --> 00:16:04,196
对于每个类都是不同的


440
00:16:04,196 --> 00:16:06,106
这是我们根据


441
00:16:06,106 --> 00:16:07,756
对分类系统中每个类的


442
00:16:07,756 --> 00:16:08,986
网络分类情况


443
00:16:08,986 --> 00:16:11,116
确定的


444
00:16:12,086 --> 00:16:13,736
不过过滤系统


445
00:16:13,736 --> 00:16:14,696
会自动为你处理


446
00:16:15,026 --> 00:16:16,756
你所要做的就只是


447
00:16:16,756 --> 00:16:18,696
指定你想操作的召回级别


448
00:16:19,736 --> 00:16:20,686
所以我们在这里讨论的


449
00:16:20,686 --> 00:16:22,426
是一种高召回搜索


450
00:16:22,426 --> 00:16:24,196
但是如果我有一个


451
00:16:24,196 --> 00:16:25,646
不能容忍这些错误预测的 App


452
00:16:25,646 --> 00:16:26,786
比如我说摩托车


453
00:16:26,786 --> 00:16:27,696
存在但实际上不存在


454
00:16:28,106 --> 00:16:29,746
也就是说我要绝对确定


455
00:16:29,746 --> 00:16:31,456
我检索的图像


456
00:16:31,626 --> 00:16:33,246
实际上确实包含一辆摩托车


457
00:16:34,026 --> 00:16:35,326
那么让我们回到


458
00:16:35,326 --> 00:16:36,956
我们的图像库


459
00:16:36,956 --> 00:16:38,236
看看如果应用更高的阈值


460
00:16:38,266 --> 00:16:39,186
会发生什么


461
00:16:39,956 --> 00:16:41,406
正如你在我身后看到的


462
00:16:41,406 --> 00:16:43,186
当我应用我的高阈值时


463
00:16:43,186 --> 00:16:44,776
实际上我只得到摩托车图像


464
00:16:45,106 --> 00:16:46,506
但总体上


465
00:16:46,506 --> 00:16:47,166
我得到的图像要少得多


466
00:16:48,536 --> 00:16:50,176
当我们谈论一种搜索


467
00:16:50,176 --> 00:16:51,816
试图最大限度地


468
00:16:51,876 --> 00:16:53,506
提高检索图像中


469
00:16:53,506 --> 00:16:55,586
目标类的百分比


470
00:16:55,586 --> 00:16:57,146
忽略一些


471
00:16:57,146 --> 00:16:58,486
更模糊的图像


472
00:16:58,536 --> 00:16:59,696
这些图像可能包含目标类


473
00:16:59,696 --> 00:17:01,216
那么我们说


474
00:17:01,216 --> 00:17:03,736
这通常谈论的是高准确度搜索


475
00:17:04,445 --> 00:17:06,296
又一次就像高召回一样


476
00:17:06,296 --> 00:17:07,316
我们需要找到一个


477
00:17:07,316 --> 00:17:08,656
更平衡的操作点


478
00:17:08,656 --> 00:17:10,435
在这个点上我可以接受


479
00:17:10,435 --> 00:17:11,486
目标类出现在我的结果中的可能性


480
00:17:11,486 --> 00:17:12,886
但我得到的图像


481
00:17:12,986 --> 00:17:13,756
并不会太少


482
00:17:14,816 --> 00:17:16,116
现在让我们来看一下


483
00:17:16,116 --> 00:17:17,816
如何通过修改代码


484
00:17:17,816 --> 00:17:19,586
来执行这个高精度的搜索


485
00:17:20,175 --> 00:17:22,546
这里有相同的代码片段


486
00:17:22,715 --> 00:17:24,215
但这次我的过滤筛选


487
00:17:24,215 --> 00:17:26,366
是用 hasMinimumRecall


488
00:17:26,366 --> 00:17:27,935
和我指定的准确度值完成的


489
00:17:28,806 --> 00:17:30,636
同样只有当与之相关的


490
00:17:30,636 --> 00:17:32,186
置信度达到


491
00:17:32,186 --> 00:17:33,786
我指定的准确度水平时


492
00:17:33,786 --> 00:17:35,406
我才保留观察结果


493
00:17:36,166 --> 00:17:37,546
对于每一个类


494
00:17:37,546 --> 00:17:38,806
实际需要的阈值


495
00:17:38,806 --> 00:17:40,286
都是不同的


496
00:17:40,286 --> 00:17:41,646
但是过滤系统会自动


497
00:17:41,646 --> 00:17:42,256
为我处理这个问题


498
00:17:42,626 --> 00:17:44,056
我需要做的就是


499
00:17:44,056 --> 00:17:45,616
告诉它我想要的操作精度


500
00:17:46,936 --> 00:17:48,576
所以我们在这里


501
00:17:48,576 --> 00:17:49,846
谈到了两个不同的极端


502
00:17:49,906 --> 00:17:51,236
一个是高召回


503
00:17:51,236 --> 00:17:53,516
另一个是高准确度


504
00:17:53,516 --> 00:17:55,216
但在实践中


505
00:17:55,416 --> 00:17:57,076
最好找到两者之间的平衡点


506
00:17:58,096 --> 00:17:59,496
那么我们现在就来看看如何做到这一点


507
00:17:59,496 --> 00:18:01,966
了解发生了什么


508
00:18:02,246 --> 00:18:03,266
我首先需要介绍一下


509
00:18:03,396 --> 00:18:04,836
什么是准确度


510
00:18:04,836 --> 00:18:05,646
和召回曲线


511
00:18:06,346 --> 00:18:08,976
在实践中


512
00:18:08,976 --> 00:18:10,406
需要做出权衡


513
00:18:10,406 --> 00:18:11,986
增加一单位的准确度或召回率


514
00:18:11,986 --> 00:18:14,196
会导致另一方面的减少


515
00:18:14,646 --> 00:18:16,376
我可以将这种权衡


516
00:18:16,376 --> 00:18:17,956
用图形表示


517
00:18:18,026 --> 00:18:19,586
对于每个操作点


518
00:18:19,586 --> 00:18:21,336
我可以计算相应的准确度和召回率


519
00:18:22,016 --> 00:18:23,476
例如


520
00:18:23,506 --> 00:18:25,156
当我实现召回 0.7 的操作点


521
00:18:25,156 --> 00:18:27,116
我发现我得到了


522
00:18:27,116 --> 00:18:29,436
相应的 0.74 准确度


523
00:18:29,436 --> 00:18:31,886
我可以通过


524
00:18:31,886 --> 00:18:33,486
计算多个操作点


525
00:18:33,486 --> 00:18:35,016
来形成我的完整曲线


526
00:18:36,246 --> 00:18:37,956
正如我之前所说


527
00:18:37,956 --> 00:18:39,846
我希望在这条曲线上


528
00:18:40,056 --> 00:18:41,056
找到一个平衡点


529
00:18:41,056 --> 00:18:42,286
它可以达到对我的 App


530
00:18:42,286 --> 00:18:43,536
有意义的召回和准确度


531
00:18:44,486 --> 00:18:45,726
下面让我们看看


532
00:18:45,756 --> 00:18:47,366
如何更改代码来实现它


533
00:18:47,366 --> 00:18:48,636
以及准确度和召回曲线


534
00:18:48,636 --> 00:18:52,146
如何发挥作用


535
00:18:52,286 --> 00:18:54,276
我在这里使用


536
00:18:54,276 --> 00:18:55,896
hasMinimumPrecision 进行过滤


537
00:18:55,896 --> 00:18:57,546
在这里我指定了最小准确度


538
00:18:57,546 --> 00:18:57,976
和召回值


539
00:18:58,416 --> 00:19:00,746
当我指定


540
00:19:00,746 --> 00:19:02,486
MinimumPrecision 时


541
00:19:02,486 --> 00:19:04,286
我实际上是在图表中


542
00:19:04,286 --> 00:19:05,976
选择我想要操作的区域


543
00:19:06,866 --> 00:19:08,236
当我用 forRecall


544
00:19:08,286 --> 00:19:10,226
选择一个召回点时


545
00:19:10,296 --> 00:19:11,896
我会沿着曲线选择一个点


546
00:19:11,896 --> 00:19:13,006
作为我的操作点


547
00:19:13,876 --> 00:19:15,516
现在如果操作点


548
00:19:15,576 --> 00:19:16,666
在我选择的有效区域中


549
00:19:16,666 --> 00:19:18,166
那么这就是过滤系统


550
00:19:18,166 --> 00:19:19,616
在查看特定类时


551
00:19:19,616 --> 00:19:21,626
将应用的阈值


552
00:19:22,586 --> 00:19:24,076
如果操作点


553
00:19:24,076 --> 00:19:25,636
不在有效区域中


554
00:19:25,636 --> 00:19:27,046
则没有满足


555
00:19:27,046 --> 00:19:28,536
我所述约束的操作点


556
00:19:28,536 --> 00:19:29,606
并且该类将始终


557
00:19:29,606 --> 00:19:30,816
从我的结果中过滤掉


558
00:19:31,856 --> 00:19:33,556
从这个意义上讲


559
00:19:33,556 --> 00:19:35,166
你需要做的就是


560
00:19:35,166 --> 00:19:36,476
提供你想要操作的


561
00:19:36,476 --> 00:19:37,686
准确度和召回率


562
00:19:37,686 --> 00:19:38,626
过滤系统将


563
00:19:38,626 --> 00:19:40,796
自动为你确定必要的阈值


564
00:19:41,386 --> 00:19:44,906
总而言之


565
00:19:44,906 --> 00:19:46,226
我在执行图像分类时


566
00:19:46,226 --> 00:19:47,656
得到的观察


567
00:19:47,986 --> 00:19:49,106
实际上是一个观察数组


568
00:19:49,106 --> 00:19:50,656
分类中的每个类


569
00:19:50,656 --> 00:19:51,736
都有一个值


570
00:19:52,876 --> 00:19:54,106
因为这是一个多标签的问题


571
00:19:54,206 --> 00:19:56,456
所以置信度之和不等于1


572
00:19:57,166 --> 00:19:58,716
相反，我们有独立的


573
00:19:58,716 --> 00:20:00,376
置信值


574
00:20:00,376 --> 00:20:02,616
在0到1之间每个类都有一个


575
00:20:02,616 --> 00:20:04,096
我们需要了解准确度和召回


576
00:20:04,096 --> 00:20:05,656
以及它们如何


577
00:20:05,656 --> 00:20:07,676
应用于我们的特定用例


578
00:20:07,676 --> 00:20:08,606
以便应用


579
00:20:08,606 --> 00:20:10,226
hasMinimumPrecision 


580
00:20:10,226 --> 00:20:11,856
或 hasMinimumRecall 进行过滤筛选


581
00:20:11,856 --> 00:20:13,166
这对我们的 App 有意义


582
00:20:13,606 --> 00:20:15,846
所以这就总结了


583
00:20:16,146 --> 00:20:17,496
图像分类部分


584
00:20:17,986 --> 00:20:19,416
我想换个时间


585
00:20:19,416 --> 00:20:20,676
再探讨相关的话题


586
00:20:21,226 --> 00:20:23,976
图像相似性


587
00:20:26,216 --> 00:20:27,216
当我们谈论


588
00:20:27,216 --> 00:20:29,076
图像相似性时


589
00:20:29,076 --> 00:20:30,406
我们真正的意思是


590
00:20:30,406 --> 00:20:32,476
一种描述图像内容的方法


591
00:20:32,476 --> 00:20:34,956
和比较这些描述的另一种方法


592
00:20:36,116 --> 00:20:38,106
描述图像内容


593
00:20:38,106 --> 00:20:39,676
最基本方法是


594
00:20:39,676 --> 00:20:41,986
使用源像素本身


595
00:20:43,536 --> 00:20:45,286
也就是说 我可以搜索


596
00:20:45,286 --> 00:20:47,076
其他具有接近


597
00:20:47,076 --> 00:20:49,196
或完全相同像素值的图像


598
00:20:49,256 --> 00:20:50,076
并检索它们


599
00:20:51,146 --> 00:20:52,136
然而如果我以这种方式进行搜索


600
00:20:52,136 --> 00:20:54,046
它会非常脆弱


601
00:20:54,046 --> 00:20:55,886
很容易被旋转


602
00:20:55,986 --> 00:20:57,646
或光照增强等


603
00:20:57,916 --> 00:20:59,466
微小变化所影响


604
00:20:59,466 --> 00:21:00,656
这些变化会大幅改变像素值


605
00:21:00,656 --> 00:21:02,256
但不会改变图像中的


606
00:21:02,256 --> 00:21:03,376
语义内容


607
00:21:04,266 --> 00:21:05,536
我真正想要的是


608
00:21:05,536 --> 00:21:07,246
对图像内容的


609
00:21:07,246 --> 00:21:08,836
更高级描述


610
00:21:08,836 --> 00:21:10,426
可能有点儿像自然语言


611
00:21:10,936 --> 00:21:13,126
我可以使用我之前描述的


612
00:21:13,126 --> 00:21:14,696
图像分类 API


613
00:21:14,696 --> 00:21:16,426
以便提取


614
00:21:16,426 --> 00:21:18,626
描述我的图像的一组词


615
00:21:19,446 --> 00:21:20,936
然后我可以检索


616
00:21:20,936 --> 00:21:23,316
具有类似分类的其他图像


617
00:21:23,756 --> 00:21:25,106
我甚至可以


618
00:21:25,106 --> 00:21:26,256
将它与词向量结合起来


619
00:21:26,326 --> 00:21:27,716
来解释


620
00:21:27,716 --> 00:21:29,386
类似但不完全匹配的单词


621
00:21:29,386 --> 00:21:30,356
如 cat 和 kitten


622
00:21:30,696 --> 00:21:31,896
如果我执行这样的搜索


623
00:21:31,896 --> 00:21:33,556
我可能会在一般意义上


624
00:21:33,556 --> 00:21:35,046
得到类似的对象


625
00:21:35,446 --> 00:21:36,406
但这些对象的


626
00:21:36,406 --> 00:21:37,556
出现方式以及


627
00:21:37,556 --> 00:21:39,066
它们之间的关系


628
00:21:39,066 --> 00:21:40,026
可能会有很大差异


629
00:21:40,966 --> 00:21:43,036
同样我也会受到


630
00:21:43,036 --> 00:21:44,586
分类方法的限制


631
00:21:45,386 --> 00:21:46,736
也就是说


632
00:21:46,736 --> 00:21:48,426
在我的图像中


633
00:21:48,426 --> 00:21:49,756
出现的任何不在


634
00:21:49,756 --> 00:21:51,686
分类网络分类系统中的对象


635
00:21:51,786 --> 00:21:52,876
都不能在这样的搜索中表示出来


636
00:21:54,216 --> 00:21:55,486
我真正想要的是


637
00:21:55,486 --> 00:21:56,966
对图像中出现的对象的


638
00:21:56,996 --> 00:21:58,176
高级描述


639
00:21:58,386 --> 00:21:59,856
这些对象没有


640
00:21:59,856 --> 00:22:01,096
固定在精确的像素值上


641
00:22:01,096 --> 00:22:01,556
但仍然受到注意


642
00:22:02,256 --> 00:22:04,006
我也希望这适用于


643
00:22:04,006 --> 00:22:05,896
任何自然图像


644
00:22:05,896 --> 00:22:07,266
而不仅仅是在特定的分类中


645
00:22:07,746 --> 00:22:09,946
事实证明


646
00:22:09,946 --> 00:22:11,756
这种表征学习


647
00:22:11,756 --> 00:22:12,676
是我们分类网络


648
00:22:12,676 --> 00:22:14,016
作为其训练过程的一部分


649
00:22:14,016 --> 00:22:15,786
自然产生的


650
00:22:16,786 --> 00:22:18,736
网络的上层


651
00:22:18,976 --> 00:22:20,356
包含执行分类所需的所有显著信息


652
00:22:20,356 --> 00:22:22,166
同时丢弃


653
00:22:22,166 --> 00:22:23,686
对该任务没有帮助的


654
00:22:23,756 --> 00:22:25,466
任何冗余


655
00:22:25,466 --> 00:22:28,336
或不必要信息


656
00:22:28,386 --> 00:22:29,596
我们可以利用


657
00:22:29,596 --> 00:22:31,136
这些上层作为特征描述符


658
00:22:31,136 --> 00:22:32,456
就是我们所说的


659
00:22:32,456 --> 00:22:34,156
FeaturePrint


660
00:22:35,196 --> 00:22:36,706
FeaturePrint 是一个向量


661
00:22:36,706 --> 00:22:37,946
这些内容


662
00:22:37,946 --> 00:22:39,666
不受特定分类系统的约束


663
00:22:39,666 --> 00:22:40,696
甚至不受分类网络


664
00:22:40,696 --> 00:22:43,636
所训练的分类系统限制


665
00:22:43,896 --> 00:22:45,106
它只是


666
00:22:45,106 --> 00:22:46,486
在训练过程中


667
00:22:46,486 --> 00:22:47,736
利用网络对图像的了解


668
00:22:48,816 --> 00:22:50,296
如果我们查看这对图像


669
00:22:50,296 --> 00:22:51,696
我们可以比较


670
00:22:51,696 --> 00:22:52,786
它们 FeaturePrint 的相似程度


671
00:22:52,786 --> 00:22:54,236
并且值越小


672
00:22:54,236 --> 00:22:55,486
两个图像


673
00:22:55,486 --> 00:22:57,296
在语义上越相似


674
00:22:58,046 --> 00:22:59,316
我们可以看到


675
00:22:59,316 --> 00:23:00,556
尽管这两张猫的图像


676
00:23:00,556 --> 00:23:02,026
在视觉上是不同的


677
00:23:02,026 --> 00:23:03,506
但它们具有


678
00:23:03,746 --> 00:23:05,526
比视觉上相似的不同动物


679
00:23:05,596 --> 00:23:06,636
更相似的 FeaturePrint


680
00:23:07,136 --> 00:23:09,416
为了使这个更具体一点


681
00:23:09,416 --> 00:23:10,646
下面我们来看


682
00:23:10,646 --> 00:23:11,686
一个具体的例子


683
00:23:12,396 --> 00:23:13,476
假设我在屏幕上有


684
00:23:13,476 --> 00:23:15,186
源图像


685
00:23:15,186 --> 00:23:16,706
我想找到其他


686
00:23:16,706 --> 00:23:17,476
语义相似的的图像


687
00:23:18,336 --> 00:23:19,656
我将选取一个图像库


688
00:23:19,656 --> 00:23:21,196
计算每个图像的


689
00:23:21,196 --> 00:23:22,896
FeaturePrint


690
00:23:22,896 --> 00:23:24,476
然后检索那些


691
00:23:24,476 --> 00:23:26,296
与源图像具有


692
00:23:26,296 --> 00:23:26,986
最相似 FeaturePrint 的图像


693
00:23:27,746 --> 00:23:29,136
当我用咖啡厅里


694
00:23:29,136 --> 00:23:29,926
一位绅士的图像做这件事时


695
00:23:29,926 --> 00:23:31,876
我发现我也同时得到了


696
00:23:31,926 --> 00:23:33,176
咖啡厅和餐厅里


697
00:23:33,176 --> 00:23:33,946
其他人的图像


698
00:23:34,886 --> 00:23:36,266
如果我把注意力


699
00:23:36,266 --> 00:23:38,246
集中在一份报纸上


700
00:23:38,246 --> 00:23:39,586
我就会看到其他报纸的图片


701
00:23:40,246 --> 00:23:42,156
如果我把焦点放在茶壶上


702
00:23:42,236 --> 00:23:43,906
我会看到其他的茶壶图像


703
00:23:45,326 --> 00:23:46,616
现在我想邀请 


704
00:23:46,616 --> 00:23:48,616
Vision 团队在舞台上


705
00:23:48,616 --> 00:23:50,116
帮我做个演示


706
00:23:50,116 --> 00:23:51,546
演示下如何进一步扩展


707
00:23:51,546 --> 00:23:52,646
图像相似性


708
00:23:54,516 --> 00:23:58,166
[掌声]


709
00:23:58,666 --> 00:23:59,466
>> 大家好


710
00:23:59,886 --> 00:24:00,846
我叫 Brett 


711
00:24:00,846 --> 00:24:02,046
今天我们将用一种非常有趣的方式


712
00:24:02,046 --> 00:24:03,546
来展示图像相似性


713
00:24:03,546 --> 00:24:05,286
我们创造性地称之为


714
00:24:05,286 --> 00:24:06,846
图像相似性游戏


715
00:24:07,876 --> 00:24:09,086
这里是玩法说明


716
00:24:09,676 --> 00:24:11,066
你在一张纸上画些东西


717
00:24:11,066 --> 00:24:14,026
然后请几个朋友


718
00:24:14,026 --> 00:24:15,166
尽可能的重新创作出


719
00:24:15,166 --> 00:24:15,726
你的原作


720
00:24:16,296 --> 00:24:17,976
所以我将从画原图开始


721
00:24:30,096 --> 00:24:33,496
好 接下来点击 Continue


722
00:24:33,496 --> 00:24:33,976
将其作为原始文件进行扫描


723
00:24:42,046 --> 00:24:42,976
然后保存


724
00:24:44,526 --> 00:24:46,456
现在我的团队


725
00:24:46,456 --> 00:24:47,886
将扮演参赛者的角色 


726
00:24:48,156 --> 00:24:48,976
他们将尽可能地把这个画得最好


727
00:24:54,266 --> 00:24:55,206
当他们正在绘图时


728
00:24:55,206 --> 00:24:57,316
我要告诉你


729
00:24:57,316 --> 00:24:58,476
这个示例 App 


730
00:24:58,536 --> 00:25:00,576
现在作为示例代码


731
00:25:00,576 --> 00:25:04,356
可在开发者文档网站上提供给你


732
00:25:04,356 --> 00:25:05,986
而且我们正在使用


733
00:25:05,986 --> 00:25:07,586
VisionKit 文档扫描仪


734
00:25:07,586 --> 00:25:08,886
扫描我们的绘图


735
00:25:08,886 --> 00:25:09,966
你可以在我们的


736
00:25:09,966 --> 00:25:11,226
文字识别会议中了解更多


737
00:25:12,706 --> 00:25:15,076
再给他们


738
00:25:16,636 --> 00:25:16,886
几秒钟的时间


739
00:25:16,966 --> 00:25:19,896
五 四 三 好


740
00:25:19,926 --> 00:25:20,186
我想他们都已经完成了


741
00:25:20,266 --> 00:25:22,186
好的现在我们把它们拿出来


742
00:25:22,186 --> 00:25:24,266
开始扫描


743
00:25:24,996 --> 00:25:26,256
第一位参赛者作品


744
00:25:31,046 --> 00:25:31,576
非常不错 [掌声]


745
00:25:32,756 --> 00:25:33,406
这有可能会是冠军


746
00:25:33,406 --> 00:25:35,256
再来看看第二位选手的


747
00:25:38,296 --> 00:25:39,786
也很不错


748
00:25:40,096 --> 00:25:40,506
很棒


749
00:25:41,256 --> 00:25:43,256
[掌声]


750
00:25:43,496 --> 00:25:44,706
接下来三号选手


751
00:25:48,056 --> 00:25:50,056
[笑声和掌声]


752
00:25:50,096 --> 00:25:50,976
我觉得也很好


753
00:25:51,016 --> 00:25:52,586
[掌声]


754
00:25:52,586 --> 00:25:53,696
第四位


755
00:25:57,046 --> 00:25:58,136
这个我就不知道了


756
00:25:58,196 --> 00:26:00,836
我们要看会出现什么情况


757
00:26:01,151 --> 00:26:03,151
[掌声]


758
00:26:03,286 --> 00:26:03,836
好了


759
00:26:03,836 --> 00:26:07,006
我们把这些保存下来


760
00:26:07,006 --> 00:26:08,286
其实可以发现


761
00:26:08,286 --> 00:26:09,636
获胜者是一号参赛者


762
00:26:09,636 --> 00:26:10,446
恭喜


763
00:26:11,016 --> 00:26:12,736
[掌声]


764
00:26:12,736 --> 00:26:14,716
现在我划过去


765
00:26:14,796 --> 00:26:16,076
我们可以看到


766
00:26:16,076 --> 00:26:17,656
这些面孔在语义上更相似


767
00:26:18,336 --> 00:26:19,996
它们更接近原始绘图


768
00:26:20,096 --> 00:26:21,376
而树在语义上是不同的


769
00:26:21,376 --> 00:26:22,416
这与原来的图像


770
00:26:22,416 --> 00:26:22,686
有一些差别


771
00:26:23,336 --> 00:26:24,426
这就是图像相似度游戏


772
00:26:24,426 --> 00:26:25,376
下面让我们把舞台


773
00:26:25,376 --> 00:26:25,726
交给我的伙伴 Rohan


774
00:26:26,516 --> 00:26:31,776
[掌声]


775
00:26:32,276 --> 00:26:32,876
>> 谢谢大家


776
00:26:33,476 --> 00:26:34,736
我想快速浏览


777
00:26:34,736 --> 00:26:35,846
演示 App 中的一个片段


778
00:26:35,846 --> 00:26:37,196
向大家展示


779
00:26:37,196 --> 00:26:38,546
我们如何确定获胜的选手


780
00:26:39,686 --> 00:26:41,906
这里我有一部分代码


781
00:26:41,906 --> 00:26:43,506
它比较了每个参赛者绘画的


782
00:26:43,506 --> 00:26:45,286
FeaturePrint


783
00:26:45,396 --> 00:26:47,666
和 Brett 绘画的 FeaturePrint


784
00:26:48,326 --> 00:26:49,006
现在我通过 App 中定义的


785
00:26:49,006 --> 00:26:51,396
featureprintObservationForImage 函数


786
00:26:51,396 --> 00:26:54,466
提取了参赛者的 FeaturePrint


787
00:26:55,186 --> 00:26:56,956
一旦我有了每一个 FeaturePrint


788
00:26:57,226 --> 00:26:58,666
我就需要确定它


789
00:26:58,666 --> 00:27:00,056
与原始绘图的相似程度


790
00:27:00,056 --> 00:27:01,846
我可以使用 computeDistance 


791
00:27:01,846 --> 00:27:03,246
来做到这一点


792
00:27:03,246 --> 00:27:04,576
它会反馈给我一个浮点值


793
00:27:05,166 --> 00:27:05,826
浮点值越小


794
00:27:05,826 --> 00:27:08,646
说明两个图像越相似


795
00:27:09,236 --> 00:27:10,456
因此 一旦我为每个参赛者


796
00:27:10,456 --> 00:27:11,636
确定了这个


797
00:27:11,636 --> 00:27:13,116
我则只需要对它们进行排序


798
00:27:13,116 --> 00:27:14,276
就可以确定获胜者


799
00:27:15,336 --> 00:27:17,006
好了 这就是


800
00:27:17,156 --> 00:27:18,156
关于图像相似性部分的阐释


801
00:27:18,486 --> 00:27:19,536
现在有请我的搭档 Sergey 


802
00:27:19,536 --> 00:27:20,816
由他和大家共同探讨


803
00:27:20,816 --> 00:27:21,866
人脸识别技术的


804
00:27:21,866 --> 00:27:22,836
变化和发展


805
00:27:23,516 --> 00:27:28,500
[掌声]


806
00:27:33,056 --> 00:27:33,876
>> 大家上午好


807
00:27:34,196 --> 00:27:35,226
我是 Sergey Kamensky


808
00:27:35,226 --> 00:27:36,196
是来自 Vision 框架团队的


809
00:27:36,196 --> 00:27:36,926
一名软件工程师


810
00:27:37,356 --> 00:27:38,456
我很高兴


811
00:27:38,456 --> 00:27:39,786
今天能与大家分享


812
00:27:39,876 --> 00:27:40,926
今年框架中的更多新特性


813
00:27:41,096 --> 00:27:43,606
我们先来谈谈人脸识别技术


814
00:27:44,206 --> 00:27:45,956
记不记得 两年前


815
00:27:45,956 --> 00:27:47,836
当我们介绍 Vision 框架时


816
00:27:47,836 --> 00:27:49,806
我们还谈到了人脸检测识别


817
00:27:50,266 --> 00:27:51,596
今年


818
00:27:51,596 --> 00:27:52,796
我们将对该算法进行新的修改


819
00:27:53,176 --> 00:27:54,806
那么 变化是什么呢


820
00:27:55,916 --> 00:27:57,786
首先


821
00:27:57,926 --> 00:27:59,616
与之前的 65 点消除相比


822
00:27:59,616 --> 00:28:01,426
我们现在发展到了


823
00:28:01,426 --> 00:28:02,166
76 点消除


824
00:28:02,166 --> 00:28:04,116
76 点消除给了我们


825
00:28:04,116 --> 00:28:05,146
更大的密度


826
00:28:05,146 --> 00:28:06,686
来表示不同的面部区域


827
00:28:07,556 --> 00:28:09,556
其次我们现在报告


828
00:28:09,556 --> 00:28:11,306
每个标志点的置信度得分


829
00:28:11,306 --> 00:28:12,886
正如我们之前报告的那样


830
00:28:12,886 --> 00:28:14,836
这是对单个平均置信度得分的对比


831
00:28:14,836 --> 00:28:16,646
不过最大的改进


832
00:28:16,646 --> 00:28:18,126
来自瞳孔检测


833
00:28:18,706 --> 00:28:19,996
如你所见 


834
00:28:19,996 --> 00:28:21,296
右侧的图像检测到瞳孔的


835
00:28:21,296 --> 00:28:23,066
准确度要高得多
 
00:28:23,516 --> 00:28:25,886
我们来看看


836
00:28:25,886 --> 00:28:26,416
客户端代码示例


837
00:28:27,986 --> 00:28:29,676
这个代码片段


838
00:28:29,676 --> 00:28:31,056
将在整个演示过程中重复


839
00:28:31,056 --> 00:28:33,206
所以我们第一次将逐行进行


840
00:28:33,496 --> 00:28:36,246
另外，我在我的样本中


841
00:28:36,246 --> 00:28:37,946
没有使用边界条件


842
00:28:37,946 --> 00:28:39,376
只是为了简化演示


843
00:28:39,376 --> 00:28:40,736
那么在开发 App 时


844
00:28:40,786 --> 00:28:42,276
你也许应该使用适当的错误处理


845
00:28:42,276 --> 00:28:43,516
来避免不想要的边界情况


846
00:28:44,256 --> 00:28:45,156
下面我们回到示例上来


847
00:28:46,276 --> 00:28:47,276
为了得到你的


848
00:28:47,276 --> 00:28:48,866
面部标志


849
00:28:48,866 --> 00:28:49,416
首先你需要创建一个


850
00:28:49,416 --> 00:28:50,716
DetectFaceLandmarksRequest


851
00:28:51,296 --> 00:28:52,536
然后，你需要创建


852
00:28:52,536 --> 00:28:54,036
ImageRequestHandler


853
00:28:54,036 --> 00:28:56,096
将需要处理的图像传递给它


854
00:28:56,096 --> 00:28:58,346
然后使用


855
00:28:58,626 --> 00:28:59,626
该请求处理器


856
00:28:59,626 --> 00:29:00,776
来处理你的请求


857
00:29:01,426 --> 00:29:03,606
最后 你需要查看结果


858
00:29:04,386 --> 00:29:05,446
这张人脸


859
00:29:05,446 --> 00:29:06,736
在 Vision 框架中的


860
00:29:06,736 --> 00:29:08,156
所有相关结果


861
00:29:08,196 --> 00:29:09,946
都将以 faceObservation 的形式出现


862
00:29:10,506 --> 00:29:12,006
faceObservation 得出一些


863
00:29:12,006 --> 00:29:13,486
检测到的对象观察


864
00:29:13,856 --> 00:29:15,006
它继承了边界框属性


865
00:29:15,046 --> 00:29:16,396
并且还在其级别上


866
00:29:16,396 --> 00:29:17,766
添加了几个其他属性


867
00:29:17,766 --> 00:29:19,886
来描述人脸


868
00:29:20,766 --> 00:29:21,806
这次我们对


869
00:29:21,806 --> 00:29:22,636
标记属性感兴趣


870
00:29:23,136 --> 00:29:24,366
标记属性属于


871
00:29:24,426 --> 00:29:25,976
FaceLandmarks2D 类


872
00:29:26,176 --> 00:29:28,166
FaceLandmarks2D 类


873
00:29:28,416 --> 00:29:29,896
由置信度得分组成


874
00:29:30,086 --> 00:29:31,356
这是整个集合


875
00:29:31,356 --> 00:29:32,476
和多个面部区域的


876
00:29:32,476 --> 00:29:34,716
单个平均置信度得分


877
00:29:34,716 --> 00:29:37,296
其中每个面部区域


878
00:29:37,296 --> 00:29:40,176
由 FaceLandmarksRegion2D 类表示


879
00:29:40,636 --> 00:29:41,856
让我们仔细看一下


880
00:29:41,856 --> 00:29:44,536
这个类的属性


881
00:29:44,716 --> 00:29:46,166
首先是 pointCount


882
00:29:46,666 --> 00:29:48,156
PointCount 将告诉你


883
00:29:48,156 --> 00:29:49,546
有多少点代表


884
00:29:49,546 --> 00:29:50,756
特定的面部区域


885
00:29:50,996 --> 00:29:52,256
此属性将显示不同的值 


886
00:29:52,256 --> 00:29:53,616
具体取决于


887
00:29:53,616 --> 00:29:55,366
你如何配置请求


888
00:29:55,366 --> 00:29:56,936
比如 65 点消除


889
00:29:56,936 --> 00:29:58,376
还是 76 点消除


890
00:29:59,646 --> 00:30:01,146
normalizedPoints 属性


891
00:30:01,916 --> 00:30:05,596
将表示实际的标记点


892
00:30:05,596 --> 00:30:07,326
而 precisionEstimatesPerPoint 将表示


893
00:30:07,326 --> 00:30:08,916
实际标记点的


894
00:30:08,916 --> 00:30:10,416
实际置信度得分


895
00:30:11,456 --> 00:30:12,666
我们来看看所需的代码


896
00:30:12,666 --> 00:30:14,496
这与上一张幻灯片中


897
00:30:14,496 --> 00:30:16,106
代码片段相同


898
00:30:16,106 --> 00:30:17,056
但现在我们将从


899
00:30:17,056 --> 00:30:18,196
一个稍微不同的角度来看待它


900
00:30:18,576 --> 00:30:20,266
我们想看看


901
00:30:20,266 --> 00:30:22,696
在 Vision 框架中如何修改算法


902
00:30:23,446 --> 00:30:25,056
如果你获取此代码段


903
00:30:25,056 --> 00:30:26,276
并用去年的 SDK


904
00:30:26,276 --> 00:30:28,076
重新编译它


905
00:30:28,076 --> 00:30:30,086
你将得到的请求对象


906
00:30:30,086 --> 00:30:31,806
将配置如下


907
00:30:31,806 --> 00:30:33,196
Revision 属性将设置为 


908
00:30:33,196 --> 00:30:34,786
Revision2


909
00:30:34,786 --> 00:30:36,136
Cancellation 属性将设置为


910
00:30:36,136 --> 00:30:37,816
Cancellation65Points


911
00:30:38,396 --> 00:30:39,056
从技术上讲


912
00:30:39,056 --> 00:30:40,286
去年我们没有消除属性


913
00:30:40,286 --> 00:30:41,456
但如果我们这样做的话


914
00:30:41,456 --> 00:30:42,616
可以将它设置为一个单一的值


915
00:30:43,426 --> 00:30:45,726
另一方面 如果你获取相同的代码片段


916
00:30:45,726 --> 00:30:49,136
并用今年的 SDK 重新编译它


917
00:30:49,686 --> 00:30:50,336
你将得到的是 


918
00:30:50,336 --> 00:30:51,856
Revision 属性将被设置为


919
00:30:51,856 --> 00:30:52,846
Revision3


920
00:30:52,846 --> 00:30:55,056
Cancellation 属性


921
00:30:55,056 --> 00:30:57,076
将被设置为 Cancellation76Points


922
00:30:58,626 --> 00:30:59,706
这实际上代表了


923
00:30:59,706 --> 00:31:00,846
Vision 框架在默认情况下


924
00:31:00,846 --> 00:31:04,006
如何处理算法修订的原理


925
00:31:04,096 --> 00:31:05,766
如果你不能指定修订版


926
00:31:05,766 --> 00:31:07,716
我们将提供


927
00:31:07,716 --> 00:31:09,366
编译和链接代码所依据的


928
00:31:09,366 --> 00:31:11,916
SDK支持的最新版本


929
00:31:12,116 --> 00:31:14,066
当然


930
00:31:14,066 --> 00:31:14,856
我们始终建议


931
00:31:14,856 --> 00:31:15,836
明确设置这些属性


932
00:31:16,116 --> 00:31:17,106
这只是为了保证


933
00:31:17,106 --> 00:31:18,656
将来的确定性行为


934
00:31:19,386 --> 00:31:22,266
现在让我们使用


935
00:31:22,266 --> 00:31:23,556
今年开发的一项新指标


936
00:31:23,556 --> 00:31:24,286
面部捕捉质量


937
00:31:24,786 --> 00:31:25,946
屏幕上有两个图像


938
00:31:26,296 --> 00:31:27,416
你可以清楚地看到


939
00:31:27,416 --> 00:31:28,536
其中一幅图像


940
00:31:28,536 --> 00:31:29,436
是在更好的照明


941
00:31:29,436 --> 00:31:29,996
和聚焦条件下拍摄的


942
00:31:30,846 --> 00:31:32,016
我们想开发一个度量标准


943
00:31:32,016 --> 00:31:33,376
将图像作为一个整体来看待


944
00:31:33,376 --> 00:31:35,016
并给出一个分数


945
00:31:35,016 --> 00:31:36,796
用来说明面部捕捉质量


946
00:31:36,796 --> 00:31:37,966
的好坏


947
00:31:37,966 --> 00:31:40,366
因此 


948
00:31:40,366 --> 00:31:41,596
我们提出了面部捕捉质量指标


949
00:31:42,466 --> 00:31:43,656
我们用这种方法


950
00:31:43,656 --> 00:31:45,026
训练我们的模型


951
00:31:45,026 --> 00:31:46,876
因此如果图像是用弱光


952
00:31:46,876 --> 00:31:48,456
或弱焦点拍摄的


953
00:31:48,456 --> 00:31:50,206
或一个人有负面表情


954
00:31:50,236 --> 00:31:51,936
那他们的得分就会更低


955
00:31:52,896 --> 00:31:54,316
如果我们在这两个图像上


956
00:31:54,346 --> 00:31:55,626
运行此指标


957
00:31:55,626 --> 00:31:56,206
我们将得到我们的分数


958
00:31:57,036 --> 00:31:58,316
这些是浮点数


959
00:31:58,636 --> 00:31:59,636
你可以将它们相互比较


960
00:31:59,636 --> 00:32:00,776
你可以说


961
00:32:00,776 --> 00:32:02,856
得分更高的图像


962
00:32:02,856 --> 00:32:04,716
就是质量更好的图像


963
00:32:05,346 --> 00:32:08,136
接下来我们看一下代码示例


964
00:32:09,606 --> 00:32:10,756
这与我们之前看到的


965
00:32:10,756 --> 00:32:11,986
几张幻灯片非常相似


966
00:32:12,196 --> 00:32:13,416
不同之处


967
00:32:13,416 --> 00:32:16,136
在于请求类型和结果


968
00:32:16,846 --> 00:32:18,526
由于我们使用的图像没变


969
00:32:18,736 --> 00:32:19,556
我们可以再次查看 faceObservation 的结果


970
00:32:19,556 --> 00:32:20,826
但现在我们要看一下


971
00:32:20,826 --> 00:32:21,686
faceObservation 的不同属性


972
00:32:21,686 --> 00:32:24,626
即面部捕捉质量属性


973
00:32:24,876 --> 00:32:27,946
我们一起来看些其他的例子


974
00:32:28,616 --> 00:32:29,646
假设


975
00:32:29,646 --> 00:32:30,586
我有一系列图像


976
00:32:30,586 --> 00:32:31,976
可以通过自拍相机


977
00:32:31,976 --> 00:32:34,316
或照片连拍中的连拍模式获得


978
00:32:34,626 --> 00:32:36,166
你要问自己一个问题


979
00:32:36,426 --> 00:32:37,816
哪张照片


980
00:32:37,816 --> 00:32:38,566
拍摄的质量最好


981
00:32:39,686 --> 00:32:40,966
现在你可以做的是


982
00:32:40,966 --> 00:32:42,166
对每一张图像运行我们的算法


983
00:32:42,506 --> 00:32:45,366
分配分数 对它们进行排名


984
00:32:45,366 --> 00:32:47,876
最亮的地方的图像


985
00:32:47,876 --> 00:32:50,896
是以最佳质量捕捉的图像


986
00:32:50,966 --> 00:32:52,436
我们试着去理解


987
00:32:52,436 --> 00:32:54,026
如何解释


988
00:32:54,026 --> 00:32:55,856
面部捕捉质量指标的结果


989
00:32:56,586 --> 00:32:58,206
在幻灯片上


990
00:32:58,356 --> 00:32:58,886
我有两个图像序列


991
00:32:59,426 --> 00:33:00,726
每个序列都是同一个人的


992
00:33:00,726 --> 00:33:02,146
每个序列由


993
00:33:02,146 --> 00:33:03,456
在面部捕获质量方面


994
00:33:03,456 --> 00:33:07,216
得分最低和最高的图像表示


995
00:33:08,116 --> 00:33:09,076
对于这些范围


996
00:33:09,076 --> 00:33:09,496
我们能说些什么呢


997
00:33:10,816 --> 00:33:12,356
存在一些重叠区域 


998
00:33:12,356 --> 00:33:13,696
但也有一些区域


999
00:33:13,696 --> 00:33:15,006
属于某一区域


1000
00:33:15,006 --> 00:33:16,336
而不属于另一个区域


1001
00:33:16,516 --> 00:33:18,216
如果你有另一个序列


1002
00:33:18,536 --> 00:33:19,306
可能会发生


1003
00:33:19,306 --> 00:33:20,926
完全没有重叠区域的情况


1004
00:33:21,856 --> 00:33:22,976
在这里我提出的观点是


1005
00:33:22,976 --> 00:33:24,686
不应将面部捕捉质量


1006
00:33:24,686 --> 00:33:26,906
与阈值进行比较


1007
00:33:28,126 --> 00:33:29,466
在这个特定的例子中


1008
00:33:29,466 --> 00:33:32,096
如果我选择 0.52


1009
00:33:32,326 --> 00:33:33,876
我将会错过左边的所有图像


1010
00:33:33,876 --> 00:33:36,556
而且我几乎可以得到


1011
00:33:36,556 --> 00:33:37,406
刚刚超过右边中点的


1012
00:33:37,406 --> 00:33:38,626
所有图像


1013
00:33:39,996 --> 00:33:41,676
但是什么是面部捕捉质量呢


1014
00:33:42,706 --> 00:33:44,246
我们定义面部捕获质量


1015
00:33:44,246 --> 00:33:46,286
是对同一主题的比较


1016
00:33:46,326 --> 00:33:47,766
或排名测量


1017
00:33:48,186 --> 00:33:49,756
现在比较和相同


1018
00:33:49,756 --> 00:33:51,086
是这句话的关键词


1019
00:33:51,526 --> 00:33:53,216
如果你在想


1020
00:33:53,216 --> 00:33:54,476
很酷我有这个很棒的新指标


1021
00:33:54,476 --> 00:33:55,606
我将用它开发我的


1022
00:33:55,606 --> 00:33:55,726
选美比赛 App


1023
00:33:56,946 --> 00:33:58,126
我想可能不是一个好主意


1024
00:33:58,606 --> 00:33:59,636
在选美比赛 App 中


1025
00:33:59,636 --> 00:34:01,296
你必须比较不同人的面孔


1026
00:34:01,296 --> 00:34:02,806
而这并不是该指标


1027
00:34:02,806 --> 00:34:04,626
开发和设计的目的


1028
00:34:06,266 --> 00:34:07,696
这就是人脸技术的改变和发展


1029
00:34:09,295 --> 00:34:10,076
下面我们来看一下


1030
00:34:10,076 --> 00:34:11,146
今年新增加的


1031
00:34:11,146 --> 00:34:11,335
探测技术


1032
00:34:12,896 --> 00:34:15,466
我们正在推出人体探测器


1033
00:34:15,466 --> 00:34:16,726
探测由人头和躯干组成的


1034
00:34:16,726 --> 00:34:18,076
人体上半身


1035
00:34:18,076 --> 00:34:20,386
还有一个宠物探测器


1036
00:34:20,795 --> 00:34:22,326
一个探测猫和狗的


1037
00:34:22,696 --> 00:34:23,406
动物探测器


1038
00:34:23,735 --> 00:34:24,755
动物探测器能为你提供边界框


1039
00:34:24,755 --> 00:34:26,166
除了边界框之外


1040
00:34:26,166 --> 00:34:27,286
它还为你提供了一个标签


1041
00:34:27,286 --> 00:34:30,235
说明检测到哪种动物


1042
00:34:31,795 --> 00:34:32,746
我们来看看


1043
00:34:32,746 --> 00:34:33,226
客户端代码示例


1044
00:34:35,956 --> 00:34:37,916
两个片段


1045
00:34:37,916 --> 00:34:39,366
一个用于人体探测器


1046
00:34:39,366 --> 00:34:39,815
一个用于动物探测器


1047
00:34:40,326 --> 00:34:41,386
与我们之前的情况非常相似


1048
00:34:41,386 --> 00:34:41,835
非常相似


1049
00:34:42,016 --> 00:34:43,646
同样差异在于你创建的


1050
00:34:43,646 --> 00:34:46,386
请求类型和结果中


1051
00:34:47,186 --> 00:34:49,525
现在对于人类探测器部分


1052
00:34:49,826 --> 00:34:51,275
我们关心的只是边界框


1053
00:34:51,886 --> 00:34:53,166
因此我们用其为


1054
00:34:53,306 --> 00:34:54,606
DetectedObjectObservation 服务


1055
00:34:55,726 --> 00:34:56,706
另一方面


1056
00:34:56,706 --> 00:34:57,706
对于动物探测器


1057
00:34:57,706 --> 00:34:58,986
我们也需要标签


1058
00:34:59,676 --> 00:35:01,206
因此我们使用


1059
00:35:01,206 --> 00:35:02,496
源于检测到的对象观察的


1060
00:35:02,496 --> 00:35:03,076
RecognizedObjectObservation


1061
00:35:03,186 --> 00:35:04,536
它延续了边界框


1062
00:35:04,536 --> 00:35:07,296
但也在边界框上添加了标签属性


1063
00:35:07,806 --> 00:35:10,486
这是新的探测器


1064
00:35:11,246 --> 00:35:12,456
下面让我们来看看


1065
00:35:12,456 --> 00:35:13,336
今年有关追踪的新内容


1066
00:35:14,216 --> 00:35:15,126
我们正在为追踪技术


1067
00:35:15,126 --> 00:35:16,556
设计一个新修订版


1068
00:35:16,556 --> 00:35:17,996
今年的变化是


1069
00:35:17,996 --> 00:35:19,516
边界框扩展区域


1070
00:35:19,516 --> 00:35:20,396
有所改进


1071
00:35:21,266 --> 00:35:22,386
我们现在可以更好的处理


1072
00:35:22,386 --> 00:35:22,976
遮挡问题


1073
00:35:23,536 --> 00:35:25,716
我们这次是以


1074
00:35:25,716 --> 00:35:26,176
机器学习为基础的


1075
00:35:26,886 --> 00:35:28,086
我们可以在多个


1076
00:35:28,086 --> 00:35:29,046
处理器设备上


1077
00:35:29,046 --> 00:35:29,476
以低功耗运行


1078
00:35:29,476 --> 00:35:32,516
我们来看一个示例


1079
00:35:32,956 --> 00:35:35,056
我这里有一个微视频剪辑


1080
00:35:35,056 --> 00:35:36,446
视频中一个男人在森林里跑步


1081
00:35:36,606 --> 00:35:38,646
他有时出现在树后


1082
00:35:38,976 --> 00:35:40,126
如你所见


1083
00:35:40,126 --> 00:35:41,606
跟踪器能够成功地


1084
00:35:41,606 --> 00:35:42,926
重新捕捉跟踪对象


1085
00:35:42,926 --> 00:35:44,186
并继续跟踪序列


1086
00:35:46,016 --> 00:35:47,046
[掌声]


1087
00:35:47,046 --> 00:35:47,486
谢谢


1088
00:35:48,516 --> 00:35:51,976
[掌声]


1089
00:35:52,476 --> 00:35:54,256
我们来看看客户端代码示例


1090
00:35:54,676 --> 00:35:56,046
这与我们去年


1091
00:35:56,046 --> 00:35:56,996
展示的片段完全相同


1092
00:35:57,256 --> 00:35:58,306
它代表了


1093
00:35:58,306 --> 00:35:59,416
你可以想象到的


1094
00:35:59,416 --> 00:35:59,946
最简单的跟踪序列


1095
00:36:00,166 --> 00:36:01,106
它会连续 5 帧


1096
00:36:01,106 --> 00:36:02,786
跟踪你感兴趣的对象


1097
00:36:03,986 --> 00:36:05,356
我想逐行解释


1098
00:36:05,356 --> 00:36:07,116
但在这里我想强调两点


1099
00:36:07,486 --> 00:36:09,116
首先是我们使用


1100
00:36:09,116 --> 00:36:10,036
SequenceRequestHandler


1101
00:36:11,066 --> 00:36:12,046
这与我们目前


1102
00:36:12,046 --> 00:36:13,226
在整个演示中使用的


1103
00:36:13,226 --> 00:36:14,226
ImageRequestHandler


1104
00:36:14,226 --> 00:36:14,816
完全不同


1105
00:36:15,336 --> 00:36:16,616
SequenceRequestHandler


1106
00:36:16,616 --> 00:36:17,986
在处理帧序列时用于 Vision


1107
00:36:17,986 --> 00:36:19,216
你需要在


1108
00:36:19,216 --> 00:36:20,426
帧与帧之间


1109
00:36:20,426 --> 00:36:21,496
缓存一些信息


1110
00:36:22,826 --> 00:36:24,356
第二点是


1111
00:36:24,356 --> 00:36:25,106
在实现跟踪序列时


1112
00:36:25,106 --> 00:36:26,836
需要从迭代编号 n 中


1113
00:36:26,836 --> 00:36:28,356
获取结果


1114
00:36:28,356 --> 00:36:30,026
并将其作为输入


1115
00:36:30,026 --> 00:36:31,276
提供给持续时间数 n+1


1116
00:36:31,976 --> 00:36:35,216
当然如果你使用


1117
00:36:35,216 --> 00:36:36,106
当前版本 SDK 重新编译


1118
00:36:36,106 --> 00:36:37,646
则默认情况下


1119
00:36:37,646 --> 00:36:38,846
请求的 revision


1120
00:36:38,846 --> 00:36:40,316
将设置为 Revision2


1121
00:36:40,496 --> 00:36:41,836
不过我们也


1122
00:36:41,836 --> 00:36:42,426
建议明确设置它


1123
00:36:42,966 --> 00:36:45,106
这就是跟踪


1124
00:36:46,136 --> 00:36:47,376
让我们来看看


1125
00:36:47,376 --> 00:36:50,396
有关 Vision 和 Core ML 集成的消息


1126
00:36:51,146 --> 00:36:52,766
去年 我们展示了


1127
00:36:52,766 --> 00:36:53,836
Vision 和 Core ML 的集成


1128
00:36:53,836 --> 00:36:55,496
并展示了


1129
00:36:55,496 --> 00:36:57,066
如何通过 Vision API 


1130
00:36:57,066 --> 00:36:57,596
运行 Core ML 模型


1131
00:36:57,596 --> 00:36:59,776
这样做的好处是


1132
00:36:59,776 --> 00:37:01,706
你可以使用 1 到 5 个


1133
00:37:01,816 --> 00:37:03,236
不同的图像请求处理程序重载


1134
00:37:03,236 --> 00:37:04,916
来将你手中的图像


1135
00:37:04,916 --> 00:37:06,266
转换为 Core ML 模型所需的


1136
00:37:06,586 --> 00:37:08,976
图像类型


1137
00:37:08,976 --> 00:37:11,266
大小和颜色方案


1138
00:37:12,246 --> 00:37:13,256
我们将为你运行推理


1139
00:37:13,256 --> 00:37:15,016
并将来自 Core ML 模型的


1140
00:37:15,016 --> 00:37:16,436
输出或结果


1141
00:37:16,436 --> 00:37:17,886
打包到 Vision 观察值中


1142
00:37:20,716 --> 00:37:22,426
现在假设你有


1143
00:37:22,426 --> 00:37:23,696
一个不同的任务


1144
00:37:23,696 --> 00:37:24,646
例如你想进行


1145
00:37:24,646 --> 00:37:26,066
图像样式传输


1146
00:37:26,066 --> 00:37:27,416
你需要至少有两个图像


1147
00:37:27,416 --> 00:37:29,056
即图像内容和图像样式


1148
00:37:29,256 --> 00:37:30,356
你可能还需要


1149
00:37:30,356 --> 00:37:31,816
一些混合比例


1150
00:37:31,926 --> 00:37:33,826
说明在内容上需要应用多少样式


1151
00:37:34,306 --> 00:37:35,506
所以现在我有三个参数


1152
00:37:36,826 --> 00:37:37,886
今年


1153
00:37:37,886 --> 00:37:39,666
我们将推出 API


1154
00:37:39,666 --> 00:37:41,716
在这里我们可以


1155
00:37:41,896 --> 00:37:43,476
通过 Vision 将多个输入用于 Core ML


1156
00:37:43,476 --> 00:37:44,426
这包括多个图像输入


1157
00:37:44,426 --> 00:37:47,696
此外在输出部分


1158
00:37:48,186 --> 00:37:49,686
这个示例仅显示了一个输出


1159
00:37:49,686 --> 00:37:50,616
但是如果你有多个


1160
00:37:50,616 --> 00:37:52,116
特别是如果你有


1161
00:37:52,116 --> 00:37:53,376
多个相同类型


1162
00:37:53,376 --> 00:37:55,256
那么当它们后面


1163
00:37:55,256 --> 00:37:56,356
以观察形式出现时


1164
00:37:56,356 --> 00:37:57,946
很难区分开来


1165
00:37:58,416 --> 00:37:59,616
那么今年我们做了什么


1166
00:37:59,616 --> 00:38:00,646
我们在观察中


1167
00:38:00,646 --> 00:38:02,616
引入了一个新的字段


1168
00:38:02,616 --> 00:38:04,376
该字段完全映射到


1169
00:38:04,376 --> 00:38:05,056
输出部分中显示的名称


1170
00:38:06,216 --> 00:38:08,316
我们来看看输入和输出


1171
00:38:08,316 --> 00:38:09,316
我们将在下一张幻灯片中


1172
00:38:09,316 --> 00:38:09,576
使用它们


1173
00:38:12,936 --> 00:38:14,136
这是一段代码片段


1174
00:38:14,176 --> 00:38:16,816
它表示如何通过 Vision


1175
00:38:16,956 --> 00:38:17,396
使用 Core ML


1176
00:38:18,676 --> 00:38:20,236
突出显示的部分


1177
00:38:20,446 --> 00:38:21,256
显示了今年的新内容


1178
00:38:21,716 --> 00:38:22,796
我们现在先暂时保存它们


1179
00:38:22,796 --> 00:38:23,896
我们去查看代码


1180
00:38:23,896 --> 00:38:25,316
稍后再返回


1181
00:38:26,156 --> 00:38:27,466
要通过 Vision 运行 Core ML


1182
00:38:27,466 --> 00:38:29,446
首先需要


1183
00:38:29,446 --> 00:38:30,236
记录 Core ML 模型


1184
00:38:31,266 --> 00:38:32,926
然后你需要围绕它创建


1185
00:38:32,926 --> 00:38:34,956
Vision CoreMLmodel 包装器


1186
00:38:35,726 --> 00:38:37,476
之后你需要创建


1187
00:38:37,476 --> 00:38:39,346
Vision CoreMLRequest


1188
00:38:39,346 --> 00:38:39,676
并传入该包装器


1189
00:38:41,266 --> 00:38:42,326
下一步


1190
00:38:42,556 --> 00:38:44,136
创建 ImageRequestHandler


1191
00:38:44,136 --> 00:38:45,446
处理你的请求


1192
00:38:45,446 --> 00:38:46,086
然后查看结果


1193
00:38:47,386 --> 00:38:49,596
现在 使用我们今年添加的新 API


1194
00:38:49,596 --> 00:38:51,596
只有你去年


1195
00:38:51,596 --> 00:38:53,116
可以使用的图像是默认图像


1196
00:38:53,116 --> 00:38:54,986
或者主图像


1197
00:38:54,986 --> 00:38:56,276
是传递给 ImageRequestHandler 的图像


1198
00:38:56,476 --> 00:38:58,046
但这也是需要


1199
00:38:58,046 --> 00:38:59,526
将图像名称分配给


1200
00:38:59,526 --> 00:39:01,126
CoreMLModel 包装器的


1201
00:39:01,506 --> 00:39:03,796
inputImageFeatureName 字段


1202
00:39:05,026 --> 00:39:06,886
所有其他参数


1203
00:39:06,886 --> 00:39:08,646
无论图像与否


1204
00:39:08,716 --> 00:39:10,406
都必须通过 CoreMLmodel 包装的


1205
00:39:10,446 --> 00:39:12,016
featureProvider 属性传递


1206
00:39:12,376 --> 00:39:14,036
如你所见


1207
00:39:14,036 --> 00:39:15,436
图像样式和混合比例


1208
00:39:15,436 --> 00:39:15,666
是以这种方式传递的


1209
00:39:16,956 --> 00:39:18,336
最后当你查看结果时


1210
00:39:18,336 --> 00:39:19,506
你可以查看


1211
00:39:19,786 --> 00:39:21,386
出现的观察结果的


1212
00:39:21,386 --> 00:39:22,646
featureName


1213
00:39:22,646 --> 00:39:24,186
并且你可以在此情况下


1214
00:39:24,186 --> 00:39:25,246
将其与 imageResult 进行比较


1215
00:39:25,506 --> 00:39:26,656
这正是 Core ML 输出部分中


1216
00:39:26,656 --> 00:39:27,706
出现的名称


1217
00:39:27,756 --> 00:39:29,066
这样你就可以


1218
00:39:29,096 --> 00:39:30,666
相应地处理结果


1219
00:39:32,636 --> 00:39:33,626
今天的演示


1220
00:39:33,626 --> 00:39:34,666
到这里就要结束了


1221
00:39:34,966 --> 00:39:36,076
想要获得其他详细信息


1222
00:39:36,076 --> 00:39:37,136
请参阅幻灯片上的链接


1223
00:39:37,466 --> 00:39:39,976
谢谢大家


1224
00:39:40,016 --> 00:39:42,000
[掌声]


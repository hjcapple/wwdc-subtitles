1
00:00:00,506 --> 00:00:05,460
[音乐]


2
00:00:07,096 --> 00:00:07,976
>> 下午好


3
00:00:08,516 --> 00:00:14,716
[掌声]


4
00:00:15,216 --> 00:00:16,085
大家好


5
00:00:16,346 --> 00:00:18,076
欢迎来到我们


6
00:00:18,076 --> 00:00:19,636
介绍 ARKit 3 的会议


7
00:00:20,046 --> 00:00:21,286
我的名字是 Andreas


8
00:00:21,606 --> 00:00:23,466
我是 ARKit 团队的一名工程师


9
00:00:23,926 --> 00:00:25,196
今天我很高兴


10
00:00:25,196 --> 00:00:26,896
能在这里向大家介绍


11
00:00:26,896 --> 00:00:30,016
ARKit 的第三个主要发布


12
00:00:32,046 --> 00:00:33,926
当我们在 2017 年推出 ARKit 时


13
00:00:33,926 --> 00:00:37,196
我们将 iOS 打造成世界上


14
00:00:37,196 --> 00:00:39,976
最大的 AR 平台


15
00:00:39,976 --> 00:00:43,076
将 AR 带到了数亿台 iOS 设备上


16
00:00:43,236 --> 00:00:45,496
这对于你来说真的很重要


17
00:00:45,496 --> 00:00:47,516
因为你可以通过


18
00:00:47,516 --> 00:00:49,506
App 和你编写的游戏


19
00:00:49,506 --> 00:00:52,896
来接触广泛的受众


20
00:00:53,176 --> 00:00:54,626
我们的使命是


21
00:00:54,626 --> 00:00:55,946
从一开始就让你编写的


22
00:00:56,066 --> 00:00:58,056
第一个增强现实的 App


23
00:00:58,056 --> 00:01:00,106
变得很简单


24
00:01:00,106 --> 00:01:01,186
即使你是一个新的开发人员


25
00:01:01,186 --> 00:01:03,626
但我们也希望为你提供


26
00:01:03,696 --> 00:01:05,906
你需要的手头工具


27
00:01:05,906 --> 00:01:09,126
来创建真正先进和复杂的体验


28
00:01:09,126 --> 00:01:12,116
所以如果你今天看看 App Store


29
00:01:12,116 --> 00:01:15,896
我们可以看到你做了一份了不起的工作


30
00:01:15,896 --> 00:01:17,606
你构建了很棒的 App 和游戏


31
00:01:18,126 --> 00:01:19,426
现在让我们来看看


32
00:01:19,426 --> 00:01:22,956
其中的一些


33
00:01:22,956 --> 00:01:25,086
将你的游戏理念带入到 AR 


34
00:01:25,086 --> 00:01:26,866
可以让它们更有


35
00:01:26,866 --> 00:01:29,426
吸引力和物理效果 如愤怒的小鸟


36
00:01:29,776 --> 00:01:31,226
你现在可以在 AR 中玩这个了


37
00:01:31,226 --> 00:01:32,446
实际上在这些建筑周围工作


38
00:01:32,446 --> 00:01:35,146
找到最佳


39
00:01:35,146 --> 00:01:38,076
射击地点 然后必须


40
00:01:38,076 --> 00:01:39,436
用自己的手弹射


41
00:01:39,436 --> 00:01:41,926
射出那些愤怒的小鸟


42
00:01:45,196 --> 00:01:47,426
对于大规模的用例


43
00:01:47,426 --> 00:01:48,806
ARKit 也非常有效


44
00:01:49,416 --> 00:01:52,236
iScape 是一种用于户外景观美化的工具


45
00:01:52,236 --> 00:01:54,856
我们可以放置灌木和树木


46
00:01:54,856 --> 00:01:56,116
并在你的花园或后院


47
00:01:56,116 --> 00:01:57,776
观看你的下一个


48
00:01:57,776 --> 00:02:02,976
花园改造项目在 AR 里开始运作


49
00:02:03,056 --> 00:02:05,706
去年 通过 ARKit 2 我们


50
00:02:05,706 --> 00:02:08,746
推出了 USDZ 一个新的 3D 文件


51
00:02:08,746 --> 00:02:11,766
格式 用于交换为 AR 


52
00:02:11,766 --> 00:02:13,106
制作的格式


53
00:02:14,126 --> 00:02:16,116
Wayfair 用它把 


54
00:02:16,116 --> 00:02:18,136
虚拟家具放在你的家里


55
00:02:18,136 --> 00:02:21,266
但是利用 ARKit 先进的


56
00:02:21,366 --> 00:02:22,806
场景理解功能


57
00:02:22,806 --> 00:02:24,226
比如环境纹理


58
00:02:24,396 --> 00:02:25,646
这些物品就真的会与你的


59
00:02:25,646 --> 00:02:27,626
客厅完美融合


60
00:02:27,776 --> 00:02:31,236
乐高正在用


61
00:02:31,236 --> 00:02:33,836
ARKit 的 3D 目标检测功能


62
00:02:35,326 --> 00:02:37,006
它可以找到你的物理乐高集


63
00:02:37,006 --> 00:02:38,696
并在 AR 中增强


64
00:02:38,696 --> 00:02:40,706
多亏 ARKit 的多用户支持


65
00:02:40,706 --> 00:02:42,456
你甚至可以


66
00:02:42,456 --> 00:02:44,906
和你的朋友们一起玩


67
00:02:45,396 --> 00:02:47,486
所以这些只是你创建的


68
00:02:47,486 --> 00:02:48,686
一些例子


69
00:02:49,476 --> 00:02:52,366
ARKit 帮助你处理所有的 


70
00:02:52,786 --> 00:02:54,296
技术细节


71
00:02:54,296 --> 00:02:56,266
为你做繁重的工作


72
00:02:56,266 --> 00:02:57,886
使得现实感增强


73
00:02:58,016 --> 00:02:59,886
以便你可以集中精神


74
00:03:00,186 --> 00:03:02,216
创建它们周围美妙的体验


75
00:03:03,016 --> 00:03:04,986
让我们快速回顾一下


76
00:03:05,186 --> 00:03:06,256
ARKit 为你提供的


77
00:03:06,256 --> 00:03:09,046
三大功能支柱


78
00:03:10,806 --> 00:03:12,376
首先是追踪


79
00:03:12,376 --> 00:03:16,206
追踪可以确定你的


80
00:03:16,206 --> 00:03:17,886
设备相对于环境的位置


81
00:03:17,886 --> 00:03:21,596
这样虚拟内容就可以准确定位


82
00:03:21,706 --> 00:03:24,026
并在摄像机图像上


83
00:03:24,026 --> 00:03:26,046
实时更新


84
00:03:27,186 --> 00:03:28,876
这就产生了


85
00:03:29,136 --> 00:03:30,876
虚拟内容实际上放置于


86
00:03:30,996 --> 00:03:32,736
现实世界的假象


87
00:03:33,906 --> 00:03:35,576
ARKit 还为你提供


88
00:03:35,636 --> 00:03:37,116
不同的追踪技术


89
00:03:37,116 --> 00:03:38,766
如道路世界追踪


90
00:03:39,136 --> 00:03:42,306
人脸追踪或图像追踪


91
00:03:42,996 --> 00:03:45,766
在追踪方面 我们有场景理解


92
00:03:47,606 --> 00:03:49,036
通过场景理解


93
00:03:49,036 --> 00:03:51,696
你可以识别场景中的表面


94
00:03:51,826 --> 00:03:54,266
图像和 3D 对象


95
00:03:54,266 --> 00:03:56,566
并将虚拟内容直接附加其上


96
00:03:58,196 --> 00:03:59,696
场景理解还可以


97
00:03:59,696 --> 00:04:01,416
了解环境中的光照


98
00:04:01,456 --> 00:04:03,556
甚至纹理


99
00:04:03,556 --> 00:04:06,226
以帮助你的内容看起来更加真实


100
00:04:06,486 --> 00:04:09,016
最后是渲染


101
00:04:09,146 --> 00:04:11,266
它让你的 3D 内容栩栩如生


102
00:04:12,806 --> 00:04:14,066
我们一直支持不同的


103
00:04:14,066 --> 00:04:17,406
渲染器 如 SceneKit SpriteKit 和 Metal


104
00:04:17,446 --> 00:04:19,906
现在 今年的


105
00:04:20,166 --> 00:04:22,176
RealityKit 也从一开始


106
00:04:22,176 --> 00:04:24,666
就在考虑增强现实


107
00:04:27,416 --> 00:04:29,046
因此今年随着 ARKit 的发布


108
00:04:29,046 --> 00:04:31,536
它们正在实现巨大的飞跃


109
00:04:32,416 --> 00:04:34,526
你的体验不仅


110
00:04:34,766 --> 00:04:36,236
看起来更好


111
00:04:36,316 --> 00:04:38,616
更自然 你还可以


112
00:04:38,616 --> 00:04:40,886
为之前无法


113
00:04:40,886 --> 00:04:43,396
实现的用例


114
00:04:43,396 --> 00:04:44,936
创建全新的体验


115
00:04:45,056 --> 00:04:47,646
感谢 ARKit 3 带来的


116
00:04:47,756 --> 00:04:49,706
许多新功能


117
00:04:50,796 --> 00:04:53,116
如人物遮挡 动作


118
00:04:53,116 --> 00:04:55,286
捕捉 协作会话


119
00:04:55,826 --> 00:04:57,396
同时使用前


120
00:04:57,396 --> 00:04:59,246
后摄像头 追踪


121
00:04:59,246 --> 00:05:00,836
多个人脸等等


122
00:05:01,616 --> 00:05:02,966
我们要涉及很多内容


123
00:05:02,966 --> 00:05:05,586
所以让我们直接进入


124
00:05:05,586 --> 00:05:07,986
从人物遮挡开始


125
00:05:07,986 --> 00:05:12,066
让我们看看这里的场景


126
00:05:13,356 --> 00:05:14,406
因此为了创造一个


127
00:05:14,406 --> 00:05:16,846
令人信服的 AR 体验


128
00:05:16,846 --> 00:05:18,586
准确定位


129
00:05:18,736 --> 00:05:21,226
虚拟内容以及


130
00:05:21,226 --> 00:05:22,536
匹配真实世界中的灯光很重要


131
00:05:23,626 --> 00:05:25,076
所以让我们把一个虚拟的


132
00:05:25,406 --> 00:05:27,746
浓缩咖啡机放在这 把它放在桌子上


133
00:05:29,146 --> 00:05:31,066
但是等一下 当人们进入


134
00:05:31,176 --> 00:05:32,616
框架 就像在这个例子中一样


135
00:05:32,646 --> 00:05:34,356
它很容易打破幻觉


136
00:05:34,996 --> 00:05:35,956
因为你会期望


137
00:05:36,086 --> 00:05:37,996
前面的人实际上


138
00:05:38,136 --> 00:05:39,356
覆盖浓缩咖啡机


139
00:05:41,266 --> 00:05:43,186
所以有了 ARKit 3 和 


140
00:05:43,306 --> 00:05:45,996
人物遮挡 你就可以解决这个问题


141
00:05:47,106 --> 00:05:49,106
[掌声]


142
00:05:49,236 --> 00:05:49,526
谢谢


143
00:05:51,766 --> 00:05:53,666
所以让我们来看看这是如何完成的


144
00:05:55,546 --> 00:05:57,356
默认情况下虚拟内容


145
00:05:57,356 --> 00:05:59,736
呈现在相机图像的顶部


146
00:05:59,826 --> 00:06:04,726
如你所见 对于纯桌面体验


147
00:06:04,726 --> 00:06:06,296
这很好 但如果


148
00:06:06,296 --> 00:06:07,626
框架中的任何人


149
00:06:07,626 --> 00:06:09,806
在该对象前面


150
00:06:09,806 --> 00:06:10,816
增强则看起来不再准确


151
00:06:12,046 --> 00:06:14,566
所以 ARKit 3 现在为你做的是


152
00:06:15,456 --> 00:06:17,396
由于机器学习


153
00:06:17,676 --> 00:06:19,316
识别出框架中存在的人


154
00:06:19,316 --> 00:06:21,266
创建一个单独的图层


155
00:06:21,266 --> 00:06:23,506
只有包含这些


156
00:06:24,026 --> 00:06:25,176
人的像素


157
00:06:25,556 --> 00:06:26,976
我们称之为分割


158
00:06:28,286 --> 00:06:31,216
然后我们就可以在其他层之上渲染该层


159
00:06:31,456 --> 00:06:34,776
让我们来看看合成图像


160
00:06:35,416 --> 00:06:36,696
现在看起来好多了


161
00:06:37,676 --> 00:06:38,786
如果你凑近看


162
00:06:38,786 --> 00:06:39,996
其实仍然不够正确


163
00:06:41,206 --> 00:06:42,836
所以现在前面的人


164
00:06:43,146 --> 00:06:45,106
堵住了浓缩咖啡机


165
00:06:46,246 --> 00:06:47,506
但是这里如果拉近镜头


166
00:06:47,506 --> 00:06:49,086
你可以看到后面的人


167
00:06:49,086 --> 00:06:51,066
也被渲染到


168
00:06:51,066 --> 00:06:53,206
虚拟对象的顶部


169
00:06:53,306 --> 00:06:55,376
虽然她实际上站在桌子后面


170
00:06:55,856 --> 00:06:57,616
因此在这种情况下 虚拟模型


171
00:06:57,706 --> 00:07:00,406
应该挡住她 而不是反过来这种情况


172
00:07:00,406 --> 00:07:04,446
现在 发生这种情况是因为


173
00:07:04,446 --> 00:07:06,396
我们没有考虑到


174
00:07:06,516 --> 00:07:08,316
人们与摄像机的距离


175
00:07:10,786 --> 00:07:12,816
当 ARKit 3 使用高级的


176
00:07:12,816 --> 00:07:14,246
机器学习来执行


177
00:07:14,246 --> 00:07:15,716
额外的深度估算


178
00:07:15,716 --> 00:07:18,446
步骤 用此估计


179
00:07:18,446 --> 00:07:19,886
分割的人离


180
00:07:19,886 --> 00:07:22,306
相机多远


181
00:07:22,346 --> 00:07:23,786
我们现在可以更正渲染顺序


182
00:07:23,786 --> 00:07:26,016
保证人在前面时进行渲染


183
00:07:26,016 --> 00:07:28,586
如果他们实际上离相机更近的话


184
00:07:28,746 --> 00:07:31,286
并且由于 Apple


185
00:07:31,286 --> 00:07:33,606
神经网络引擎我们能够


186
00:07:33,606 --> 00:07:36,206
在每个框架上进行实时操作


187
00:07:36,206 --> 00:07:40,736
现在让我们来看看


188
00:07:40,736 --> 00:07:41,876
合成图像


189
00:07:42,636 --> 00:07:43,436
正如你所期待的那样


190
00:07:43,536 --> 00:07:44,946
你看到人物


191
00:07:45,246 --> 00:07:47,186
遮挡的虚拟内容


192
00:07:50,356 --> 00:07:52,356
[掌声]


193
00:07:52,696 --> 00:07:53,976
这真的很棒 谢谢你们


194
00:07:58,146 --> 00:08:00,946
因此人物遮挡可以让


195
00:08:00,946 --> 00:08:03,186
虚拟内容呈现在人物后面


196
00:08:04,296 --> 00:08:05,856
它也适用于场景中


197
00:08:05,906 --> 00:08:08,276
的多个人 甚至


198
00:08:08,276 --> 00:08:09,726
可以在人们只是


199
00:08:09,796 --> 00:08:11,936
部分可见的情况下就像


200
00:08:11,936 --> 00:08:13,696
在示例之前 


201
00:08:13,696 --> 00:08:15,766
桌子后面的女人实际上


202
00:08:15,766 --> 00:08:18,726
看不到整个身体 但是它仍然有效


203
00:08:19,816 --> 00:08:21,196
现在 这非常重要


204
00:08:21,196 --> 00:08:23,166
因为它不仅使


205
00:08:23,166 --> 00:08:25,226
你的体验看起来


206
00:08:25,226 --> 00:08:27,686
比以前更真实 而且


207
00:08:27,976 --> 00:08:29,356
对你来说也意味着你现在可以


208
00:08:29,356 --> 00:08:32,556
创建以前不可能的体验


209
00:08:33,446 --> 00:08:34,566
例如 考虑到一个


210
00:08:34,566 --> 00:08:36,655
多人游戏 在这个游戏中


211
00:08:36,966 --> 00:08:38,556
框架中有人和你的


212
00:08:38,856 --> 00:08:40,666
虚拟内容


213
00:08:42,976 --> 00:08:44,786
人物遮挡也


214
00:08:44,876 --> 00:08:48,396
集成在 ARView 和 ARSCNView 中


215
00:08:48,396 --> 00:08:51,616
而且由于深度估算


216
00:08:51,736 --> 00:08:53,326
我们可以为你提供一个


217
00:08:53,326 --> 00:08:55,456
关于相机监测到的


218
00:08:55,516 --> 00:09:00,436
人物距离的相似值


219
00:09:00,436 --> 00:09:03,266
我们正在用 Apple 神经网络引擎做此工作


220
00:09:03,796 --> 00:09:05,236
因此人物遮挡将在


221
00:09:05,236 --> 00:09:08,346
A12 处理器或者更高版本的设备上工作


222
00:09:08,346 --> 00:09:12,706
所以让我们来看看如何


223
00:09:12,706 --> 00:09:13,806
在 API 中打开它


224
00:09:14,716 --> 00:09:16,996
我们在 ARConfiguration 上


225
00:09:17,146 --> 00:09:20,106
有一个名为 FrameSemantics 的新性能


226
00:09:21,256 --> 00:09:23,086
这将为你提供


227
00:09:27,686 --> 00:09:29,416
你还可以使用


228
00:09:29,416 --> 00:09:31,336
ARConfiguration 上的其他方法


229
00:09:31,516 --> 00:09:33,616
检查特定设备或配置上


230
00:09:34,246 --> 00:09:37,176
是否有特定的语义可用


231
00:09:38,646 --> 00:09:40,306
尤其对于人物遮挡来说


232
00:09:40,426 --> 00:09:46,096
这有两种方法可用


233
00:09:46,316 --> 00:09:48,476
一种方法是人物分割


234
00:09:49,186 --> 00:09:51,096
这将帮助你只提供


235
00:09:51,356 --> 00:09:55,606
摄像头图像上呈现的人物分割


236
00:09:56,826 --> 00:09:58,096
如果你知道


237
00:09:58,096 --> 00:10:01,736
人们总是站在最前面 而你的虚拟内容


238
00:10:05,476 --> 00:10:07,386
例如 绿屏用例


239
00:10:07,386 --> 00:10:10,346
就是你现在不需要绿屏了


240
00:10:10,616 --> 00:10:15,316
另一种选择是具有深度的人物分割


241
00:10:16,016 --> 00:10:17,266
这将为你提供


242
00:10:17,406 --> 00:10:22,326
这些人与相机距离的额外深度估计


243
00:10:23,266 --> 00:10:24,866
如果人们可以在


244
00:10:24,866 --> 00:10:26,546
内容之后或之前


245
00:10:26,586 --> 00:10:29,236
与虚拟内容一起显示


246
00:10:29,236 --> 00:10:33,286
那么这就是最佳选择


247
00:10:33,386 --> 00:10:34,876
如果你使用 Metal


248
00:10:35,046 --> 00:10:37,506
或者高级用例进行自己的渲染


249
00:10:37,506 --> 00:10:39,616
你还可以使用


250
00:10:39,616 --> 00:10:43,576
ARFrame 上的分段和估算的深度数据


251
00:10:43,576 --> 00:10:46,516
直接访问像素缓冲区 


252
00:10:47,756 --> 00:10:50,446
现在 让我向你展示


253
00:10:56,516 --> 00:11:02,056
[掌声]


254
00:11:02,556 --> 00:11:04,356
所以在 Xcode 中 我有一个 


255
00:11:04,356 --> 00:11:07,656
使用新的 RealityKit API 的示例项目 


256
00:11:08,816 --> 00:11:11,456
让我快速带你了解一下它的作用


257
00:11:12,806 --> 00:11:15,296
因此在我们的 viewDidLoad 方法中


258
00:11:15,636 --> 00:11:17,666
我们正在创建一个 AnchorEntity


259
00:11:18,456 --> 00:11:19,926
用来查找水平面


260
00:11:20,486 --> 00:11:25,146
并将这个锚实体添加到场景中


261
00:11:25,146 --> 00:11:27,386
然后 我们检索一个


262
00:11:27,386 --> 00:11:30,816
模型的 URL 


263
00:11:31,186 --> 00:11:33,136
使用 ModelEntity 的异步模式


264
00:11:33,136 --> 00:11:34,536
加载 API 来载入它


265
00:11:34,536 --> 00:11:38,386
我们将实体添加为


266
00:11:38,386 --> 00:11:42,466
锚的子项 并且安装


267
00:11:42,466 --> 00:11:44,216
手势 以便我可以在


268
00:11:44,266 --> 00:11:46,736
平面上拖动对象


269
00:11:47,976 --> 00:11:49,996
因此由于 RealityKit  


270
00:11:49,996 --> 00:11:51,886
它所做的是自动


271
00:11:52,006 --> 00:11:53,126
设置世界追踪


272
00:11:53,126 --> 00:11:55,406
配置 因为我们知道


273
00:11:55,406 --> 00:11:58,536
我们需要使用世界追踪来进行平面估算


274
00:11:59,116 --> 00:12:00,916
然后 一旦检测到


275
00:12:04,636 --> 00:12:08,296
现在还没有使用人物遮挡


276
00:12:09,516 --> 00:12:12,106
但是我已经停下来打开它了


277
00:12:12,866 --> 00:12:14,986
这叫做 togglePeopleOcclusion 


278
00:12:14,986 --> 00:12:16,556
我想要执行的是一种方法


279
00:12:16,786 --> 00:12:18,326
当用户点击屏幕时


280
00:12:18,326 --> 00:12:21,136
它允许我打开和关闭人物遮挡


281
00:12:22,256 --> 00:12:24,376
现在让我们继续实施吧


282
00:12:25,046 --> 00:12:28,306
所以我要做的第一件事是


283
00:12:28,436 --> 00:12:31,756
检查我的世界追踪


284
00:12:31,756 --> 00:12:34,076
配置是否支持具有


285
00:12:34,656 --> 00:12:37,296
深度框架语义的人物分割


286
00:12:38,176 --> 00:12:39,406
建议你始终


287
00:12:39,406 --> 00:12:42,776
这样做 因为如果代码


288
00:12:42,776 --> 00:12:47,166
在没有 Apple 神经网络引擎的设备上运行


289
00:12:47,286 --> 00:12:48,426
且不支持此框架语义


290
00:12:48,426 --> 00:12:53,396
我们希望可以从容地处理此事


291
00:12:53,606 --> 00:12:56,066
因此如果是这样的话


292
00:12:56,066 --> 00:12:57,186
我们将向用户展示一条信息


293
00:12:57,186 --> 00:13:00,496
即人物遮挡在该设备上不可用


294
00:13:03,456 --> 00:13:06,826
让我们继续执行切换


295
00:13:08,056 --> 00:13:09,386
我将在这里


296
00:13:09,386 --> 00:13:11,436
对配置的 frameSemantics 属性


297
00:13:11,506 --> 00:13:13,276
执行 switch 语句


298
00:13:13,916 --> 00:13:16,586
如果框架语义中


299
00:13:16,586 --> 00:13:19,036
包含 personSegmentationWithDepth  


300
00:13:19,036 --> 00:13:22,116
我们将删除它并告诉用户


301
00:13:22,116 --> 00:13:23,786
人物遮挡已关闭


302
00:13:24,446 --> 00:13:29,436
我只需执行另一个案例


303
00:13:30,046 --> 00:13:30,986
如果你没有启用


304
00:13:30,986 --> 00:13:32,756
人物分割 那么我们


305
00:13:32,756 --> 00:13:35,416
将 frameSemantics 插入到


306
00:13:36,356 --> 00:13:38,096
不同的语义属性中


307
00:13:38,096 --> 00:13:39,696
并显示一条消息


308
00:13:39,786 --> 00:13:42,766
我们现在开启人物遮挡


309
00:13:43,346 --> 00:13:45,856
现在 我需要在会话中


310
00:13:45,886 --> 00:13:47,326
重新运行更新的配置


311
00:13:48,516 --> 00:13:51,086
因此让我从 ARView 中 


312
00:13:51,806 --> 00:13:55,136
检索会话 并使用


313
00:13:55,676 --> 00:13:58,826
我刚才更新的配置调用运行


314
00:14:00,036 --> 00:14:01,786
所以现在 我的


315
00:14:01,786 --> 00:14:03,936
togglePeopleOcclusion 方法已经完成 


316
00:14:03,936 --> 00:14:06,446
我现在需要确保


317
00:14:06,446 --> 00:14:08,116
用户点击屏幕时


318
00:14:08,776 --> 00:14:10,146
它实际上是被调用的


319
00:14:11,336 --> 00:14:12,696
我已经安装了一个轻点


320
00:14:12,696 --> 00:14:14,786
手势识别器 在我的


321
00:14:14,786 --> 00:14:20,736
onTap 方法中 我只调用 
togglePeopleOcclusion


322
00:14:21,396 --> 00:14:24,266
这就是我需要做的全部事情


323
00:14:25,376 --> 00:14:27,236
现在 让我继续


324
00:14:28,556 --> 00:14:33,086
构建代码并在我的设备上运行它


325
00:14:34,346 --> 00:14:36,876
我们已经看到


326
00:14:36,926 --> 00:14:40,066
平面被检测到 内容被放置


327
00:14:40,066 --> 00:14:41,856
我可以移动它


328
00:14:41,976 --> 00:14:44,226
由于我添加了手势


329
00:14:45,026 --> 00:14:46,886
你可以看到 RealityKit 已经


330
00:14:46,886 --> 00:14:48,966
增加了一个很好的接地阴影


331
00:14:49,726 --> 00:14:53,806
现在 让我们来看看人物遮挡


332
00:14:53,946 --> 00:14:55,176
现在 它仍然是关闭的


333
00:14:55,176 --> 00:14:57,366
所以如果我用手


334
00:14:57,486 --> 00:14:58,676
你会发现内容总是


335
00:14:58,676 --> 00:15:00,106
放在最上面的


336
00:15:00,426 --> 00:15:04,616
这是你从 ARKit 2 中所知道的行为


337
00:15:04,856 --> 00:15:07,736
现在 让我打开它 再把我的手拿回来


338
00:15:07,736 --> 00:15:10,156
现在你会看到


339
00:15:10,206 --> 00:15:12,866
[掌声] 虚拟物体实际上被覆盖了


340
00:15:14,516 --> 00:15:18,466
[掌声]


341
00:15:18,966 --> 00:15:22,236
这就是 ARKit 3 里的人体遮挡


342
00:15:27,786 --> 00:15:32,276
[掌声] 谢谢


343
00:15:32,276 --> 00:15:34,656
那么 让我们来谈一谈 ARKit 3 的


344
00:15:34,656 --> 00:15:38,616
另一个令人兴奋的功能 即动作捕捉


345
00:15:39,746 --> 00:15:42,736
通过动作捕捉


346
00:15:42,786 --> 00:15:45,056
你可以追踪人物的身体


347
00:15:45,056 --> 00:15:46,696
然后可以实时


348
00:15:46,696 --> 00:15:49,046
将其映射到虚拟角色


349
00:15:49,966 --> 00:15:51,156
现在 这只能通过


350
00:15:51,156 --> 00:15:53,776
外部设置和特殊装备来完成


351
00:15:54,466 --> 00:15:56,636
现在有了 ARKit 3 它只


352
00:15:56,706 --> 00:15:58,566
需要几行代码


353
00:15:58,566 --> 00:16:00,806
就可以在 iPad 和 iPhone 上正常工作


354
00:16:03,476 --> 00:16:06,086
现在 动作捕捉让你在


355
00:16:06,126 --> 00:16:10,026
2D 和 3D 中追踪人体


356
00:16:10,586 --> 00:16:12,976
它为你提供了


357
00:16:12,976 --> 00:16:15,676
该人的骨架表现


358
00:16:16,286 --> 00:16:21,256
例如 这可以驱动虚拟角色


359
00:16:22,036 --> 00:16:23,796
这是通过在


360
00:16:23,796 --> 00:16:24,856
Apple 神经网络引擎上运行的


361
00:16:24,856 --> 00:16:27,426
高级机器学习算法实现的


362
00:16:28,116 --> 00:16:29,586
因此它可以在


363
00:16:29,586 --> 00:16:31,496
A12 或更高版本处理器的设备上使用


364
00:16:32,656 --> 00:16:35,076
让我们先来看看 2D 人体检测


365
00:16:35,676 --> 00:16:39,286
怎么打开呢


366
00:16:40,336 --> 00:16:42,186
我们有一个名为


367
00:16:42,186 --> 00:16:43,866
bodyDetection 的新框架语义选项


368
00:16:44,746 --> 00:16:46,396
世界追踪配置


369
00:16:46,396 --> 00:16:48,666
以及图像和方向追踪配置


370
00:16:48,666 --> 00:16:51,416
也支持此功能


371
00:16:52,276 --> 00:16:53,506
所以你只需将它添加到


372
00:16:53,506 --> 00:16:56,036
你的框架语义中并在会话中调用运行


373
00:16:57,636 --> 00:16:58,486
现在 让我们来看看


374
00:16:58,486 --> 00:17:02,696
我们将要得到的数据


375
00:17:02,696 --> 00:17:05,116
如果检测到某人


376
00:17:05,165 --> 00:17:07,945
则每个 ARFrame 在 detectedBody 属性中


377
00:17:07,945 --> 00:17:12,506
提供 ARBody2D 类型的对象


378
00:17:13,376 --> 00:17:15,996
该对象包含 2D 骨架


379
00:17:17,455 --> 00:17:19,106
骨架为你提供


380
00:17:19,106 --> 00:17:21,016
标准化图像空间中的


381
00:17:21,076 --> 00:17:22,896
所有关节标志


382
00:17:23,876 --> 00:17:24,896
它们以数组的


383
00:17:24,896 --> 00:17:26,896
平面层次结构返回


384
00:17:27,026 --> 00:17:28,246
因为这是最有效率的处理方法


385
00:17:29,846 --> 00:17:31,386
但是你将获得一个骨架定义


386
00:17:33,406 --> 00:17:34,706
由于骨架定义


387
00:17:34,706 --> 00:17:36,416
你拥有如何解释


388
00:17:36,416 --> 00:17:39,656
骨架数据的所有信息


389
00:17:40,446 --> 00:17:43,356
特别是 它包含有关关节层次结构的


390
00:17:43,356 --> 00:17:45,426
信息 比如 实际上


391
00:17:45,676 --> 00:17:48,386
手关节是肘关节的子关节


392
00:17:49,786 --> 00:17:51,176
并且还为你提供关节


393
00:17:51,266 --> 00:17:55,266
的名称 然后来方便访问


394
00:17:55,936 --> 00:17:58,256
所以让我们来看看这个是什么样子的


395
00:17:59,056 --> 00:18:01,006
这是我们在框架中检测到的人


396
00:18:01,926 --> 00:18:03,526
这就是 ARKit 提供的


397
00:18:03,526 --> 00:18:05,426
2D 骨架


398
00:18:06,016 --> 00:18:10,056
如前所述 重要关节的命名是为了


399
00:18:10,056 --> 00:18:11,986
便于你找到


400
00:18:11,986 --> 00:18:13,036
你感兴趣的


401
00:18:13,036 --> 00:18:17,446
特定关节的位置 比如头部和右手


402
00:18:18,636 --> 00:18:19,996
所以这是 2D


403
00:18:20,626 --> 00:18:24,166
现在 让我们来看看 3D 运动捕捉


404
00:18:25,216 --> 00:18:26,766
3D 动作捕捉让你


405
00:18:26,766 --> 00:18:29,286
在 3D 空间中追踪人体


406
00:18:29,936 --> 00:18:31,046
并为你提供


407
00:18:34,656 --> 00:18:36,496
它还为你提供比例估计


408
00:18:36,496 --> 00:18:38,516
以便你确定


409
00:18:38,516 --> 00:18:40,696
正在跟踪的人的大小


410
00:18:41,746 --> 00:18:45,256
3D 骨架定位在世界坐标系中


411
00:18:46,516 --> 00:18:48,496
让我们看看在 API 中如何使用这个


412
00:18:52,006 --> 00:18:53,276
我们正在引入一种


413
00:18:53,336 --> 00:18:56,806
名为 ARBodyTrackingConfiguration 的新配置


414
00:18:58,426 --> 00:19:00,346
这可以让你使用 3D 身体


415
00:19:00,346 --> 00:19:02,856
追踪 但它也提供了


416
00:19:03,186 --> 00:19:06,476
我们之前看到的 2D 身体检测


417
00:19:06,916 --> 00:19:09,346
因此在该配置中


418
00:19:09,346 --> 00:19:11,486
框架语义是默认打开的


419
00:19:12,686 --> 00:19:14,626
因此 该配置


420
00:19:14,686 --> 00:19:16,236
还追踪设备的


421
00:19:16,296 --> 00:19:18,666
位置和方向并


422
00:19:18,666 --> 00:19:20,166
提供选定的世界追踪功能


423
00:19:20,166 --> 00:19:23,926
如平面估算或图像检测


424
00:19:24,746 --> 00:19:26,756
因此有了它 你就可以


425
00:19:26,756 --> 00:19:28,666
在你的 AR App 中


426
00:19:29,316 --> 00:19:32,086
使用身体追踪来做更多的事情


427
00:19:33,516 --> 00:19:35,716
若要对其进行设置 你


428
00:19:35,716 --> 00:19:37,426
只需创建身体追踪


429
00:19:37,426 --> 00:19:40,256
配置并在会话中运行它


430
00:19:41,396 --> 00:19:43,096
注意 我们还有 API 来


431
00:19:43,146 --> 00:19:45,076
检查当前设备


432
00:19:45,076 --> 00:19:49,136
是否支持该配置


433
00:19:49,306 --> 00:19:51,746
所以 现在 当 ARKit 正在运行


434
00:19:52,076 --> 00:19:54,026
并检测到一个人时 它将


435
00:19:54,026 --> 00:19:58,026
添加一种新类型的锚 即 ARBodyAnchor 


436
00:20:00,096 --> 00:20:02,366
这将在调用锚


437
00:20:02,366 --> 00:20:03,546
的会话中提供给你


438
00:20:03,546 --> 00:20:05,186
就像你知道的


439
00:20:05,186 --> 00:20:06,436
其他锚类型一样


440
00:20:07,786 --> 00:20:09,526
和其他锚一样


441
00:20:09,526 --> 00:20:11,796
它也有一个转换


442
00:20:11,796 --> 00:20:13,066
为你提供被检测者


443
00:20:13,066 --> 00:20:15,136
在世界坐标中的


444
00:20:15,206 --> 00:20:17,306
位置和方向


445
00:20:17,856 --> 00:20:19,576
此外 你将获得


446
00:20:19,686 --> 00:20:22,296
比例系数和 3D
 
00:20:22,676 --> 00:20:23,826
骨架的参考


447
00:20:24,446 --> 00:20:29,176
让我们来看看它是怎么样的


448
00:20:29,846 --> 00:20:31,056
你可以看到它比


449
00:20:31,056 --> 00:20:33,406
2D 骨架更加详细


450
00:20:34,436 --> 00:20:35,626
因此黄色关节是


451
00:20:35,626 --> 00:20:37,036
将通过运动捕捉数据


452
00:20:37,036 --> 00:20:39,316
传递给用户的关节


453
00:20:40,396 --> 00:20:42,516
白色的是我们在骨骼中


454
00:20:42,876 --> 00:20:45,046
额外提供的叶关节


455
00:20:46,096 --> 00:20:47,936
这些没有被有效追踪


456
00:20:48,246 --> 00:20:49,806
因此变换对于


457
00:20:49,806 --> 00:20:51,156
被追踪的父项是静态的


458
00:20:51,846 --> 00:20:53,486
但是 当然 你可以直接


459
00:20:53,486 --> 00:20:55,666
访问每个关节并


460
00:20:56,006 --> 00:20:58,766
检索它们的道路坐标


461
00:21:00,016 --> 00:21:01,606
同样 我们有一个


462
00:21:01,606 --> 00:21:04,056
很重要的标签 一个


463
00:21:04,326 --> 00:21:06,066
按名称查询它们的 API 这样


464
00:21:06,066 --> 00:21:08,826
你就可以很容易地找到


465
00:21:08,826 --> 00:21:11,456
你感兴趣的特定关节


466
00:21:12,016 --> 00:21:13,686
现在 我确信你可以


467
00:21:13,686 --> 00:21:15,486
为这个新的 API 提出


468
00:21:15,746 --> 00:21:18,376
很多很好的用例 但是我想


469
00:21:18,376 --> 00:21:20,026
谈谈一个可能对于


470
00:21:20,056 --> 00:21:21,866
你们很多人来说


471
00:21:21,866 --> 00:21:24,256
都很有趣的特殊用例 它就是


472
00:21:24,346 --> 00:21:26,036
动画 3D 角色


473
00:21:27,696 --> 00:21:30,256
通过将 ARKit 和 RealityKit 


474
00:21:30,256 --> 00:21:32,736
结合使用 你可以驱动一个 


475
00:21:32,736 --> 00:21:34,986
基于三维骨架姿势的模型


476
00:21:34,986 --> 00:21:36,836
这真的非常简单


477
00:21:37,936 --> 00:21:41,486
你所需要的只是一个装配好的网格


478
00:21:42,586 --> 00:21:43,916
你可以在我们的示例 App 中


479
00:21:43,916 --> 00:21:45,506
找到一个这样的例子


480
00:21:45,506 --> 00:21:46,646
你可以在会话主页上


481
00:21:46,646 --> 00:21:48,606
下载 当然 你也可以


482
00:21:48,606 --> 00:21:50,586
在自己选择的内容创建工具中


483
00:21:50,586 --> 00:21:52,296
自由创建自己的 App


484
00:21:52,826 --> 00:21:56,576
让我们看看在代码中实现这一点有多容易


485
00:21:56,856 --> 00:21:58,756
它内置于 RealityKit API 中


486
00:21:59,736 --> 00:22:01,036
我们将使用的


487
00:22:01,036 --> 00:22:03,686
主要类是 BodyTrackedEntity 


488
00:22:09,486 --> 00:22:12,056
你要做的第一件事是


489
00:22:12,056 --> 00:22:14,676
创建一个 AnchorEntity 的


490
00:22:14,736 --> 00:22:17,376
类型主体 并将此锚添加到场景中 


491
00:22:18,786 --> 00:22:21,736
接下来 你将加载模型


492
00:22:22,036 --> 00:22:23,496
在我们的例子中 它被称为机器人


493
00:22:24,606 --> 00:22:26,136
为此 我们使用异步


494
00:22:26,136 --> 00:22:27,386
加载 API   


495
00:22:27,826 --> 00:22:29,076
在完成处理器中


496
00:22:29,516 --> 00:22:32,806
你将获得 BodyTrackedEntity 我们现在


497
00:22:32,866 --> 00:22:35,906
只需将其添加为 bodyAnchor 的子项


498
00:22:39,666 --> 00:22:42,516
因此只要 ARKit 现在将


499
00:22:42,516 --> 00:22:44,536
AR 主体锚点添加到会话中


500
00:22:45,846 --> 00:22:48,176
骨架的 3D 姿势就会


501
00:22:48,176 --> 00:22:50,386
自动实时应用于


502
00:22:50,386 --> 00:22:52,276
虚拟模型


503
00:22:53,466 --> 00:22:54,916
这就是使用 ARKit 3 进行


504
00:22:54,916 --> 00:22:56,896
动作捕捉的简单方法


505
00:22:58,316 --> 00:23:03,936
[掌声] 谢谢


506
00:23:03,936 --> 00:23:06,436
那么 现在让我们谈谈


507
00:23:06,436 --> 00:23:10,526
同步的前后摄像头


508
00:23:10,526 --> 00:23:12,166
ARKit 让你使用后置摄像头来                                      


509
00:23:12,166 --> 00:23:14,316
进行世界追踪 


510
00:23:14,316 --> 00:23:16,056
并且前面使用 2Depth


511
00:23:16,056 --> 00:23:17,326
摄像头系统来进行人脸追踪


512
00:23:18,176 --> 00:23:20,116
现在 你非常需要的一个


513
00:23:20,116 --> 00:23:22,386
功能是能够


514
00:23:22,386 --> 00:23:24,896
把前置和后置摄像头的


515
00:23:24,896 --> 00:23:26,456
用户体验结合到一起


516
00:23:27,316 --> 00:23:30,666
现在 有了 ARKit 3 你现在可以做这个了


517
00:23:31,956 --> 00:23:33,426
因此有了这个新功能 你


518
00:23:33,426 --> 00:23:35,596
可以同时使用两个摄像头


519
00:23:35,676 --> 00:23:37,696
来构建 AR 体验


520
00:23:37,996 --> 00:23:39,336
这就意味着 


521
00:23:39,866 --> 00:23:41,666
你现在可以构建两种新


522
00:23:41,716 --> 00:23:42,866
类型的用例


523
00:23:43,896 --> 00:23:47,106
首先 你可以创建世界追踪体验


524
00:23:47,106 --> 00:23:48,816
因此 使用后置摄像头


525
00:23:48,886 --> 00:23:51,156
还可以从前置摄像头


526
00:23:51,156 --> 00:23:54,096
捕获的面部数据中受益


527
00:23:55,106 --> 00:23:56,746
你可以创建面部追踪


528
00:23:56,746 --> 00:23:59,086
体验 利用


529
00:23:59,216 --> 00:24:00,826
全方位设备定位和


530
00:24:00,826 --> 00:24:02,956
6 度自由度的位置


531
00:24:03,406 --> 00:24:06,896
所有这些都支持 A12


532
00:24:06,896 --> 00:24:08,546
设备和更高版本


533
00:24:08,806 --> 00:24:10,956
让我们来看一个例子


534
00:24:11,426 --> 00:24:12,586
在这里 我们使用


535
00:24:12,586 --> 00:24:14,196
平面估算来进行世界追踪


536
00:24:14,806 --> 00:24:16,836
但是我们也在平面顶部


537
00:24:16,836 --> 00:24:18,656
放置了一个人脸网格


538
00:24:18,656 --> 00:24:20,586
并通过前置摄像头


539
00:24:20,636 --> 00:24:23,186
捕捉到的面部表情


540
00:24:23,186 --> 00:24:24,216
实时更新它


541
00:24:24,216 --> 00:24:27,436
所以让我们看看如何


542
00:24:27,516 --> 00:24:31,336
在 API 中使用并存的前后摄像头


543
00:24:31,546 --> 00:24:34,066
首先 让我们创建一个世界追踪配置


544
00:24:34,416 --> 00:24:36,416
现在 我选择的配置


545
00:24:36,506 --> 00:24:38,706
决定了屏幕上实际显示的


546
00:24:38,706 --> 00:24:40,926
是哪个摄像头流


547
00:24:41,206 --> 00:24:44,256
因此在这种情况下 是后置摄像头


548
00:24:45,666 --> 00:24:47,756
现在我打开新的


549
00:24:47,916 --> 00:24:50,386
userFaceTrackingEnabled 属性


550
00:24:50,586 --> 00:24:52,046
并运行会话


551
00:24:52,366 --> 00:24:58,156
这将导致我收到面部锚点


552
00:24:58,276 --> 00:25:01,326
然后我可以使用


553
00:25:01,326 --> 00:25:02,876
来自锚点的任何信息


554
00:25:03,066 --> 00:25:05,496
如面部网格 地形


555
00:25:05,926 --> 00:25:08,666
或者锚点自身的变换


556
00:25:09,236 --> 00:25:11,496
现在 请注意 由于我们


557
00:25:11,666 --> 00:25:13,646
在这处理世界坐标


558
00:25:13,646 --> 00:25:15,256
用户面部传输将被


559
00:25:15,256 --> 00:25:17,646
放置在摄像机后面


560
00:25:17,646 --> 00:25:19,196
这意味着为了能看见面部


561
00:25:19,256 --> 00:25:20,486
你需要


562
00:25:20,616 --> 00:25:22,156
将其转换到


563
00:25:22,156 --> 00:25:26,366
相机前面的某个位置


564
00:25:26,366 --> 00:25:29,486
现在 让我们看看人脸追踪配置


565
00:25:30,366 --> 00:25:31,746
你可以像往常一样


566
00:25:31,746 --> 00:25:33,646
创建面部追踪配置


567
00:25:33,686 --> 00:25:37,496
并将 worldTrackingEnabled 设置成 true


568
00:25:38,226 --> 00:25:42,876
然后 在运行配置后 你可以


569
00:25:42,876 --> 00:25:45,216
在每个帧中访问 例如


570
00:25:45,216 --> 00:25:46,696
在更新帧的会话中


571
00:25:46,856 --> 00:25:50,386
回调当前


572
00:25:50,386 --> 00:25:51,586
相机位置的转换


573
00:25:52,216 --> 00:25:54,506
然后你就可以把它用于


574
00:25:54,616 --> 00:25:56,116
任何你想到的用例


575
00:25:56,326 --> 00:25:58,626
这就是 ARKit 3 里面


576
00:25:58,626 --> 00:26:00,076
同步的前后摄像头


577
00:26:01,036 --> 00:26:02,376
我们认为你能够


578
00:26:02,526 --> 00:26:04,516
使用这个新的 API 


579
00:26:04,516 --> 00:26:06,126
来做出许多出色的用例


580
00:26:07,281 --> 00:26:09,281
[掌声]


581
00:26:09,546 --> 00:26:12,421
谢谢 [掌声]


582
00:26:12,826 --> 00:26:14,446
现在 让我把它交给


583
00:26:14,446 --> 00:26:16,506
Thomas 他将会告诉你所有 


584
00:26:16,506 --> 00:26:17,816
关于协作会议的内容


585
00:26:20,516 --> 00:26:22,636
[掌声]


586
00:26:23,136 --> 00:26:23,426
>> 谢谢


587
00:26:24,366 --> 00:26:25,056
谢谢你 Andreas 


588
00:26:25,416 --> 00:26:26,386
大家下午好


589
00:26:26,736 --> 00:26:28,876
我的名字是 Thomas 来自 ARKit 团队


590
00:26:29,506 --> 00:26:31,686
让我们谈谈协作会议


591
00:26:32,216 --> 00:26:35,276
在 ARKit 2 中 你可以创建


592
00:26:35,666 --> 00:26:37,786
具有保存和加载世界地图


593
00:26:37,786 --> 00:26:40,086
功能的多用户体验


594
00:26:40,726 --> 00:26:42,046
你必须在一台设备上保存地图


595
00:26:42,046 --> 00:26:43,686
并将其发送到另一台


596
00:26:43,686 --> 00:26:45,756
设备 以便你的用户再次


597
00:26:45,756 --> 00:26:47,276
获得相同的体验


598
00:26:48,586 --> 00:26:50,556
这是一个单一的地图 一次性的


599
00:26:50,556 --> 00:26:52,026
地图共享体验 在这


600
00:26:52,026 --> 00:26:54,306
之后 大多数用户


601
00:26:54,306 --> 00:26:55,186
将不会相同


602
00:26:55,296 --> 00:26:57,506
不再共享相同的信息


603
00:26:58,006 --> 00:27:02,816
好吧 通过在 ARKit 3 上的协作会话 我们现在


604
00:27:02,816 --> 00:27:04,416
可以在整个网络上


605
00:27:04,416 --> 00:27:05,886
持续共享你的映射信息


606
00:27:07,226 --> 00:27:09,016
这可以使你创建临时的


607
00:27:09,016 --> 00:27:11,366
多用户体验 你的


608
00:27:11,366 --> 00:27:14,686
用户可以更轻松地访问同一会话


609
00:27:15,896 --> 00:27:18,076
此外 我们还允许你


610
00:27:18,826 --> 00:27:20,626
在所有设备上


611
00:27:20,626 --> 00:27:22,556
分享或者我们实际共享 ARAnchors 


612
00:27:23,186 --> 00:27:24,226
所有这些锚点


613
00:27:24,226 --> 00:27:26,016
都可以通过锚点的会话 ID 


614
00:27:26,016 --> 00:27:28,216
识别出来 


615
00:27:29,536 --> 00:27:31,386
注意 在这一点上 大多数


616
00:27:31,386 --> 00:27:32,846
所有的坐标系都是


617
00:27:32,846 --> 00:27:34,016
相互独立的


618
00:27:34,016 --> 00:27:35,116
即使我们仍然在幕后


619
00:27:35,116 --> 00:27:37,206
共享信息


620
00:27:37,406 --> 00:27:38,976
让我给你展示一下这是如何运作的


621
00:27:41,566 --> 00:27:44,956
所以在这个视频中 我们可以看到两个用户


622
00:27:45,016 --> 00:27:46,246
注意颜色


623
00:27:46,246 --> 00:27:47,656
一个用户将以绿色


624
00:27:47,656 --> 00:27:49,826
显示功能点 另一个用户


625
00:27:49,826 --> 00:27:52,896
将以红色显示功能点


626
00:27:53,086 --> 00:27:53,976
当他们在环境中移动时


627
00:27:53,976 --> 00:27:56,976
他们开始


628
00:27:56,976 --> 00:28:00,986
映射环境并添加更多的特征点


629
00:28:01,976 --> 00:28:05,316
此时 这是他们内部地图的


630
00:28:05,316 --> 00:28:07,186
内部展示 他们不知道


631
00:28:07,186 --> 00:28:08,246
彼此的地图


632
00:28:08,506 --> 00:28:14,576
当他们四处移动时 会收集更多的特征点


633
00:28:17,796 --> 00:28:19,026
当他们在场景中


634
00:28:19,026 --> 00:28:20,776
收集到更多的特征点 你可以


635
00:28:20,776 --> 00:28:21,876
看到内部地图 


636
00:28:21,876 --> 00:28:23,426
注意颜色和它们的


637
00:28:23,426 --> 00:28:25,416
最终匹配点 然后这些


638
00:28:25,416 --> 00:28:28,806
内部地图将合并到一起


639
00:28:28,806 --> 00:28:30,716
只形成一个地图 这意味着


640
00:28:30,716 --> 00:28:32,366
每个用户现在都


641
00:28:32,366 --> 00:28:36,056
了解彼此 也了解场景理解


642
00:28:36,056 --> 00:28:40,206
当他们四处移动时


643
00:28:40,206 --> 00:28:41,346
会提供更多信息


644
00:28:42,606 --> 00:28:43,786
他们又持续在


645
00:28:43,786 --> 00:28:45,106
幕后共享信息


646
00:28:46,316 --> 00:28:48,766
此外 ARKit 3 还为你提供了


647
00:28:48,766 --> 00:28:51,016
类似 AR 的参与者锚点


648
00:28:51,016 --> 00:28:54,786
使你可以了解其他用户


649
00:28:54,786 --> 00:28:56,386
在你的环境中的实时位置


650
00:28:57,226 --> 00:28:59,106
如果你想展示


651
00:28:59,106 --> 00:29:00,646
一个图标或者其他代表该用户的东西


652
00:29:00,646 --> 00:29:02,066
这会非常方便


653
00:29:02,616 --> 00:29:07,436
如前所述 ARKit 3


654
00:29:07,436 --> 00:29:09,076
也会在幕后共享 ARAnchors 


655
00:29:09,076 --> 00:29:11,656
这意味着如果你在


656
00:29:11,656 --> 00:29:13,406
一台设备上共享或添加一个锚点


657
00:29:13,406 --> 00:29:15,696
它将自动显示在另一台设备上


658
00:29:17,106 --> 00:29:18,846
让我们看看它在代码中是如何工作的


659
00:29:20,576 --> 00:29:22,066
正如 Andreas 之前提到的


660
00:29:22,096 --> 00:29:24,406
ARKit 和 RealityKit 完全融为一体


661
00:29:25,386 --> 00:29:26,876
如果你想启用


662
00:29:26,876 --> 00:29:27,916
与 RealityKit 的协作会话


663
00:29:27,916 --> 00:29:29,366
这将会非常简单


664
00:29:30,276 --> 00:29:33,126
你首先需要设置你的
Multipeer Connectivity 会话


665
00:29:33,506 --> 00:29:36,326
Multipeer Connectivity 框架是


666
00:29:36,326 --> 00:29:37,346
一个 Apple 框架


667
00:29:37,346 --> 00:29:38,676
允许你进行发现和


668
00:29:38,676 --> 00:29:40,196
点对点连接


669
00:29:40,196 --> 00:29:43,126
然后 你需要将这个


670
00:29:43,126 --> 00:29:45,116
多点连接会话传递


671
00:29:45,116 --> 00:29:48,826
给 AR 场景视图同步服务


672
00:29:49,316 --> 00:29:53,806
最后 正如每个 ARKit


673
00:29:53,806 --> 00:29:55,456
体验一样 你必须设置


674
00:29:55,506 --> 00:29:57,286
你的 ARWorldTrackingConfiguration 


675
00:29:57,436 --> 00:29:59,576
将 isCollaborationEnabled


676
00:29:59,576 --> 00:30:01,566
标志设置为 true 并在


677
00:30:01,566 --> 00:30:02,706
会话中运行该配置


678
00:30:03,236 --> 00:30:08,196
就是这样 接下来会发生什么


679
00:30:09,416 --> 00:30:11,046
因此 ARKit 将


680
00:30:11,046 --> 00:30:13,286
isCollaborationEnabled 标志 


681
00:30:13,796 --> 00:30:16,336
设置成 true 时 基本上将


682
00:30:16,336 --> 00:30:17,466
并且在会话上运行该配置


683
00:30:17,466 --> 00:30:18,836
基本上是将在


684
00:30:18,836 --> 00:30:22,506
ARSessionDelegate 上创建


685
00:30:22,506 --> 00:30:24,006
一个新方法 


686
00:30:24,006 --> 00:30:24,936
以便你传输该数据


687
00:30:25,576 --> 00:30:27,016
在 RealityKit 用例中


688
00:30:27,016 --> 00:30:28,686
我们将处理它 但是如果


689
00:30:28,686 --> 00:30:29,906
你在另一个渲染器中使用 ARKit 


690
00:30:29,906 --> 00:30:31,646
那么我们将


691
00:30:31,646 --> 00:30:33,596
你将必须通过网络发送该数据


692
00:30:35,576 --> 00:30:37,756
此数据成为 AR 协作数据


693
00:30:39,006 --> 00:30:41,036
ARKit 可以在任何时间点


694
00:30:41,036 --> 00:30:43,226
创建 AR 协作数据包


695
00:30:43,226 --> 00:30:45,106
然后你必须再次


696
00:30:45,106 --> 00:30:47,526
转发给其他用户


697
00:30:47,886 --> 00:30:49,886
这不仅限于两个用户


698
00:30:49,886 --> 00:30:51,916
你可以在该会话中


699
00:30:51,916 --> 00:30:53,456
拥有大量用户


700
00:30:54,676 --> 00:30:57,676
在此过程中 ARKit 将


701
00:30:57,676 --> 00:30:59,196
生成额外的 AR  


702
00:30:59,196 --> 00:31:00,746
协作数据 你必须


703
00:31:00,746 --> 00:31:02,776
将这些数据转发到其他设备


704
00:31:02,776 --> 00:31:03,816
并广播这些数据


705
00:31:07,876 --> 00:31:10,006
让我们看看这在代码中是如何工作的


706
00:31:11,296 --> 00:31:13,236
因此 在本例中


707
00:31:13,236 --> 00:31:14,736
你首先需要设置你的多点连接


708
00:31:14,736 --> 00:31:17,236
或者你也可以设置


709
00:31:17,236 --> 00:31:19,716
任何框架 你选择的任何网络框架


710
00:31:19,716 --> 00:31:20,856
并确保你的设备


711
00:31:20,856 --> 00:31:21,866
共享相同的会话


712
00:31:22,336 --> 00:31:25,676
当它们执行时 你需要


713
00:31:25,676 --> 00:31:27,826
启用 ARWorldTrackingConfiguration


714
00:31:27,826 --> 00:31:30,396
并将 isCollaborationEnabled
标志设置为 true


715
00:31:31,706 --> 00:31:33,466
在这种情况下 你需要 


716
00:31:36,296 --> 00:31:40,196
此时 在代理下将为你提供一个新方法


717
00:31:40,196 --> 00:31:41,806
你将在其中


718
00:31:41,806 --> 00:31:44,296
接收一些协作数据


719
00:31:44,776 --> 00:31:49,066
收到这些数据后


720
00:31:49,066 --> 00:31:50,296
你需要确保将其在


721
00:31:50,296 --> 00:31:52,406
网络上广播给同时


722
00:31:52,726 --> 00:31:54,496
处于协作会话中的其他用户


723
00:31:54,976 --> 00:31:58,376
在其他设备上


724
00:31:58,826 --> 00:32:00,516
接收这些数据后 你需要


725
00:32:00,516 --> 00:32:02,936
更新 URL 会话 以便它


726
00:32:02,936 --> 00:32:04,016
了解这些新数据


727
00:32:04,406 --> 00:32:05,566
就是这样的


728
00:32:07,436 --> 00:32:09,266
此协作数据


729
00:32:10,276 --> 00:32:12,456
自动交换所有


730
00:32:12,456 --> 00:32:14,226
用户创建的 ARAnchors


731
00:32:15,566 --> 00:32:18,126
每个锚点都可以


732
00:32:18,126 --> 00:32:20,506
通过会话 ID 识别 这样你就可以


733
00:32:20,506 --> 00:32:21,706
确保了解


734
00:32:21,706 --> 00:32:25,316
锚点来自哪个设备或者哪个 AR 会话


735
00:32:25,836 --> 00:32:28,426
如前所述


736
00:32:28,426 --> 00:32:30,386
ARParticipantAnchor 实时地


737
00:32:30,386 --> 00:32:32,626
展示参与者的


738
00:32:32,626 --> 00:32:34,586
位置 这在你的某些用例中


739
00:32:34,586 --> 00:32:35,706
非常方便


740
00:32:39,296 --> 00:32:48,546
这就是你创建协作会话的方式 [掌声]


741
00:32:49,046 --> 00:32:50,906
现在让我们来谈谈指导


742
00:32:51,636 --> 00:32:53,316
当你创建一种体验


743
00:32:53,556 --> 00:32:56,436
一种 AR 体验 指导非常重要 


744
00:32:57,056 --> 00:32:58,056
你真的想引导你的


745
00:32:58,056 --> 00:33:00,336
用户 不管他们是新用户


746
00:33:00,336 --> 00:33:03,176
还是返回到你的 AR 体验中的用户


747
00:33:04,036 --> 00:33:05,256
这不是一个简单的过程


748
00:33:05,656 --> 00:33:07,216
有时候 对于你来说


749
00:33:07,216 --> 00:33:09,256
理解甚至指导用户获得新体验


750
00:33:09,256 --> 00:33:11,766
是很难的


751
00:33:13,136 --> 00:33:14,626
在整个过程中


752
00:33:14,626 --> 00:33:16,656
你必须对某些追踪事件做出反应


753
00:33:16,846 --> 00:33:19,216
有时追踪会受限 因为用户移动得


754
00:33:19,216 --> 00:33:21,836
过于快速


755
00:33:21,836 --> 00:33:23,266
到目前为止 我们一直在为你


756
00:33:23,266 --> 00:33:25,336
提供 Human Interface Guideline


757
00:33:25,976 --> 00:33:28,276
这允许你为用户引导体验


758
00:33:28,276 --> 00:33:30,166
提供一些指导


759
00:33:31,736 --> 00:33:33,346
今年 我们将它嵌入到


760
00:33:33,346 --> 00:33:34,836
UI 视图中


761
00:33:38,206 --> 00:33:39,906
我们将它称为 AR Coaching View


762
00:33:40,406 --> 00:33:43,746
这是一个内置的叠加层


763
00:33:43,746 --> 00:33:46,106
你可以直接嵌入到 AR App 中


764
00:33:46,746 --> 00:33:48,386
它可以引导你的用户


765
00:33:48,386 --> 00:33:49,646
获得非常好的追踪体验


766
00:33:50,176 --> 00:33:53,146
它为你的 App 提供一个


767
00:33:53,676 --> 00:33:55,096
连续的设计


768
00:33:55,096 --> 00:33:57,846
便于你的用户能够非常熟悉它


769
00:33:58,496 --> 00:34:00,726
实际上你可能之前已经看过这种设计


770
00:34:00,946 --> 00:34:03,496
我们有 AR 快速查看和测量


771
00:34:05,296 --> 00:34:07,906
这个新的 UI 叠加层


772
00:34:07,906 --> 00:34:09,906
会根据不同的追踪事件


773
00:34:09,906 --> 00:34:12,246
自动激活和停用


774
00:34:13,036 --> 00:34:15,576
而且你可以调整某些指导的目标


775
00:34:16,255 --> 00:34:17,686
让我们来看看其中的一些叠加层


776
00:34:17,686 --> 00:34:21,235
所以在 AR Coaching View 中 我们


777
00:34:21,985 --> 00:34:23,206
有多个叠加层


778
00:34:24,036 --> 00:34:25,846
用户引导 UI 使


779
00:34:25,846 --> 00:34:28,065
用户能够理解


780
00:34:28,065 --> 00:34:29,235
你正在寻找的内容 在


781
00:34:29,235 --> 00:34:30,246
本例中 是表面


782
00:34:30,835 --> 00:34:32,106
大多数情况下 你的体验


783
00:34:32,106 --> 00:34:34,235
需要表面来放置内容


784
00:34:34,235 --> 00:34:36,556
因此如果在你的配置中


785
00:34:36,556 --> 00:34:37,956
启用平面检测 则会


786
00:34:37,956 --> 00:34:39,946
自动显示此叠加层


787
00:34:41,335 --> 00:34:42,846
其次 我们有另外一个


788
00:34:42,846 --> 00:34:46,936
叠加层 使用户能够理解


789
00:34:46,936 --> 00:34:48,005
他们必须更多地移动


790
00:34:48,005 --> 00:34:50,746
以收集其他功能


791
00:34:50,746 --> 00:34:52,565
以便更好地进行追踪


792
00:34:53,216 --> 00:34:55,426
最后 我们还有


793
00:34:55,426 --> 00:34:57,536
另外一个叠加层 来帮助你的


794
00:34:57,536 --> 00:34:59,156
用户在某些特定环境下


795
00:34:59,156 --> 00:35:00,766
重新定位 以防你跟丢


796
00:35:00,766 --> 00:35:04,446
例如 如果 App 进入后台


797
00:35:05,066 --> 00:35:08,306
让我们来看一个例子


798
00:35:11,346 --> 00:35:12,876
因此 在这个例子中


799
00:35:12,876 --> 00:35:14,646
我们要求用户移动设备


800
00:35:14,646 --> 00:35:16,466
来找到一个新的平面


801
00:35:16,466 --> 00:35:17,886
一旦用户移动


802
00:35:17,886 --> 00:35:19,566
并收集更多的


803
00:35:19,566 --> 00:35:21,636
功能 那么内容就会


804
00:35:21,706 --> 00:35:24,116
被放置 视图就会自动关闭


805
00:35:24,596 --> 00:35:26,086
所以你不用做任何事情


806
00:35:26,086 --> 00:35:32,966
一切都是自动处理 [掌声]


807
00:35:33,466 --> 00:35:36,646
让我们来看看如何设置


808
00:35:37,376 --> 00:35:38,886
同样这很容易


809
00:35:39,636 --> 00:35:41,686
因为它是一个简单的 UI 视图


810
00:35:41,686 --> 00:35:43,306
所以你必须将其设置为另一个


811
00:35:43,346 --> 00:35:44,506
UI 视图的子视图


812
00:35:44,766 --> 00:35:46,386
理想情况下 你将其设置为


813
00:35:46,386 --> 00:35:47,826
AR 视图的子级


814
00:35:48,436 --> 00:35:50,676
然后 你需要将此会话


815
00:35:50,676 --> 00:35:53,036
连接到指导视图


816
00:35:53,036 --> 00:35:54,076
以便指导视图知道要对


817
00:35:54,076 --> 00:35:57,386
哪些事情做出反应


818
00:35:57,386 --> 00:35:58,296
或者如果你正在使用


819
00:35:58,296 --> 00:36:00,866
故事板 则需要将教指导


820
00:36:00,866 --> 00:36:02,016
视图的会话提供程序


821
00:36:02,016 --> 00:36:04,326
出口连接到


822
00:36:04,326 --> 00:36:07,556
会话提供程序本身


823
00:36:07,626 --> 00:36:09,416
或者 如果你想


824
00:36:09,416 --> 00:36:10,556
对视图提供给你的某些事件做出反应


825
00:36:10,556 --> 00:36:13,156
可以设置一组委托


826
00:36:13,686 --> 00:36:18,146
最后 如果你想


827
00:36:18,146 --> 00:36:19,706
禁用某些功能


828
00:36:19,706 --> 00:36:21,606
还可以提供一组


829
00:36:21,606 --> 00:36:23,166
特定的指导目标


830
00:36:23,686 --> 00:36:26,976
让我们看看其中的一些委托


831
00:36:30,296 --> 00:36:32,666
因此我们在 AR Coaching View 方面 


832
00:36:32,666 --> 00:36:35,346
有三种新方法 CoachingOverlayViewDelegate


833
00:36:35,826 --> 00:36:37,146
其中两个可以对


834
00:36:37,146 --> 00:36:38,516
激活和停用做出反应


835
00:36:38,516 --> 00:36:40,476
因此你可以选择


836
00:36:40,476 --> 00:36:41,636
在整个体验过程中


837
00:36:41,636 --> 00:36:43,016
是否仍需要启用该功能


838
00:36:43,016 --> 00:36:45,376
或者你认为 例如 如果该用户曾经


839
00:36:45,466 --> 00:36:47,696
用过此功能 则不需要再了解此功能


840
00:36:49,106 --> 00:36:50,916
此外 你可以自动设置


841
00:36:50,916 --> 00:36:52,866
UI 视图 对某些重新定位中止


842
00:36:52,866 --> 00:36:56,946
请求做出响应


843
00:36:56,946 --> 00:36:58,506
指导视图实际上


844
00:36:58,506 --> 00:37:01,106
会为你和你的用户提供一个


845
00:37:01,106 --> 00:37:02,946
新的 UI 界面 在这里他们   


846
00:37:03,066 --> 00:37:04,266
可以重新定位并且


847
00:37:04,266 --> 00:37:05,636
重新启动会话或者


848
00:37:05,636 --> 00:37:06,786
重置追踪


849
00:37:07,806 --> 00:37:09,676
因此这个新的视图


850
00:37:09,756 --> 00:37:11,746
对于你的 App 来说非常方便 


851
00:37:11,746 --> 00:37:12,596
这样你可以确保获得


852
00:37:12,596 --> 00:37:14,676
一致的设计并帮助你的用户 


853
00:37:15,306 --> 00:37:18,596
现在我们来谈谈面部追踪


854
00:37:19,316 --> 00:37:21,996
在 ARKit 1 中 我们启用面部


855
00:37:21,996 --> 00:37:24,386
追踪功能 可以来追踪一张脸


856
00:37:25,396 --> 00:37:27,456
在 ARKit 2 中 我们能够


857
00:37:27,456 --> 00:37:28,786
追踪多张脸


858
00:37:28,786 --> 00:37:31,516
能够同时追踪三张脸


859
00:37:32,536 --> 00:37:36,476
此外 你还可以


860
00:37:36,476 --> 00:37:38,196
确保在此人离开框架


861
00:37:38,196 --> 00:37:39,576
并再次返回时


862
00:37:39,576 --> 00:37:41,466
识别出他


863
00:37:41,466 --> 00:37:52,546
并再次为你提供相同的面部锚点 ID [掌声]


864
00:37:53,046 --> 00:37:55,796
因此多人面部追踪可以


865
00:37:55,796 --> 00:37:59,036
同时追踪三张脸


866
00:37:59,186 --> 00:38:00,826
并为你提供一个持久的


867
00:38:00,826 --> 00:38:02,986
面部锚点 ID 这样你就


868
00:38:02,986 --> 00:38:04,496
可以确保在整个会话中


869
00:38:04,496 --> 00:38:05,376
识别出一个用户


870
00:38:06,016 --> 00:38:07,616
如果你重新启动一个会话


871
00:38:08,436 --> 00:38:09,846
那么这个 ID 就会消失


872
00:38:09,846 --> 00:38:11,206
并出现一个新的会话 


873
00:38:12,536 --> 00:38:14,776
要实现这一点 其实非常简单


874
00:38:15,446 --> 00:38:17,036
我们在 ARFaceTrackingConfiguration 方面  


875
00:38:17,036 --> 00:38:18,856
有两个新属性


876
00:38:20,206 --> 00:38:21,386
第一个允许你


877
00:38:21,386 --> 00:38:23,756
查询在该特定设备上的


878
00:38:23,756 --> 00:38:25,386
一个会话中可以同时


879
00:38:25,386 --> 00:38:27,286
追踪多少面孔


880
00:38:27,846 --> 00:38:29,416
另一个允许你


881
00:38:29,416 --> 00:38:31,046
设置你想要同时追踪的


882
00:38:31,096 --> 00:38:32,356
追踪面数


883
00:38:32,856 --> 00:38:40,956
这就是多面部追踪 [掌声]


884
00:38:41,456 --> 00:38:43,096
让我们来讨论一种新的


885
00:38:43,246 --> 00:38:45,306
追踪配置 我们


886
00:38:45,806 --> 00:38:48,556
称它为 ARPositional
TrackingConfiguration 


887
00:38:49,486 --> 00:38:50,606
所以这个新的追踪


888
00:38:50,606 --> 00:38:53,416
配置只用于


889
00:38:53,416 --> 00:38:54,906
追踪用例


890
00:38:55,846 --> 00:38:58,096
比如你经常有这样


891
00:38:58,096 --> 00:39:00,066
一个用例 它实际上不需要


892
00:39:00,096 --> 00:39:01,846
渲染相机背景


893
00:39:03,206 --> 00:39:05,146
嗯 这是针对该用例制作的


894
00:39:06,376 --> 00:39:07,816
我们可以通过将


895
00:39:07,816 --> 00:39:09,956
渲染速率保持在 60 赫兹 


896
00:39:09,956 --> 00:39:13,006
来降低捕获帧速率和


897
00:39:13,006 --> 00:39:15,426
相机分辨率


898
00:39:15,426 --> 00:39:18,546
从而实现低能耗


899
00:39:21,956 --> 00:39:24,536
接下来 让我们谈谈


900
00:39:24,566 --> 00:39:25,376
我们今年所做的


901
00:39:25,376 --> 00:39:28,636
一些场景理解方面的改进


902
00:39:29,966 --> 00:39:31,166
图像检测和图像


903
00:39:31,166 --> 00:39:32,216
追踪已经存在了


904
00:39:32,216 --> 00:39:33,366
一段时间


905
00:39:34,086 --> 00:39:35,816
今年我们可以同时


906
00:39:35,816 --> 00:39:38,056
检测多达 100 个图像  


907
00:39:38,616 --> 00:39:42,226
例如 我们还为你提供


908
00:39:42,316 --> 00:39:44,316
检测打印


909
00:39:44,316 --> 00:39:46,306
图像比例的功能


910
00:39:47,386 --> 00:39:49,016
通常 当新的 App  


911
00:39:49,016 --> 00:39:51,346
要求用户使用图像


912
00:39:51,346 --> 00:39:52,696
来放置内容并


913
00:39:52,696 --> 00:39:55,216
相应地缩放内容时 图像


914
00:39:55,216 --> 00:39:57,456
可能会以不同的大小或者


915
00:39:57,456 --> 00:39:58,726
不同的纸张大小进行打印


916
00:39:59,426 --> 00:40:01,086
通过这种自动比例


917
00:40:01,086 --> 00:40:03,516
估算 你现在可以


918
00:40:03,516 --> 00:40:06,736
检测物理尺寸并相应地调整比例


919
00:40:06,736 --> 00:40:10,786
当你想要创建一个


920
00:40:10,786 --> 00:40:14,356
新的 AR 参考图像时


921
00:40:14,356 --> 00:40:15,666
我们还能够在


922
00:40:15,666 --> 00:40:17,196
运行时查询你传递给 ARKit 的  


923
00:40:17,196 --> 00:40:19,096
图像的质量


924
00:40:21,516 --> 00:40:23,376
我们还改进了


925
00:40:23,376 --> 00:40:24,806
我们的目标检测算法


926
00:40:25,966 --> 00:40:27,756
通过机器学习 我们可以


927
00:40:27,756 --> 00:40:29,536
提高目标检测算法


928
00:40:29,536 --> 00:40:34,896
为你提供更快的识别 以及


929
00:40:34,896 --> 00:40:36,626
更强大的环境


930
00:40:36,626 --> 00:40:38,006
在更多不同的环境中


931
00:40:38,326 --> 00:40:39,976
通常 你必须扫描


932
00:40:40,286 --> 00:40:42,346
环境中的特定对象 以便它在


933
00:40:42,346 --> 00:40:43,456
另一个环境中完美工作


934
00:40:44,226 --> 00:40:45,586
现在 这些都更灵活了


935
00:40:48,456 --> 00:40:51,166
最后 场景理解的


936
00:40:51,166 --> 00:40:52,246
另一个重要方面是


937
00:40:52,246 --> 00:40:54,616
平面估算


938
00:40:55,376 --> 00:40:56,536
通常 你需要平面


939
00:40:56,536 --> 00:40:58,706
估算来放置内容


940
00:40:58,856 --> 00:40:59,926
通过机器学习


941
00:41:00,476 --> 00:41:01,686
我们实际上使它


942
00:41:01,686 --> 00:41:04,006
更精确 我们使它


943
00:41:04,006 --> 00:41:05,956
更强大 使它更快地


944
00:41:05,956 --> 00:41:06,996
检测到平面


945
00:41:07,566 --> 00:41:10,356
让我们来看一个例子


946
00:41:10,986 --> 00:41:16,406
通过机器学习


947
00:41:16,406 --> 00:41:18,756
我们不仅在检测到


948
00:41:18,756 --> 00:41:19,956
这些特征时在地面上


949
00:41:19,956 --> 00:41:21,886
扩展这些平面 而且实际上


950
00:41:21,886 --> 00:41:23,466
流动的范围更广


951
00:41:23,746 --> 00:41:26,056
但是我们也有


952
00:41:26,056 --> 00:41:27,526
检测的能力


953
00:41:27,526 --> 00:41:29,676
当我们现在找不到特征时


954
00:41:30,026 --> 00:41:31,676
我们也有检测侧壁的能力


955
00:41:32,166 --> 00:41:41,376
这些都是机器学习的功劳 [掌声]


956
00:41:41,876 --> 00:41:44,056
正如你在这里看到的


957
00:41:44,056 --> 00:41:45,146
正如可以在上一个视频中看到的


958
00:41:45,146 --> 00:41:46,646
我们在平面上


959
00:41:46,696 --> 00:41:48,326
有好多种分类


960
00:41:49,026 --> 00:41:50,886
好吧 这又是通过机器学习来完成的


961
00:41:50,996 --> 00:41:52,336
去年 我们推出了


962
00:41:52,816 --> 00:41:54,446
五种不同的分类


963
00:41:55,136 --> 00:41:57,366
墙壁 地板 天花板 桌子和座椅


964
00:41:58,466 --> 00:41:59,806
今年 我们又推出了


965
00:41:59,936 --> 00:42:01,026
额外两种


966
00:42:01,406 --> 00:42:03,106
我们如今增加了


967
00:42:03,106 --> 00:42:05,886
检测门和窗的能力


968
00:42:07,296 --> 00:42:10,676
如前所述 平面分类真的非常重要


969
00:42:10,676 --> 00:42:12,526
或者平面估算


970
00:42:12,526 --> 00:42:13,566
对于在世界上放置内容


971
00:42:13,566 --> 00:42:14,696
非常重要


972
00:42:14,696 --> 00:42:17,256
这实际上是目标放置的理想选择


973
00:42:17,526 --> 00:42:19,566
你总是将你的对象放在一个表面上


974
00:42:20,896 --> 00:42:21,916
今年有了新的


975
00:42:21,916 --> 00:42:25,126
光线投射 API 你甚至可以


976
00:42:25,126 --> 00:42:26,556
更容易地将你的内容


977
00:42:26,746 --> 00:42:29,506
放置地更加准确 更加灵活


978
00:42:30,866 --> 00:42:32,956
它支持任何形式的表明对齐


979
00:42:33,286 --> 00:42:34,566
所以你不必再局限于


980
00:42:34,566 --> 00:42:36,116
垂直和水平方向了


981
00:42:38,156 --> 00:42:40,966
而且 你可以随时追踪你的光线投射


982
00:42:42,716 --> 00:42:44,466
这就意味着 当 ARKit 或者当


983
00:42:44,466 --> 00:42:45,486
你移动你的设备时


984
00:42:45,486 --> 00:42:47,066
ARKit 会检测到更多


985
00:42:47,066 --> 00:42:49,456
关于环境的信息 它可以


986
00:42:49,666 --> 00:42:51,486
准确地把你的物体


987
00:42:51,486 --> 00:42:52,986
放到物理表面的上边


988
00:42:52,986 --> 00:42:54,026
比如那些平面正在展开


989
00:42:54,546 --> 00:42:57,806
让我们来看看如何在 ARKit 中启用它


990
00:43:00,336 --> 00:43:02,816
听起来我们正在创建一个光线投射查询


991
00:43:03,526 --> 00:43:06,466
一个光线投射查询有三个参数


992
00:43:06,596 --> 00:43:08,096
第一个决定


993
00:43:08,096 --> 00:43:09,536
你想要进行光线投射的位置


994
00:43:10,106 --> 00:43:11,246
在此示例中 我们从屏幕中心


995
00:43:11,246 --> 00:43:12,266
开始执行此操作的


996
00:43:12,266 --> 00:43:15,896
然后你需要告诉它


997
00:43:16,126 --> 00:43:18,686
你想要允许的内容


998
00:43:18,686 --> 00:43:21,366
以便放置该内容或恢复转换


999
00:43:21,936 --> 00:43:25,406
此外 你还需要告诉它


1000
00:43:25,406 --> 00:43:26,566
你想要哪种对齐方式


1001
00:43:26,746 --> 00:43:30,686
它可以是水平的 垂直的或者任何


1002
00:43:32,616 --> 00:43:34,756
然后你需要在你的


1003
00:43:35,076 --> 00:43:36,616
AR 会话中将该查询传递给 


1004
00:43:36,616 --> 00:43:38,206
trackedRaycast 方法


1005
00:43:40,056 --> 00:43:41,976
这个方法有一个回调


1006
00:43:41,976 --> 00:43:43,836
它允许你对


1007
00:43:43,836 --> 00:43:45,406
新的转换做出反应 并使用


1008
00:43:45,406 --> 00:43:47,746
该光线投射给你结果 这样


1009
00:43:47,746 --> 00:43:49,996
你就可以相应地调整内容或者锚点


1010
00:43:50,456 --> 00:43:55,696
最后 当你完成这个光线投射 你就可以停下


1011
00:43:56,346 --> 00:44:00,236
这些是一些场景 这些是今年


1012
00:44:00,236 --> 00:44:02,466
我们在光线投射方面所做的改进


1013
00:44:07,256 --> 00:44:08,666
让我们继续我们


1014
00:44:08,666 --> 00:44:11,346
在视觉连贯性方面的增强


1015
00:44:12,216 --> 00:44:16,456
所以今年 我们有了这个新的


1016
00:44:16,456 --> 00:44:18,696
ARView 允许你 


1017
00:44:18,696 --> 00:44:19,986
根据需要激活和停用


1018
00:44:20,216 --> 00:44:22,456
不同的渲染选项


1019
00:44:23,396 --> 00:44:26,876
它还可以根据你的设备功能


1020
00:44:26,876 --> 00:44:29,226
自动停用和激活


1021
00:44:30,076 --> 00:44:31,936
让我们看一下这个例子


1022
00:44:32,266 --> 00:44:35,496
你可能之前已经看过这个视频


1023
00:44:35,496 --> 00:44:38,426
看看飞行器是如何移动的


1024
00:44:38,426 --> 00:44:39,646
物体是如何在表面上


1025
00:44:39,646 --> 00:44:42,066
移动的 所有这些看起来


1026
00:44:42,066 --> 00:44:43,646
都很真实


1027
00:44:44,276 --> 00:44:45,716
当一切都消失 你


1028
00:44:47,256 --> 00:44:49,156
实际上甚至没有意识到


1029
00:44:49,156 --> 00:44:50,176
那些物体是虚拟的


1030
00:44:50,666 --> 00:44:52,726
让我们看一下


1031
00:44:52,726 --> 00:44:55,196
我们所做的一些视觉连贯性的增强


1032
00:44:55,736 --> 00:44:57,956
让我们再看看


1033
00:44:57,956 --> 00:44:59,396
这个视频的开头 


1034
00:44:59,396 --> 00:45:00,936
让这些球在桌子上滚动时


1035
00:45:00,936 --> 00:45:02,986
暂停一下


1036
00:45:04,376 --> 00:45:05,926
这里你可以看到


1037
00:45:06,006 --> 00:45:08,256
景深效果


1038
00:45:08,256 --> 00:45:09,736
这是 RealityKit 的一个新功能


1039
00:45:11,226 --> 00:45:13,216
你的 AR 体验专为


1040
00:45:13,216 --> 00:45:15,746
大型和小型房间而设计


1041
00:45:16,316 --> 00:45:19,196
你 iOS 设备上的相机


1042
00:45:19,196 --> 00:45:20,696
总是根据环境调整焦距


1043
00:45:20,696 --> 00:45:23,266
而景深功能


1044
00:45:23,266 --> 00:45:25,436
允许调整对


1045
00:45:25,436 --> 00:45:26,776
虚拟内容的焦距


1046
00:45:26,776 --> 00:45:29,596
使其与你的物理内容完美匹配


1047
00:45:29,596 --> 00:45:31,626
从而使对象


1048
00:45:31,676 --> 00:45:36,306
在环境中完美融合


1049
00:45:36,306 --> 00:45:37,416
此外 当你快速


1050
00:45:37,416 --> 00:45:39,026
移动相机或物体


1051
00:45:39,026 --> 00:45:41,566
快速移动时 你可以看到


1052
00:45:41,566 --> 00:45:43,356
会出现模糊


1053
00:45:44,496 --> 00:45:46,516
好吧 大部分时间


1054
00:45:46,516 --> 00:45:48,626
你有一个常规渲染器


1055
00:45:48,626 --> 00:45:50,986
并没有运动模糊效果


1056
00:45:50,986 --> 00:45:52,896
那么你的虚拟内容就会很突出


1057
00:45:54,346 --> 00:45:56,436
它并没有真正融入环境


1058
00:45:57,096 --> 00:45:58,356
由于 VIO 相机


1059
00:45:58,356 --> 00:46:01,226
运动和参数感


1060
00:46:01,226 --> 00:46:06,116
我们可以合成运动模糊


1061
00:46:06,436 --> 00:46:07,746
我们可以将它应用于


1062
00:46:07,746 --> 00:46:10,506
可视对象 以便它在你的


1063
00:46:10,586 --> 00:46:11,696
环境中完美融合


1064
00:46:11,696 --> 00:46:15,506
这是 ARSCNView 和


1065
00:46:15,506 --> 00:46:16,596
ARView 上的一个变量


1066
00:46:16,716 --> 00:46:20,446
让我们再看一下这个例子


1067
00:46:20,446 --> 00:46:21,876
这里的一切看起来都很好


1068
00:46:55,616 --> 00:46:58,646
今年我们为增强视觉一致性而


1069
00:46:58,646 --> 00:47:00,306
提供的另外两个 API 是


1070
00:47:00,616 --> 00:47:03,756
HDR 环境纹理和相机颗粒


1071
00:47:06,286 --> 00:47:07,996
放置内容时


1072
00:47:07,996 --> 00:47:08,956
你真的希望该内容能够


1073
00:47:08,956 --> 00:47:10,026
反映现实世界


1074
00:47:11,296 --> 00:47:13,206
有了高动态范围 


1075
00:47:13,986 --> 00:47:16,226
即使在明亮的


1076
00:47:16,226 --> 00:47:17,236
光线环境中


1077
00:47:17,356 --> 00:47:18,746
你也可以捕捉那些突出的或者更高的高光


1078
00:47:19,376 --> 00:47:21,076
使你的内容更有活力


1079
00:47:22,626 --> 00:47:23,846
今年使用 ARKit 我们


1080
00:47:23,846 --> 00:47:25,386
实际上可以请求这些


1081
00:47:25,386 --> 00:47:26,666
HDR 环境纹理


1082
00:47:26,666 --> 00:47:28,576
以便你的内容看起来更好 


1083
00:47:29,896 --> 00:47:32,846
此外 我们还有一个相机颗粒 API


1084
00:47:33,786 --> 00:47:36,236
你可能会注意到


1085
00:47:36,236 --> 00:47:38,026
当你在一个光线很暗的环境中


1086
00:47:38,026 --> 00:47:39,506
有一个 AR 体验


1087
00:47:39,806 --> 00:47:41,856
与相机相比 


1088
00:47:42,176 --> 00:47:43,046
其他内容看起来非常闪亮


1089
00:47:44,906 --> 00:47:46,476
每个相机都会产生一些颗粒


1090
00:47:46,786 --> 00:47:48,056
尤其在很暗的环境下


1091
00:47:48,126 --> 00:47:51,216
颗粒感看起来更重


1092
00:47:51,856 --> 00:47:53,336
使用这种新的相机颗粒


1093
00:47:53,336 --> 00:47:55,906
API 我们可以确保 


1094
00:47:56,716 --> 00:47:58,686
在你的虚拟内容上


1095
00:47:58,686 --> 00:48:00,576
应用相同的颗粒图案 以便它


1096
00:48:00,576 --> 00:48:03,386
很好地融合而且不会非常突出


1097
00:48:04,516 --> 00:48:06,296
所以这些是今年


1098
00:48:07,916 --> 00:48:10,966
但是我们并没有就此止步


1099
00:48:11,776 --> 00:48:13,156
我认为很多人都想要


1100
00:48:13,156 --> 00:48:14,456
这样一个功能


1101
00:48:16,146 --> 00:48:17,076
当你开发一种 AR


1102
00:48:17,076 --> 00:48:19,076
体验时 你总有


1103
00:48:19,116 --> 00:48:21,506
或者经常为了原型


1104
00:48:21,506 --> 00:48:23,236
或者测试你的体验去


1105
00:48:23,236 --> 00:48:24,266
一个特定的地方


1106
00:48:26,356 --> 00:48:28,386
大多数时候 当你去那里的时候


1107
00:48:28,386 --> 00:48:29,386
你会回到你的


1108
00:48:29,386 --> 00:48:30,186
桌子 你开发你的体验


1109
00:48:30,186 --> 00:48:31,696
你想要再回来


1110
00:48:31,866 --> 00:48:34,216
今年使用 Reality


1111
00:48:34,216 --> 00:48:36,876
Composer App 你现在可以记录


1112
00:48:36,966 --> 00:48:38,976
体验或者顺序


1113
00:48:40,076 --> 00:48:41,216
这意味着你可以去


1114
00:48:41,266 --> 00:48:42,936
你最喜欢的地方 在那里


1115
00:48:43,006 --> 00:48:44,556
你将通过捕捉环境


1116
00:48:44,556 --> 00:48:48,206
来获得体验


1117
00:48:48,706 --> 00:48:50,376
ARKit 将确保将 


1118
00:48:50,376 --> 00:48:52,596
传感器数据和视频流一起


1119
00:48:52,596 --> 00:48:56,436
保存到电影文件


1120
00:48:56,436 --> 00:48:58,906
容器中 这样你就可以


1121
00:48:58,996 --> 00:49:02,946
随身携带并将其放入 Xcode


1122
00:49:03,076 --> 00:49:06,256
此时 Xcode 方案设置


1123
00:49:06,256 --> 00:49:08,156
像一个新的


1124
00:49:08,156 --> 00:49:10,126
附加功能或一个


1125
00:49:10,226 --> 00:49:13,456
允许你选择该文件的新字段


1126
00:49:16,106 --> 00:49:18,336
选择该文件


1127
00:49:18,336 --> 00:49:19,976
然后在连接到 Xcode 的


1128
00:49:19,976 --> 00:49:23,906
设备上按运行 然后


1129
00:49:24,046 --> 00:49:26,316
你可以在桌面上重播该体验


1130
00:49:26,936 --> 00:49:28,636
这是原型设计的理想选择


1131
00:49:29,256 --> 00:49:31,336
更适合使用


1132
00:49:31,336 --> 00:49:32,736
不同参数调整


1133
00:49:32,736 --> 00:49:34,136
AR 配置 并尝试


1134
00:49:34,136 --> 00:49:35,526
查看体验的外观


1135
00:49:36,076 --> 00:49:46,456
你甚至可以对某些追踪做出反应 [掌声]


1136
00:49:46,956 --> 00:49:48,376
这很棒


1137
00:49:48,376 --> 00:49:50,136
我想今年在 ARKit 3 中


1138
00:49:50,136 --> 00:49:51,776
你将会有很多不同的工具


1139
00:49:51,776 --> 00:49:54,456
你可以通过协作会话 


1140
00:49:54,456 --> 00:49:56,296
多面部追踪


1141
00:49:56,356 --> 00:49:59,246
来增强你的多用户体验


1142
00:50:00,096 --> 00:50:02,356
你可以通过


1143
00:50:02,406 --> 00:50:03,986
RealityKit 的新的一致性效果


1144
00:50:03,986 --> 00:50:06,746
以及 ARWorldTrackingConfiguration   


1145
00:50:06,746 --> 00:50:08,676
上的新视觉效果来增强


1146
00:50:08,676 --> 00:50:10,926
所有 AR App 的真实感


1147
00:50:12,396 --> 00:50:13,916
你可以通过新的运动捕捉


1148
00:50:14,996 --> 00:50:18,086
以及同时使用前置和后置摄像头来


1149
00:50:18,086 --> 00:50:20,396
启用新的用例


1150
00:50:21,536 --> 00:50:23,396
当然 在现有功能的基础上


1151
00:50:23,396 --> 00:50:26,416
还有很多改进


1152
00:50:26,756 --> 00:50:28,526
例如 使用目标


1153
00:50:28,526 --> 00:50:29,676
检测和机器学习


1154
00:50:30,266 --> 00:50:33,456
至少 最后但并非最不重要的是


1155
00:50:33,456 --> 00:50:36,676
我认为记录和重播


1156
00:50:36,676 --> 00:50:38,446
工作流程将使


1157
00:50:38,446 --> 00:50:39,966
你的设计和原型


1158
00:50:39,966 --> 00:50:41,716
体验设计比以前更好


1159
00:50:41,716 --> 00:50:44,956
所以我真的很期待


1160
00:50:44,956 --> 00:50:46,296
你能去网站上


1161
00:50:46,296 --> 00:50:47,526
下载我们的样品


1162
00:50:49,036 --> 00:50:50,556
我们还有几个实验室


1163
00:50:50,946 --> 00:50:52,476
一个在明天


1164
00:50:52,576 --> 00:50:54,416
一个在周四 我希望


1165
00:50:54,416 --> 00:50:55,466
你能来 跟我们说说


1166
00:50:55,526 --> 00:50:58,016
你有啥问题 或者只是单纯聊聊天 


1167
00:50:59,166 --> 00:51:01,136
然后我们还有两个


1168
00:51:01,136 --> 00:51:03,016
深度会议 一个是关于


1169
00:51:03,016 --> 00:51:04,856
将人们带入到


1170
00:51:04,856 --> 00:51:07,046
AR 中 我们将更多的讨论 


1171
00:51:07,046 --> 00:51:08,166
人物遮挡和运动捕捉


1172
00:51:09,086 --> 00:51:12,146
第二个是关于协作 AR 体验


1173
00:51:12,806 --> 00:51:15,946
我希望你能享受会议的剩下部分


1174
00:51:17,336 --> 00:51:18,096
祝你有美好的一天


1175
00:51:18,416 --> 00:51:22,500
再见 [掌声]

